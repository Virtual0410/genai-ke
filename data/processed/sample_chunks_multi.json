[
  {
    "chunk_id": "doc_5dbefb04_chunk_1_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 1,
    "text": "JournalofMachineLearningResearch25(2024)1-27 Submitted7/22;Revised11/23;Published1/24\nDecorrelated Variable Importance\nIsabella Verdinelli isabella@stat.cmu.edu\nDepartment of Statistics\nCarnegie Mellon University\n5000 Forbes Ave.\nPittsburgh, PA 15213, USA\nLarry Wasserman larry@stat.cmu.edu\nDepartment of Statistics\nCarnegie Mellon University\n5000 Forbes Ave.\nPittsburgh, PA 15213, USA\nEditor: Eric Laber\nAbstract\nBecause of the widespread use of black box prediction methods such as random forests\na"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_1_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 1,
    "text": "ck box prediction methods such as random forests\nand neural nets, there is renewed interest in developing methods for quantifying variable\nimportance as part of the broader goal of interpretable prediction. A popular approach is\nto define a variable importance parameter \u2014 known as LOCO (Leave Out COvariates) \u2014\nbased on dropping covariates from a regression model. This is essentially a nonparametric\nversionofR2. Thisparameterisverygeneralandcanbeestimatednonparametrically, but\nit can be hard to i"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_1_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 1,
    "text": "stimatednonparametrically, but\nit can be hard to interpret because it is affected by correlation between covariates. We\npropose a method for mitigating the effect of correlation by defining a modified version of\nLOCO. This new parameter is difficult to estimate nonparametrically, but we show how to\nestimate it using semiparametric models.\nKeywords: Correlation, Nonparametric Estimators, Prediction, Variable Importance\n1 Introduction\nDue to the increasing popularity of black box prediction method"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_1_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 1,
    "text": "creasing popularity of black box prediction methods like random forests and\nneural nets, there has been renewed interest in the problem of quantifying variable impor-\ntance in regression. Consider predicting Y \u2208 R from covariates (X,Z) where X \u2208 Rg and\nZ \u2208 Rh. We have separated the covariates into X and Z where X represents the covariates\nwhose importance we wish to assess. In what follows, we let U = (X,Z,Y) denote all the\nvariables. Define \u00b5(x,z) = E[Y|X = x,Z = z] so that\nY = \u00b5(X,Z)+(cid:15)\n"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_1_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 1,
    "text": "z) = E[Y|X = x,Z = z] so that\nY = \u00b5(X,Z)+(cid:15)\nwhere E[(cid:15)|X,Z] = 0.\nA popular measure of the importance of X is\n\u03c8 = E[(\u00b5(Z)\u2212\u00b5(X,Z))2] = E[(Y \u2212\u00b5(Z))2]\u2212E[(Y \u2212\u00b5(X,Z))2]. (1)\nL\nwhere\u00b5(Z) = E[Y|Z = z]. Uptoscaling,\u03c8 isanonparametricversionoftheusualR2 from\nL\nstandard regression. This was called LOCO (Leave Out COvariates) in Lei et al. (2018)\n(cid:13)c2024IsabellaVerdinelliandLarryWasserman.\nLicense: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/. Attributionrequirementsareprovided"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_1_5",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 1,
    "text": "censes/by/4.0/. Attributionrequirementsareprovided\nathttp://jmlr.org/papers/v25/22-0801.html."
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_2_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 2,
    "text": "Verdinelli and Wasserman\nand Rinaldo et al. (2019) and has been further studied recently in Williamson et al. (2021),\nWilliamson et al. (2020) and Zhang and Janson (2020). The parameter \u03c8 is appealing\nL\nbecause it is very general and easy to interpret. But it suffers from some problems. In\nparticular, the value of \u03c8 depends on the correlation between X and Z. When X and\nL\nZ are highly correlated, \u03c8 will be near 0 since removing X has little effect. In some\napplications, this might be undesirable"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_2_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 2,
    "text": "t. In some\napplications, this might be undesirable as it obscures interpretability. We refer to this\nproblem as correlation distortion. Another, more technical problem with LOCO, is its\nquadratic nature which causes some issues when constructing confidence intervals.\nIn this paper, we define a modified version of \u03c8 denoted by \u03c8 that is invariant to the\nL 0\ncorrelation between X and Z. There is a tradeoff: the modified parameter \u03c8 is free from\n0\ncorrelation distortion but it is more difficult to "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_2_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 2,
    "text": "orrelation distortion but it is more difficult to estimate than \u03c8 . In a sense, we remove the\nL\ncorrelation from the estimand at the expense of larger confidence intervals. This is similar\nto estimating a coefficient in a linear regression where the value of the regression coefficient\ndoes not depend on the correlation between X and Z while the width of the confidence\ninterval does. To reduce the difficulties in estimating \u03c8 , we approximate \u00b5(x,z) with the\n0\nsemiparametric model \u00b5(x,z) = \u03b2(z)Tx"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_2_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 2,
    "text": "z) with the\n0\nsemiparametric model \u00b5(x,z) = \u03b2(z)Tx+f(z).\nRelated Work. Assessing variable importance is an active area of research. Recent\npapers on LOCO include Lei et al. (2018); Rinaldo et al. (2019); Williamson et al. (2021,\n2020); Zhang and Janson (2020). Another approach is to use derivatives of the regression\nfunctionassuggestedinSamarov(1993), andhasreceivedrenewedattentioninthemachine\nlearning literature (Ribeiro et al., 2016). There has been a surge of interest in an approach\nbasedonSh"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_2_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 2,
    "text": " been a surge of interest in an approach\nbasedonShapleyvalues,seeforexample,Messalasetal.(2019);Aasetal.(2019);Lundberg\nand Lee (2016); Covert et al. (2020); Fryer et al. (2020); Covert and Lee (2020); Israeli\n(2007); B\u00b4enard et al. (2021). We discuss derivatives and Shapley values in Section 5.\nAnother paper that uses semiparametric models for intepretability is Sani et al. (2020) but\nthatpaperdoesnotfocusonvariableimportance. LohandZhou(2021)containsareviewof\nseveral feature importance methods"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_2_5",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 2,
    "text": "ntainsareviewof\nseveral feature importance methods and, in particular, discusses the importance of missing\ndata.\nPaper Outline. In Section 2 we describe some issues related to LOCO and this leads\nus to define a few modified versions of the parameter. In Section 3 we discuss inference\nfor the parameters. Section 4 contains some simulation studies. Section 5 discusses other\nissues and other measures of variable importance. A concluding discussion is in Section 6.\nTechnical details and proofs are i"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_2_6",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 2,
    "text": "s in Section 6.\nTechnical details and proofs are in an appendix.\n2 Issues With LOCO\nThe parameter \u03c8 is general and it is easy to obtain point estimates for it; see Section 3.1.\nL\nBut it does have two shortcomings which we now discuss.\n2.1 Issue 1: Inference For Quadratic Functionals\nThe first, and less serious issue, is that \u03c8 is a quadratic parameter and it is difficult to get\nL\nconfidence intervals for quadratic parameters because their limiting distribution and rate of\n2"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_2_7",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 2,
    "text": "g distribution and rate of\n2"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_3_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 3,
    "text": "Variable Importance\nconvergence change as \u03c8 approaches 0. This is actually a common problem but it receives\nL\nlittle attention. Many other parameters have this problem, including distance correlation\n(Sz\u00b4ekely et al., 2007), RKHS correlations (Sejdinovic et al., 2013) and kernel two-sample\nstatistics (Gretton et al., 2012) among others.\nToillustrate,considerthefollowingtoyexample. LetY ,...,Y \u223c N(\u00b5,\u03c32)andconsider\n1 n\n\u221a\nestimating \u03c8 = \u00b52 with \u03c8(cid:98) = Y 2 . When \u00b5 (cid:54)= 0, we have n(\u03c8(cid:"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_3_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 3,
    "text": "d:98) = Y 2 . When \u00b5 (cid:54)= 0, we have n(\u03c8(cid:98)\u2212\u03c8) (cid:32) N(0,\u03c42) for some\nn\n\u03c42. When \u00b5 = 0, \u03c8(cid:98)\u223c \u03c32\u03c72/n. When \u00b5 is close to 0, its distribution is neither Normal nor\n1 \u221a\nchi-squared, and the rate of convergence can be anything between 1/n and 1/ n.\nMore generally, when dealing with a quadratic functional \u03c8, it is often the case that\nan estimator \u03c8(cid:98) converges to a Normal at a n\u22121/2 rate when \u03c8 (cid:54)= 0 but at the null, where\n\u03c8 = 0, the influence function for the parameter"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_3_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 3,
    "text": "re\n\u03c8 = 0, the influence function for the parameter vanishes, the rate becomes n\u22121 and the\nlimiting distribution is typically a combination of \u03c72 random variables. Near the null, we\nget behavior in between these two cases. A valid confidence interval C should satisfy\nn\nP(\u03c8 \u2208 C ) \u2192 1\u2212\u03b1 even if \u03c8 is allowed to change with n. In particular, we want to allow\nn n n\n\u03c8 \u2192 0. Finding a confidence interval with this uniformly correct coverage, with length\nn\nn\u22121/2 away from the null and length n\u22121 at the nu"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_3_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 3,
    "text": "\nn\u22121/2 away from the null and length n\u22121 at the null is, to the best of our knowledge, an\nunsolved problem.\nOurproposalistoconstructaconservativeconfidenceintervalthatdoesnothavelength\n(cid:112)\nO(1/n) at the null. We replace the standard error se of \u03c8(cid:98) with se2+c2/n where c is a\nconstant. We take c = (Var[Y])2 to put the quantity on the right scale, but other constants\ncould be used. This leads to valid confidence intervals but they are conservative near the\nnull as they shrink at rate n"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_3_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 3,
    "text": "onservative near the\nnull as they shrink at rate n\u22121/2 instead of n\u22121.\nWe are only aware of two other attempts to address this issue. Both involve expanding\nthewidthoftheconfidenceintervaltobeO(n\u22121/2). Daietal.(2021)addednoiseoftheform\n\u221a\ncZ/ n to the estimator, where Z \u223c N(0,1). They choose c by permuting the data many\ntimes and finding a c that gives good coverage under the simulated permutations. However,\nthis is computationally expensive and adding noise seems unnecessary. Williamson et al.\n("
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_3_5",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 3,
    "text": "dding noise seems unnecessary. Williamson et al.\n(2020) deal with this problem by writing \u03c8 as a sum of two parameters \u03c8 = \u03c8 +\u03c8 such\n1 2\nthat neither \u03c8 nor \u03c8 vanish when \u03c8 = 0. Then, they estimate \u03c8 and \u03c8 on separate\n1 2 \u221a1 2\nsplits of the data. This again amounts to adding noise of size O(1/ n).\nAll three approaches are basically the same; they have the effect of expanding the\nconfidence interval by O(n\u22121/2) which maintains validity at the expense of efficiency at the\nnull. Our approach has the"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_3_6",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 3,
    "text": "se of efficiency at the\nnull. Our approach has the virtue of being simple and fast. It does not require adding\nnoise, extra calculations or doing an extra split of the data.\nTo see that expanding the standard error does lead to an interval with correct coverage,\nlet \u03c8(cid:98) denote an estimator of a parameter \u03c8\nn\nwhich we allow to change with n. We are\nconcerned with the case were the bias b satisfies b = o(n\u22121/2) and the variance v satisfies\nn n n\nv = o(1/n). (The variance would be of order 1/"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_3_7",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 3,
    "text": "n n\nv = o(1/n). (The variance would be of order 1/n in the non-degenerating case.) Then, by\nn\n(cid:112)\nMarkov\u2019s inequality, the non-coverage of the interval \u03c8(cid:98)n \u00b1z\n\u03b1/2\nse2+c2/n is\n(cid:16) (cid:112) (cid:17) (cid:16) (cid:112) (cid:17)\nP |\u03c8(cid:98)n \u2212\u03c8\nn\n| > z\n\u03b1/2\nse2+c2/n \u2264 P |\u03c8(cid:98)n \u2212\u03c8\nn\n| > z\n\u03b1/2\nc2/n\nn n\n\u2264\ncz2\nE[|\u03c8(cid:98)n \u2212\u03c8\nn\n|2] =\ncz2\n(b2\nn\n+v\nn\n) = o(1).\n\u03b1/2 \u03b1/2\n3"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_4_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 4,
    "text": "Verdinelli and Wasserman\n2.2 Issue 2: Correlation Distortion\nThe second and more pernicious problem is that \u03c8 depends on the correlation between X\nL\nand Z. In particular, if X and Z are highly correlated, then \u03c8 will typically be close to\nL\n0. We call this, correlation distortion. There may be applications where this is acceptable.\nBut in some cases we may want to alleviate this distortion and that is the focus of this\npaper.\nTo appreciate the effect of correlation distortion, consider the linea"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_4_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 4,
    "text": "fect of correlation distortion, consider the linear model Y = \u03b2X +\n\u03b8Z + (cid:15). In this case, a natural measure of variable importance is \u03b2 which is unaffected\nby correlation between X and Z. The standard error of the estimate \u03b2(cid:98) is affected by\nthe correlation but the estimand itself is not. For this model, \u03c8 = \u03b22\u03b32 where \u03b32 =\nL\nE[(X \u2212 \u03bd(Z))2] and \u03bd(z) = E[X|Z = z]. This makes it clear that \u03c8 \u2192 0 as X and Z\nL\nbecome more correlated. The same fate befalls the partial correlation \u03c1 betwee"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_4_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 4,
    "text": "same fate befalls the partial correlation \u03c1 between Y and X\nwhich in this model is \u03c1 = (1+ \u03b22\u03c32 )\u22121/2 where \u03c32 = Var[(cid:15)]. Again, \u03c1 \u2192 0 as \u03b3 \u2192 0.\n\u03b32\nTodealwiththisproblem, wedefineamodifiedLOCOparameter\u03c8 whichisunaffected\n0\nby the dependence between X and Z. Let p (x,y,z) = p(y|x,z)p(x)p(z). Then p is the\n0 0\ndistribution that is closest to p in Kullback-Leibler distance subject to making X and Z\nindependent. We define\n\u03c8 = E [(\u00b5 (X,Z)\u2212\u00b5 (Z))2]. (2)\n0 0 0 0\nA simple calculation shows that \u00b5 "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_4_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 4,
    "text": "2]. (2)\n0 0 0 0\nA simple calculation shows that \u00b5 (z) = E [Y|Z = z] = (cid:82) \u00b5(x,z)p(x)dx and so\n0 0\n(cid:90)\n\u03c8 = (\u00b5 (x,z)\u2212\u00b5 (z))2p(x)p(z)dxdz. (3)\n0 0 0\nWe can think of \u03c8 as a counterfactual quantity answering the question: what would the\n0\nchange in \u00b5(X,Z) be if we dropped X and had X and Z been independent.\nThis parameter completely eliminates the correlation distortion but, as we show in our\nsimulations, it can be hard to get an accurate estimate of \u03c8 . In particular, nonparametric\n0\nconfi"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_4_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 4,
    "text": "timate of \u03c8 . In particular, nonparametric\n0\nconfidence intervals are wide. A simple, but somewhat ad-hoc solution, is to first remove\nZ(cid:48)s that are highly correlated with X. That is, define \u03c8 = E[(\u00b5(V)\u2212\u00b5(X,V))2] where\nj 1\nV = (Z : |\u03c1(X,Z )| \u2264 t) for some t where \u03c1 is a measure of dependence.\nj j\nThe main solution we propose is to use the semiparametric model \u00b5(x,z) = xT\u03b2(z)+\nf(z). Under this model, one can show that \u03c8 takes the form tr (cid:0) \u03a3 E[\u03b2(Z)\u03b2(Z)T] (cid:1) where\n0 X\n\u03a3 = Var[X]. "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_4_5",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 4,
    "text": "d:0) \u03a3 E[\u03b2(Z)\u03b2(Z)T] (cid:1) where\n0 X\n\u03a3 = Var[X]. (See appendix 8.4 for details). However, this parameter is still difficult to\nX\nestimate so we propose the following two simpler models. First, let \u00b5(x,z) = \u03b2Tx+f(z).\nThen \u03c8 becomes\n0\n\u03c8 = \u03b2T\u03a3 \u03b2. (4)\n2 X\nThe second model is\n(cid:88)(cid:88)\n\u00b5(x,z) = \u03b2Tx+ \u03b3 x z +f(z). (5)\njk j k\nj j\nIn Section 3.5 we show that \u03c8 then becomes\n0\n\u03c8 = \u03b8T\u2126\u03b8 (6)\n3\nwhere\n(cid:40) (cid:41)\u22121 (cid:34)\n(cid:18) (cid:19) (cid:18) (cid:19)\n(cid:35)\n\u03b8 = E[Z(cid:101)Z(cid:101) T"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_4_6",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 4,
    "text": "18) (cid:19)\n(cid:35)\n\u03b8 = E[Z(cid:101)Z(cid:101) T \u2297(X \u2212\u03bd(Z))(X \u2212\u03bd(Z))T] E Y \u2212\u00b5(Z) Z(cid:101)\u2297(X \u2212\u03bd(Z)) .\n4"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_5_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 5,
    "text": "Variable Importance\n\u03c8 = (cid:82) (cid:82) (\u00b5(x,z)\u2212\u00b5 (z))2p(x)p(z)dxdz \u03c8 = E[(\u00b5(X,V)\u2212\u00b5(V))2]\n0 0 1\n\u03c8 = \u03b2T\u03a3 \u03b2 \u03c8 = \u03b8T\u2126\u03b8\n2 X 3\n(cid:82)\n\u00b5 (z) = \u00b5(x,z)p(x)dx V = (Z : |\u03c1(X,Z )| \u2264 t)\n0 j j\n(cid:20) 1 mT (cid:21)\nZ(cid:101) T = (1,ZT) \u2126 = \u03a3\nX\n\u2297\nm \u03a3 +m\nZ\nmT\nZ Z Z Z\n\u03b2 = E[(Y \u2212\u00b5(Z))(X \u2212\u03bd(Z))]/E[(X \u2212\u03bd(Z))2]\n(cid:40) (cid:41)\u22121 (cid:34)\n(cid:18) (cid:19) (cid:18) (cid:19)\n(cid:35)\n\u03b8 = E[Z(cid:101)Z(cid:101) T \u2297(X \u2212\u03bd(Z))(X \u2212\u03bd(Z))T] E Y \u2212\u00b5(Z) Z(cid:101)\u2297(X \u2212\u03bd(Z))\nTable 1: Summary of Decorrelated Parameters\n.\n"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_5_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 5,
    "text": "Z))\nTable 1: Summary of Decorrelated Parameters\n.\n\u03bd(z) = E[X|Z = z], Z(cid:101) = (1,Z) and\n(cid:18) 1 mT (cid:19)\n\u2126 = \u03a3\nX\n\u2297E[Z(cid:101)Z(cid:101) T] = \u03a3\nX\n\u2297\nm \u03a3 +m\nZ\nmT\n,\nZ Z Z Z\nm = E[Z] and \u03a3 = Var[Z]. Table 1 summarizes the expressions for the parameters.\nZ Z\nRemark: Using the semiparametric model simplifies statistical inference for \u03c8 . Of\n0\ncourse, using a model always carries risks. In particular, if the model is not a reasonable\napproximationto\u00b5(x,z)thenwecouldbeintroducingbias. Therefor"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_5_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 5,
    "text": "tionto\u00b5(x,z)thenwecouldbeintroducingbias. Therefore, asinallcaseswherea\nmodel is used, one should be aware that if the model is wrong then we are actually estimating\nthe projection of \u00b5(x,z) onto the model and then \u03c8 captures the importance of X in the\n0\nprojected model.\nRemark: In all the above definitions, we can replace X with b(X) = (b (X),...,b (X))\n1 k\nfor a given set of basis functions b ,...,b to make the model more flexible. For example,\n1 k\nwe can take b(X) = (X,X2,X3) or an orthogonal"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_5_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 5,
    "text": "\n1 k\nwe can take b(X) = (X,X2,X3) or an orthogonalized version of the polynomials, which is\nwhat we use in several of our examples.\nInthesesemiparametricmodels,wecanestimatethenuisancefunctions\u03bd(z) = E[X|Z =\nz] and \u00b5(z) either nonparametrically or parametrically.\n3 Inference\nIn this section we discuss estimation of \u03c8 \u2208 {\u03c8 ,\u03c8 ,\u03c8 ,\u03c8 ,\u03c8 }. For \u03c8 ,\u03c8 and \u03c8 we\nL 0 1 2 3 0 2 3\nuse one-step estimation which we now briefly review. See Hines et al. (2021) for a recent\ntutorial on one-step estimators. Let "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_5_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 5,
    "text": "for a recent\ntutorial on one-step estimators. Let \u03c8(\u03b3) be a parameter with efficient influence function\n\u03c6(u,\u03b3,\u03c8) where \u03b3 denotes nuisance functions. We split the data into two groups D and D\n0 1\n5"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_6_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 6,
    "text": "Verdinelli and Wasserman\nand we estimate \u03b3 from D . Splitting the data is a common technique in semiparametric\n0\ninferenceasitleadstocentrallimittheoremsunderweakerconditionsthanwouldotherwise\nbe necessary. The one-step estimator is\n1 (cid:88)\n\u03c8(cid:98)= \u03c8(cid:98)pi + \u03c6(U\ni\n,\u03b3\n(cid:98)\n,\u03c8(cid:98)pi )\nn\ni\nwhere \u03c8(cid:98)pi = \u03c8(\u03b3\n(cid:98)\n) is the plug-in estimator and the average is over D\n1\n. This estimator\ncomes from the von Mises expansion of \u03c8(\u03b3) around a point \u03b3 given by \u03c8(\u03b3) = \u03c8(\u03b3) +\n(cid:8"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_6_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 6,
    "text": "(\u03b3) around a point \u03b3 given by \u03c8(\u03b3) = \u03c8(\u03b3) +\n(cid:82)\n\u03c6(u,\u03b3)dP(u)+R where R is the remainder. Alternatively, we can define \u03c8(cid:98)as the solution\nto the estimating equation\nn\u22121(cid:80)\n\u03c6(U ,\u03b3,\u03c8) = 0.\ni i (cid:98)\nBoth estimators have second order bias ||\u03b3 \u2212\u03b3||2. Under appropriate conditions, both\n\u221a (cid:98)\nestimators satisfy n(\u03c8(cid:98)\u2212\u03c8) (cid:32) N(0,\u03c42) where \u03c42 = E[\u03c62(U,\u03b3,\u03c8)]. The key condition for\nthis central limit theorem to hold is that ||\u03b3\u2212\u03b3||2 = o (n\u22121/2) which holds under standard\n("
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_6_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 6,
    "text": " ||\u03b3\u2212\u03b3||2 = o (n\u22121/2) which holds under standard\n(cid:98) P\nsmoothness assumptions. For example, if \u03b3 is in a Holder class of smoothness s, then an\noptimal estimator \u03b3 satisfies ||\u03b3 \u2212\u03b3||2 = O (n\u22122s/(2s+d)) = o (n\u22121/2) when s > d/2. The\n(cid:98) (cid:98) P P\nplugin estimator has first order bias ||\u03b3 \u2212\u03b3|| which will never be o (n\u22121/2).\n(cid:98) P\nThe usual confidence interval is \u03c8(cid:98)\u00b1z\n\u03b1/2\nse where se2 = \u03c4\n(cid:98)\n2/n and \u03c4\n(cid:98)\n2 = n\u22121(cid:80)\ni\n\u03c62(U\ni\n,\u03b3\n(cid:98)\n).\nBut we find that th"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_6_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 6,
    "text": "id:80)\ni\n\u03c62(U\ni\n,\u03b3\n(cid:98)\n).\nBut we find that this often underestimates the standard error. Instead, we use a different\napproach described in Section 3.6. We consider three different estimators for the nuisance\nfunctions \u00b5(z) and \u03bd(z): (i) linear, (ii) additive and (iii) random forests.\n3.1 Estimating \u03c8\nL\nWilliamsonetal.(2021)foundtheefficientinfluencefunctionfor\u03c8 . However,inWilliamson\nL\net al. (2020) the authors note that one can avoid having to use the influence function by\nrewriting \u03c8 as\nL"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_6_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 6,
    "text": " to use the influence function by\nrewriting \u03c8 as\nL\n\u03c8 = E[(Y \u2212\u00b5(Z))2]\u2212E[(Y \u2212\u00b5(X,Z))2].\nL\nIt is easy to check that the corresponding plugin estimator\n1 (cid:88) 1 (cid:88)\n\u03c8(cid:98)L = (Y\ni\n\u2212\u00b5\n(cid:98)\n(Z\ni\n))2\u2212 (Y\ni\n\u2212\u00b5\n(cid:98)\n(X\ni\n,Z\ni\n))2\nn n\ni i\nalreadyhassecondorderbiasO(||\u00b5\u2212\u00b5||2)sothatusingtheinfluencefunctionisunnecessary.\n(cid:98)\n3.2 Estimating \u03c8\n0\nWe first derive the efficient, nonparametric estimator of \u03c8 and then we discuss some issues.\n0\nRecall that U = (X,Y,Z).\nTheorem 1 Let\u03c8 = \u03c8 (\u00b5"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_6_5",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 6,
    "text": ".\n0\nRecall that U = (X,Y,Z).\nTheorem 1 Let\u03c8 = \u03c8 (\u00b5,p) = (cid:82) (cid:82) (\u00b5(x,z)\u2212\u00b5 (z))2p(x)p(z)dxdz. Theefficientinfluence\n0 0 0\nfunction is\n\u03c6(U,\u00b5,p) = (cid:82) (\u00b5(x,Z)\u2212\u00b5 (Z))2p(x)dx+ (cid:82) (\u00b5(X,z)\u2212\u00b5 (z))2p(z)dz\n0 0\np(X)p(Z)\n+2 (\u00b5(X,Z)\u2212\u00b5 (Z))(Y \u2212\u00b5(X,Z))\u22122\u03c8(p).\np(X,Z) 0\n6"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_7_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 7,
    "text": "Variable Importance\nIn particular, we have the following von Mises expansion. Let (\u00b5,p) be arbitrary and let\n(\u00b5,p) denote the true functions. Then\n(cid:90) (cid:90)\n\u03c8 (\u00b5,p) = \u03c8 (\u00b5,p)+ \u03c6(u,\u00b5,p)dP(u)+R\n0 0\nwhere the remainder R satisfies\nR = O(||p \u2212p ||\u00d7||\u03b4\u2212\u03b4||)+O(||p \u2212p ||\u00d7||\u03b4\u2212\u03b4||)+O(||p \u2212p ||\u00d7||p \u2212p ||)+O(||\u03b4\u2212\u03b4||2)\nX X Z Z X X Z Z\nand \u03b4 = \u00b5(x,z) \u2212 \u00b5 (z). Hence, if ||p \u2212 p || = o (n\u22121/4), ||p \u2212 p || = o (n\u22121/4),\n0 \u221a X X P Z Z P\n||\u03b4\u2212\u03b4|| = o (n\u22121/4) then nR = o (1).\nP P\nThe one-step estimator is\n1 "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_7_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 7,
    "text": " then nR = o (1).\nP P\nThe one-step estimator is\n1 (cid:88)\n\u03c8(cid:98)0 = \u03c8\n0\n(\u00b5\n(cid:98)\n,p\n(cid:98)\n)+ \u03c6(U\ni\n,\u00b5\n(cid:98)\n,p\n(cid:98)\n).\nn\ni\nThe estimator from solving the estimating equation is \u03c8(cid:98)=\n(2n)\u22121(cid:80)\ni\nL(U\ni\n,\u00b5\n(cid:98)\n,p\n(cid:98)\n) where\nL(U,\u00b5,p) = (cid:82) (\u00b5(x,Z)\u2212\u00b5 (Z))2p(x)dx+ (cid:82) (\u00b5(X,z)\u2212\u00b5 (z))2p(z)dz\n0 0\np(X)p(Z)\n+2 (\u00b5(X,Z)\u2212\u00b5 (Z))(Y \u2212\u00b5(X,Z)). (7)\np(X,Z) 0\nCorollary 2 Suppose that ||p\u2212p|| = o (n\u22121/4) and ||\u00b5\u2212\u00b5|| = o (n\u22121/4). When \u03c8 (cid:54)= 0,\n(cid:98) P (cid:98) "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_7_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 7,
    "text": " (n\u22121/4). When \u03c8 (cid:54)= 0,\n(cid:98) P (cid:98) P 0\nfor either of the two estimators above,\n\u221a\nn(\u03c8(cid:98)0 \u2212\u03c8\n0\n) (cid:32) N(0,\u03c32)\nwhere \u03c32 = E[\u03c62(U,\u00b5,p)].\nIn our implementation, we estimate p(x,z),p(x),p(z) with kernel density estimators.\nWe estimate integrals with respect to the densities by sampling from the kernel estimators.\nSpecifically,\nN\n1 (cid:88)\n\u00b5 (z) = \u00b5(X\u2217,z) where X\u2217,...,X\u2217 \u223c p(x).\n(cid:98)\u2217 N (cid:98) j 1 N (cid:98)\nj=1\nSimilarly, (cid:82) (\u00b5(X,z)\u2212\u00b5 (z))2p(z) is estimated by\n(ci"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_7_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 7,
    "text": ", (cid:82) (\u00b5(X,z)\u2212\u00b5 (z))2p(z) is estimated by\n(cid:98)0\n1 (cid:88)\n(\u00b5(X,Z\u2217)\u2212\u00b5 (Z\u2217))2 where Z\u2217,...,Z\u2217 \u223c p(z)\nN j (cid:98)0 j 1 N (cid:98)\nj\nand (cid:82) (\u00b5(x,Z)\u2212\u00b5 (Z))2p(x) is estimated by N\u22121(cid:80) (\u00b5(X\u2217,z)\u2212\u00b5 (z))2. Thus\n(cid:98)0 j j (cid:98)0\n\u03c8(cid:98)0 =\n2\n1\nn\n(cid:80)\ni\nL(U\ni\n,\u00b5\n(cid:98)\n,p\n(cid:98)\n)\n(cid:16) (cid:17)2 (cid:16) (cid:17)2\n= 1 (cid:80) (cid:80) \u00b5(X\u2217,Z )\u2212 1 (cid:80)N \u00b5(X\u2217,z) + 1 (cid:80) (cid:80) \u00b5(X ,Z\u2217)\u2212 1 (cid:80)N \u00b5(X ,Z\u2217)\nnN i j (cid:98) j i N s=1(cid:98) s nN i j (cid"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_7_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 7,
    "text": "\u2217)\nnN i j (cid:98) j i N s=1(cid:98) s nN i j (cid:98) i j N s=1(cid:98) i s\n(cid:16) (cid:17)\n+2 (cid:80) p (cid:98) (Xi)p (cid:98) (Zi) \u00b5(X ,Z )\u2212 1 (cid:80) \u00b5(X\u2217,Z ) (Y \u2212\u00b5(X ,Z )).\nn i p\n(cid:98)\n(Xi,Zi) (cid:98) i i N j (cid:98) j i i (cid:98) i i\n7"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_8_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 8,
    "text": "Verdinelli and Wasserman\nFinite Sample Problems. Inprinciple, \u03c8(cid:98)0 isfullyefficient. Inpractice, \u03c8(cid:98)0 canbehave\npoorly as we now explain. One of the terms in the von Mises remainder is ||\u00b5 (z)\u2212\u00b5 (z)||2.\n(cid:98)0 0\n(cid:82)\nNow \u00b5 (z) = \u00b5(x,z)p(x)dx. When X and Z are highly correlated, there will be a large\n0\nset A of x values, where there are no observed data and so \u00b5 (z) will be quite far from\nz (cid:98)0\n\u00b5 (z) because \u00b5(x,z) must suffer large bias or variance (or both) over that re"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_8_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 8,
    "text": "ffer large bias or variance (or both) over that region. This is\n0 (cid:98)\nknown as extrapolation error. For this reason we now consider alternative versions of \u03c8 .1\n0\n3.3 Estimating \u03c8\n1\nRecall that \u03c8 = E[(\u00b5(X,V)\u2212\u00b5(V))2] where V = (Z : |\u03c1(X,Z )| \u2264 t) for some t. We\n1 j j\ntake \u03c1(X,Z ) =\n(cid:80)g\n|\u03c1(X ,Z )| where \u03c1(X ,Z ) is the Pearson correlation. We use t = .5\nj i=1 i j i j\nin our examples. For simplicity we assume that the values \u03c1(X,Z ) are distinct. In this\nj\ncase P(V(cid:98) = V) \u2192 1 as n "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_8_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 8,
    "text": "istinct. In this\nj\ncase P(V(cid:98) = V) \u2192 1 as n \u2192 \u221e where V(cid:98) = (Z\nj\n: |\u03c1\n(cid:98)\n(X,Z\nj\n)| \u2264 t) and the randomness of\nV(cid:98) can be ignored asymptotically and \u03c8\n1\ncan be estimated in the same way as \u03c8\nL\nwith V(cid:98)\nreplacing Z.\n\u221a\nLemma 3 If ||\u00b5\n(cid:98)\n(x,v)\u2212\u00b5(x,v)|| = o\nP\n(n\u22121/4) and \u03c8\n1\n(cid:54)= 0 then n(\u03c8(cid:98)1 \u2212\u03c8\n1\n) (cid:32) N(0,\u03c42).\nAn alternative to removing correlated variables is to group together highly correlated\nvariables and only report the variable importance o"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_8_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 8,
    "text": "ariables and only report the variable importance of the group.\n3.4 Estimating \u03c8\n2\nConsider the partially linear model Y = \u03b2TX +f(Z)+(cid:15). Then \u00b5 (z) = (cid:82) \u00b5(x,z)p(x)dx =\n0\n\u03b2Tm +f(z) where m = E[X] and so\nX X\n(cid:90) (cid:90)\n\u03c8 \u2261 (\u00b5(x,z)\u2212\u00b5 (z))2p(x)p(z)dxdz = \u03b2T\u03a3 \u03b2\n2 0 X\nand \u03b2 = E[(Y \u2212\u00b5(Z))(X \u2212\u03bd(Z))]/E[(X \u2212\u03bd(Z))2].\nThe efficient influence function for \u03c8 is\n2\n\u03c6 = 2\u03b2T\u03a3 \u03c6 +\u03b2T((X \u2212m )(X \u2212m )T)\u03b2\u2212\u03c8\nX \u03b2 X X 2\nwhere\n(cid:110) (cid:111)\n\u03c6 = \u03a3\u22121(X \u2212\u03bd(Z)) (Y \u2212\u00b5(Z))\u2212(X \u2212\u03bd(Z))T\u03b2)\n\u03b2 X\n(cid:82)\nandweh"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_8_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 8,
    "text": " \u2212\u03bd(Z)) (Y \u2212\u00b5(Z))\u2212(X \u2212\u03bd(Z))T\u03b2)\n\u03b2 X\n(cid:82)\nandwehavethevonMisesexpansion\u03c8 (\u00b5,\u03bd,\u03b2,\u03a3 ) = \u03c8 (\u00b5,\u03bd,\u03b2,\u03a3 )+ \u03c6(u,\u00b5,\u03bd,\u03b2,\u03a3 )dP+\n2 X 2 X X\nR where the remainder R satisfies\nR = O(||\u00b5(P)\u2212\u00b5(P)||\u00d7||\u03bd(P)\u2212\u03bd(P)||)+O(||vec(\u03a3 (P))\u2212vec(\u03a3 (P))||2)\nX X\n+O(||\u03b2(P)\u2212\u03b2(P)||2)+O(||\u03b2(P)\u2212\u03b2(P)||\u00d7||vec(\u03a3 (P))\u2212vec(\u03a3 (P))||).\nX X\n1. Readersfamiliarwithcausalinferencewillrecognizethat,formally,\u00b5 (z)istheaveragetreatmenteffect\n0\nif we think of Z as a treatment and X as a confounder. But the role of treatment and confounder is\nswi"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_8_5",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 8,
    "text": "r. But the role of treatment and confounder is\nswitchedwiththetreatmentbeingthemultivariatevectorZ. Thedifficultyinestimating\u00b5 (z)whenX\n0\nandZ arehighlycorrelatedisknownastheoverlapproblemincausalinference(D\u2019Amouretal.,2021).\n8"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_9_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 9,
    "text": "Variable Importance\nWe omit the calculation of the influence function and remainder as they are standard.\nHence, if ||\u00b5(P)\u2212\u00b5(P)||\u00d7||\u03bd(P)\u2212\u03bd(P)||) = o(n\u22121/2), ||\u03b2(P)\u2212\u03b2(P)|| = o(n\u22121/4), and\n\u221a\n||vec(\u03a3 (P)) \u2212 vec(\u03a3 (P))|| = o(n\u22121/4), then nR = o(1). It is easy to verify that\nX X\n||\u03b2(P) \u2212 \u03b2(P)|| = O(||\u00b5(P) \u2212 \u00b5(P)|| \u00d7 ||\u03bd(P) \u2212 \u03bd(P)||) and so \u03c8 satisfies the double\n2\nrobustness property, namely, that the bias involves the product of two quantities. It suffices\nto estimate either \u00b5 or \u03bd accurately to get"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_9_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 9,
    "text": "ffices\nto estimate either \u00b5 or \u03bd accurately to get a consistent estimator.\nThe one-step estimator is given by\n1 (cid:88) 2 (cid:88)\n\u03c8(cid:98)2 = \u03b2(cid:98) T(X\ni\n\u2212\u00b5\n(cid:98)\n(Z\ni\n))(X\ni\n\u2212\u00b5\n(cid:98)\n(Z\ni\n))T\u03b2(cid:98)+ \u03b2(cid:98) T\u03a3(cid:98)X \u03c6\n\u03b2\n(X\ni\n,Z\ni\n)\nn n\ni i\nwhere\n(cid:40) (cid:41)\u22121\n1 (cid:88) 1 (cid:88)\n\u03b2(cid:98)= (X\ni\n\u2212\u03bd\n(cid:98)\n(Z\ni\n))(X\ni\n\u2212\u03bd\n(cid:98)\n(Z\ni\n))T (X\ni\n\u2212\u03bd\n(cid:98)\n(Z\ni\n))(Y\ni\n\u2212\u00b5\n(cid:98)\n(Z\ni\n))\nn n\ni i\nand the sums are over D .\n1\n3.5 Estimating \u03c8\n3\nConsider the partially li"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_9_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 9,
    "text": "D .\n1\n3.5 Estimating \u03c8\n3\nConsider the partially linear model with interactions:\ng h\n(cid:88)(cid:88)\nY = \u03b2TX + \u03b3 X Z +f(Z)+(cid:15).\njk j k\nj=1k=1\nDefine\n\uf8ee \uf8f9 \uf8ee \uf8f9\n\u03b2 \u03b3 \u00b7\u00b7\u00b7 \u03b3 X X Z \u00b7\u00b7\u00b7 X Z\n1 11 1h 1 1 1 1 h\n. . . . . . . .\n\u0398 = \uf8ef \uf8f0 . . . . . . . . \uf8fa \uf8fb , W = \uf8ef \uf8f0 . . . . . . . . \uf8fa \uf8fb = X Z(cid:101) T\n\u03b2 \u03b3 \u00b7\u00b7\u00b7 \u03b3 X X Z \u00b7\u00b7\u00b7 X Z\ng g1 gh g g 1 g h\nwhere Z(cid:101) T = (1,ZT). Then we can write\nY = \u03b8TW +f(Z)+(cid:15)\nwhere \u03b8 = vec(\u0398) and W = vec(W) = vec(XZ(cid:101) T) = Z(cid:101)\u2297X.\nLemma 4 We have\n(cid:40)"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_9_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 9,
    "text": "d:101) T) = Z(cid:101)\u2297X.\nLemma 4 We have\n(cid:40) (cid:41)\u22121 (cid:34)\n(cid:18) (cid:19) (cid:18) (cid:19)\n(cid:35)\n\u03b8 = E[Z(cid:101)Z(cid:101) T \u2297(X \u2212\u03bd(Z))(X \u2212\u03bd(Z))T] E Y \u2212\u00b5(Z) Z(cid:101)\u2297(X \u2212\u03bd(Z))\nand under this model, \u03c8 is equal to \u03c8 = \u03b8T\u2126\u03b8 where\n0 3\n(cid:18) 1 mT (cid:19)\n\u2126 = \u03a3\nX\n\u2297E[Z(cid:101)Z(cid:101) T] = \u03a3\nX\n\u2297\nm \u03a3 +m\nZ\nmT\n,\nZ Z Z Z\n9"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_10_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 10,
    "text": "Verdinelli and Wasserman\nm = E[Z] and \u03a3 = Var[Z]. The efficient influence function for \u03c8 is\nZ Z 3\n\u03c6 = 2\u03b8T\u2126\u03c6 +\u03b8T\u02da\u2126\u03b8\u2212\u03c8 (8)\n\u03b8 3\nwhere\n(cid:110) (cid:111)\u22121\n\u03c6 = E[R RT ] R (R \u2212RT \u03b8),\n\u03b8 XZ XZ XZ Y XZ\nR\nY\n= Y \u2212\u00b5(Z), R\nXZ\n= vec[(X \u2212\u03bd(Z))Z(cid:101) T],\n(cid:40) (cid:41) (cid:40) (cid:41)\n(cid:20) 1 mT (cid:21) (cid:20) 0 (Z \u2212m )T (cid:21)\n\u02da\u2126 = [(X\u2212m )(X\u2212m )T \u2212\u03a3 ]\u2297 Z + \u03a3 \u2297 Z ,\nX X X m \u0393 X Z \u2212m \u02da\u0393\nZ Z\n(the influence function of \u2126) \u0393 = \u03a3 +m mT, and\nZ Z Z\n\u02da\u0393 = (Z \u2212m )(Z \u2212m )T \u2212\u03a3 +m (Z \u2212m )T +(Z \u2212m )mT\nZ Z Z"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_10_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 10,
    "text": " = (Z \u2212m )(Z \u2212m )T \u2212\u03a3 +m (Z \u2212m )T +(Z \u2212m )mT\nZ Z Z Z Z Z Z\n(the influence function of \u0393).\n(cid:82)\nThen \u03c8 (u,\u03b8,\u2126) = \u03c8 (u,\u03b8,\u2126)+ \u03c6(u,\u03b8,\u2126)dP(u)+R where the remainder R satisfies\n3 3\nR = O(||\u03b8(P)\u2212\u03b8(P)||2)+O(||vec(\u2126(P))\u2212vec(\u2126(P))||2)\n+O(||\u03b8(P)\u2212\u03b8(P)||\u00d7||vec(\u2126(P))\u2212vec(\u2126(P))||).\n\u221a\nThus if ||\u03b8(P)\u2212\u03b8(P)|| = o(n\u22121/4) and ||vec(\u2126(P))\u2212vec(\u2126(P))|| = o(n\u22121/4) then nR =\no(1). Again, we have the double robustness property.\nThe sample estimate of \u03b8 is \u03b8(cid:98) = (RT\nXZ\nR\nXZ\n)\u22121RT\nXZ\nR\nY\nwhere the ith row of R\nXZ\n"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_10_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 10,
    "text": "RT\nXZ\nR\nXZ\n)\u22121RT\nXZ\nR\nY\nwhere the ith row of R\nXZ\nis\nvec[(X\ni\n\u2212 \u03bd\n(cid:98)\n(Z\ni\n))Z(cid:101)\ni\nT] and R\nY\n(i) = Y\ni\n\u2212 \u00b5\n(cid:98)\n(Z\ni\n). Let \u2126(cid:98) be the sample version of \u2126. The\none-step estimator is\n1 (cid:88) 2 (cid:88)\n\u03c8(cid:98)3 = \u03b8(cid:98) T\u03c6(cid:98)\u2126 (U\ni\n)\u03b8(cid:98)+ \u03b8(cid:98) T\u2126(cid:98)\u03c6\n\u03b8\n(U\ni\n)\nn n\ni i\nwhere the sums are over D .\n1\n3.6 Confidence Intervals\nNow we describe the construction of the confidence intervals using a method we refer to as\nt-Cross. Let \u03c8 denote a generic para"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_10_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 10,
    "text": "e refer to as\nt-Cross. Let \u03c8 denote a generic parameter. We combine two ideas: cross-fitting (Newey\nand Robins, 2018) and t-inference (Ibragimov and Mu\u00a8ller, 2010). Here are the steps:\n1. Divide the data into B disjoint sets D ,...,D ; we take B = 5 in the examples.\n1 B\n2. Estimate the nuisance functions using all the data except D\nj\nand compute \u03c8(cid:98)j on D\nj\n.\nHere, \u03c8(cid:98)j is the estimate of \u03c8 using the data in D\nj\n.\n3. Let \u03c8 =\nB\u22121(cid:80)B\nj=1\n\u03c8(cid:98)j . When \u03c8 (cid:54)= 0, each \u03c8(ci"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_10_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 10,
    "text": "0)B\nj=1\n\u03c8(cid:98)j . When \u03c8 (cid:54)= 0, each \u03c8(cid:98)j is asymptotically Normal so that \u03c8 is\nasymptotically t .\nB\u22121\n10"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_11_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 11,
    "text": "Variable Importance\n4. The confidence interval is\n\u03c8\u00b1t se\nB\u22121,\u03b1/2\nwhere se2 = (s2/B+c2/n) where s2 = (B\u22121)\u22121(cid:80)B\nj=1\n(\u03c8(cid:98)j \u2212\u03c8)2.\nThe t-method loses some efficiency because it divides the data into groups. The rate of\nconvergence does not change but the interval could be slightly larger. But the advantage is\nthat s2 is an unbiased estimate of the variance of \u03c8(cid:98)which does not depend on the accuracy\nof the estimated influence function. So we are trading efficiency for robustness.\nR"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_11_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 11,
    "text": "on. So we are trading efficiency for robustness.\nRemark: Nonparametric and semiparametric confidence intervals require fairly strict\nassumptions. For example, we need to assume fast rates for the nuisance functions. An\nalternative is to use variability intervals which are centered at the mean of the estimator\nrather than at the true value. This might be less informative but requires much weaker\nassumptions.\n4 Simulations\nIn this section, we compare the behavior of the different parameters in som"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_11_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 11,
    "text": "re the behavior of the different parameters in some synthetic\nexamples. For each example, we estimate all the parameters \u03c8 ,\u03c8 ,\u03c8 ,\u03c8 ,\u03c8 . To estimate\nL 0 1 2 3\nthe parameters we need to estimate the nuisance functions \u00b5(z) and \u03bd(z). As mentioned\nabove, we consider three approaches to estimating these functions: linear models, additive\nmodels and random forests. For the additive models we use the R package mgcv. For\nrandom forests we use the R package grf. We always use the default settings making"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_11_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 11,
    "text": "age grf. We always use the default settings making no\nattempt to tune the methods to achieve good coverage.\nExample 1. We start with a very simple scenario where Y = 2X + (cid:15), (cid:15) \u223c N(0,1),\nZ = \u03b4X +\u03be, \u03be \u223c N(0,1), and (Z ,...,Z ) \u223c N(0,I). Figure 2 shows the coverage as a\n1 2 5\nfunction of the correlation between X and Z . As expected, \u03c8 has poor coverage as the\n1 L\ncorrelation increases. The parameter \u03c8 partially corrects the correlation distortion while\n0\nthe other parameters do a muc"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_11_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 11,
    "text": "n distortion while\n0\nthe other parameters do a much better job. The coverage for \u03c8 decreases as correlation\n1\nincreases. However, when the correlation is large enough, it becomes easier to identify\ncorrelated variables and then the coverage increases. The true values of the parameters are\nplotted in Figure 1.\nExamples 2-5. Now we consider four multivariate examples. In each case, n = 10,000,\nh = 5 and (cid:15) \u223c N(0,1). The distributions are defined as follows:\nExample 2: X is standard Normal, Z"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_11_5",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 11,
    "text": "ned as follows:\nExample 2: X is standard Normal, Z = X +N(0,.42), (Z ,...,Z ) is standard multi-\n1 2 h\nvariate Normal. The regression function is Y = 2X3+(cid:15). Hence Cor(X ,Z) = .93.\n1\nExample 3: Here, Z \u223c N(0,I), X = 2Z +(cid:15) , X = 2Z +(cid:15) , Y = 2X X +(cid:15) where\n1 1 1 2 2 2 1 2\n(cid:15),(cid:15) ,(cid:15) \u223c N(0,1). Hence Cor(X ,Z ) = Cor(X ,Z ) = .89.\n1 2 1 1 2 2\nExample 4: LetX \u223c Unif(\u22121,1),Z \u223c Unif(\u22121,1),andY = X2(X+(7/5))+(25/9)Z2+\n(cid:15). This example is from Williamson e"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_11_6",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 11,
    "text": "/9)Z2+\n(cid:15). This example is from Williamson et al. (2021). Our coverage for \u03c8 is similar but\nL\nslightlylessthanthatinWilliamsonetal.(2021)butweareusingadifferentnonparametric\nestimator. In this case, X and Z are uncorrelated.\nExample 5: X \u223c N(0,1), Z = X +N(0,.42) (Z ,...,Z ) \u223c N(0,I) and Y = 2X2 +\n1 2 d\nXZ +(cid:15). In this case Cor(X,Z ) = .93\n1 1\n11"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_12_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 12,
    "text": "Verdinelli and Wasserman\n0.0 0.2 0.4 0.6 0.8 1.0\n4\n3\n2\n1\n0\nr\ny\nFigure 1: This plot shows the true values of the parameters in Example 1 as a function of\nthe correlation \u03c1 between X and Z. The top red line is \u03c8 = \u03c8 = \u03c8 . The green\n0 2 3\nlineis\u03c8 . Thebluelineis\u03c8 whichequals\u03c8 for\u03c1 < .5andequals\u03c8 for\u03c1 > .5.\nL 1 L 0\nIn examples 2,4 and 5, we replaced X with orthogonal polynomials b (X),b (X),b (X).\n1 2 3\nThe results from 100 simulations are summarized in Figures 2 and 3 and in Table 2.\nThe standard e"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_12_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 12,
    "text": " in Figures 2 and 3 and in Table 2.\nThe standard error of the coverage is 0.03. Figure 2 shows how often the confidence interval\ncontains the target parameter \u03c8 as a function of the correlation which varies from 0 to 1.\n0\nIn other words, we treat \u03c8 = \u03b22 = 4 as the truth and we evaluate how well an interval\n0\nbased on estimating \u03c8 covers \u03c8 . They all cover well except \u03c8 and \u03c8 . This is to be\nj 0 L 1\nexpected as \u03c8 = \u03c8 = \u03c8 in this example. However, \u03c8 decreases as a function of the\n0 2 3 L\ncorrelati"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_12_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 12,
    "text": "\u03c8 decreases as a function of the\n0 2 3 L\ncorrelation. Infact,weevaluatedhowoftentheintervalfor\u03c8 containsthetruevalueof\u03c8 .\nL L\nItturnsoutthatthecoverageoftheintervalbasedon\u03c8 doescover\u03c8 atthenominallevel\nL L\n(although,aswithmanyexamples,theforestbasedmethodtendstosometimesundercover).\nThe coverage for \u03c8 goes down and then up because Z , which is correlated with X, gets\n1 1\nremoved when the correlation is large enough. Essentially, when the correlation is less than\n.5, \u03c8 = \u03c8 but after that, Z is re"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_12_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 12,
    "text": "ion is less than\n.5, \u03c8 = \u03c8 but after that, Z is removed and \u03c8 = \u03c8 . This shows the inherent instability\n1 L 1 1 0\nof trying to remove correlated variables.\nFigure3showstheaverageoftheleftandrightendpointsoftheconfidenceintervals. The\nvertical line marks our target which is \u03c8 . The first thing to notice is that no method does\n0\nuniformly well. Inferences for \u03c8 are mostly pretty good, but the others are not and this is\n3\nto be expected. The coverage of \u03c8 is poor because it is not targeting the rig"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_12_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 12,
    "text": "e of \u03c8 is poor because it is not targeting the right parameters.\nL\nSimilarly for \u03c8 . The poor coverage of \u03c8 in some cases is due to the difficulty of estimating\n1 0\nthe parameter nonparametrically. \u03c8 does not include interactions and does poorly when\n2\nthere are interactions. The random forest method has a tendency to undercover. However,\nwhat is not shown here, is that each method does cover its own target at the nominal level.\nEstimating variable importance well is surprisingly difficult. Gene"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_12_5",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 12,
    "text": "le importance well is surprisingly difficult. Generally, we find that \u03c8\n3\nworks best. However, it does poorly in two cases: in Example 5, with linear regressions,\nand in Example 2 using random forests. \u03c8 rarely does well. Apparently, the functional is\n0\n12"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_13_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 13,
    "text": "Variable Importance\n0.1\n8.0\n6.0\n4.0\n2.0\n0.0\nCorrelation\negarevoC\ny\nL\ny 0\ny\n1\ny 2\ny\n3\n0.1\n8.0\n6.0\n4.0\n2.0\n0.0\nCorrelation\negarevoC\ny\nL\ny 0\ny\n1\ny 2\ny\n3\n0.1\n8.0\n6.0\n4.0\n2.0\n0.0\nCorrelation\negarevoC\ny\nL\ny 0\ny\n1\ny 2\ny\n3\nFigure 2: Example 1: Coverage as a function of correlation. Top: linear. Middle: additive.\nBottom: forests.\ntoo difficult to estimate nonparametrically. \u03c8 works well in a few cases, but is not reliable\n1\nenough in general. Similar behavior occurs for \u03c8 . Except for a few cases, \u03c8 neve"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_13_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 13,
    "text": "vior occurs for \u03c8 . Except for a few cases, \u03c8 never does\n2 L\nwell. This is not unexpected due to the correlation distortion.\nHowever,itshouldbenotedthatthesemethodsarealldoingwellinthesenseofcovering\nthe value of \u03c8 in the projected model at the nominal level. For example, when using linear\nmodels for \u00b5 and \u03bd, we are really estimating the value of \u03c8 for the projection of the\ndistribution onto the space of linear models. The parameter estimate may capture useful\ninformation even if it is not estim"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_13_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 13,
    "text": "capture useful\ninformation even if it is not estimating \u03c8 .\n0\n13"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_14_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 14,
    "text": "Verdinelli and Wasserman\nl ll l ll l ll ll l\nlll l ll ll l ll l\nll l l ll l l ll ll\nl ll l l l l l l l l\nll ll l l l l llll\n0 1 2 3 4 5 0 1 2 3 4 5 6\ny\nl ll lll ll ll l l 3\ny\nl ll lll ll ll l l 2\ny\nll ll\nl l\nll\nl\nlll 1\nl ll l l ll l l l y 0\nll ll l l llll l l y L\n1.0 1.2 1.4 1.6 1.8 2.0 0 1 2 3 4\nFigure 3: The average of the left and right endpoints of the confidence intervals over 100\nsimulations for Examples 2,3,4,5. The vertical line is \u03c8 . The plot shows how the\n0\nconfidence intervals of eac"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_14_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 14,
    "text": "e plot shows how the\n0\nconfidence intervals of each parameter compare to the true value of \u03c8 . Top left\n0\nis Example 2. Top right is Example 3. Bottom left is Example 4. Bottom middle\nis Example 5. The bottom right shows the legend for all the plots. In each panel,\nthe groups of three line segments correspond to the three different models: the top\nis based on linear models, the middle is based on additive models and the bottom\nis based on random forests.\n5 Other Issues\nIn this section we discuss"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_14_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 14,
    "text": "forests.\n5 Other Issues\nIn this section we discuss two further topics: other variable importance parameters, and\nShapley values.\n14"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_15_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 15,
    "text": "Variable Importance\nLinear Additive Forest\n\u03c8 \u03c8 \u03c8 \u03c8 \u03c8 \u03c8 \u03c8 \u03c8 \u03c8 \u03c8 \u03c8 \u03c8 \u03c8 \u03c8 \u03c8\nL 0 1 2 3 L 0 1 2 3 L 0 1 2 3\nExample 2 1 0.84 1 1 1.00 0.00 0.79 1.00 1.00 0.97 0.00 0.75 1.00 0.99 0.30\nExample 3 0 0.00 0 0 0.99 0.00 0.88 0.00 0.00 0.92 0.00 0.00 0.00 0.00 0.91\nExample 4 1 0.87 1 1 1.00 0.98 0.20 0.98 0.98 0.98 0.00 0.21 0.00 0.86 0.85\nExample 5 0 0.01 0 0 0.00 0.00 0.83 0.00 0.00 0.85 0.00 0.05 0.00 0.00 1.00\nTable 2: Coverage results for Examples 2,3,4 and 5. The standard error on the estimates\ncover"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_15_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 15,
    "text": "4 and 5. The standard error on the estimates\ncoverage is 0.03. Overall, \u03c8 performs best in these examples. But when linear\n3\nregressions are used, \u03c8 fails. For random forests, \u03c8 does poorly in Example 2.\n3 3\nThe most robust behavior is given by the additive model.\n5.1 Other Parameters\nWe have focused on LOCO in this paper but there are many other variable importance\nparameters all of which can be estimated in a manner similar to the methods in this paper.\nSamarov (1993) suggested \u03c8 = (cid:82) (\u2202"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_15_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 15,
    "text": "is paper.\nSamarov (1993) suggested \u03c8 = (cid:82) (\u2202\u00b5(x,z)/\u2202x)T(\u2202\u00b5(x,z)/\u2202x)dP. This parameter is not\nsubject to correlation distortion. Estimating derivatives can be difficult but in the semi-\nparametric case, \u03c8 takes a simple form. In the partially linear model we have \u03c8 = ||\u03b2||2\nand in the partially linear model with interactions (5) we have\n\u03c8 = ||\u03b2||2+2\u03b2GTm +GT\u03a3 G\nZ Z\nwhere G = \u03b3 .\njk jk\nAnother parameter is inspired by causal inference. If we viewed X as a treatment and\nZ as confounding variab"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_15_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 15,
    "text": "iewed X as a treatment and\nZ as confounding variables, then (under some conditions) the causal effect, that is the\n(cid:82)\nmean of Y had X been set to x, is given by Robins\u2019 g-formula g(x) = \u00b5(x,z)dP(z).\nWe could then define \u03c8 as the variance Var[g(X)] or the average squared derivative of\n(cid:82) (\u2202g(x)/\u2202x)T(\u2202g(x)/\u2202x)dP. These parameters do not suffer from correlation distortion.\nNow Var[g(X)] equals \u03b2T\u03a3 \u03b2 under the partially linear model and is (\u03b2+\u0393m )T\u03a3 (\u03b2+\nX Z X\n\u0393m ) under the partially lin"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_15_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 15,
    "text": "s (\u03b2+\u0393m )T\u03a3 (\u03b2+\nX Z X\n\u0393m ) under the partially linear model with interactions. Using the derivative, in the\nZ\npartially linear model we get \u03c8 = ||\u03b2||2 and in partially linear model with interactions we\nget\n\u03c8 = ||\u03b2||2+2\u03b2\u0393Tm +\u0393Tm mT\u0393.\nZ Z Z\nThe nonparametric partial correlation is defined by\nE[(Y \u2212\u00b5(Z))(X \u2212\u03bd(Z))]\n\u03c1 = .\n(cid:112)E(Y \u2212\u00b5(Z))2E(X \u2212\u03bd(Z))2\nUnder p we get a decorrelated version\n0\nE [(Y \u2212\u00b5 (Z))(X \u2212\u03bd (Z))] (cid:82) (cid:82) (\u00b5(x,z)\u2212\u00b5 (z))(x\u2212m )p(x)p(z)dxdz\n0 0 0 0 X\n\u03c1 = = .\n0 (cid:112)E 0 "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_15_5",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 15,
    "text": "\u2212m )p(x)p(z)dxdz\n0 0 0 0 X\n\u03c1 = = .\n0 (cid:112)E 0 (Y \u2212\u00b5 0 (Z))2E 0 (X \u2212\u03bd 0 (Z))2 \u03c3 (cid:113) (cid:82) (cid:82) (cid:82) (y\u2212\u00b5 (z))2p(y|x,z)p(x)p(z)\nX 0\nMore detail about \u03c1 are in the appendix.\n0\n15"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_16_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 16,
    "text": "Verdinelli and Wasserman\n5.2 Shapley Values\nAmethodfordefiningvariableimportancethathasattractedmuchattentionlatelyisbased\non Shapley values (Messalas et al., 2019; Aas et al., 2019; Lundberg and Lee, 2016; Covert\net al., 2020; Fryer et al., 2020; Covert and Lee, 2020; Israeli, 2007; Mase et al., 2019; B\u00b4enard\net al., 2021). This is an idea from game theory where the goal is to define the importance of\neach player in a cooperative game. While Shapley values can be useful in some settings, for\nex"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_16_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 16,
    "text": "pley values can be useful in some settings, for\nexample, computerexperiments(OwenandPrieur,2017)weargueherethatShapleyvalues\ndo not solve the decorrelation issue and LOCO or decorrelated LOCO may be preferable\nfor routine regression problems. However, this is an active area of research and the issue is\nfar from settled. Shapley values may indeed have some other advantages.\nThe Shapley value is defined as follows. Suppose we have covariates (Z ,...,Z ) and\n1 d\nthat we want to measure the importan"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_16_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 16,
    "text": ".,Z ) and\n1 d\nthat we want to measure the importance of Z . For any subset S \u2282 {1,...,d} let Z =\nj S\n(Z : j \u2208 S) and let \u00b5(S) = E[Y|Z ]. The Shapley value for Z is\nj S j\n1 (cid:88)\ns = [V(S+(\u03c0))\u2212V(S (\u03c0))]\nj d! j j\n\u03c0\nwhere the sum is over of permutations of (Z ,...,Z ), S (\u03c0) denotes all variables before Z\n1 d j j\nin permutation \u03c0, S+(\u03c0) = {S (\u03c0) (cid:83) {j}} and V(S) is some measure of fit the regression\nj j\nmodel with variables S. If V(S) = \u2212E[(Y \u2212\u00b5(S))2], then\n1 (cid:88)\ns = E[(\u00b5(S )\u2212\u00b5(S+))2]"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_16_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 16,
    "text": "(Y \u2212\u00b5(S))2], then\n1 (cid:88)\ns = E[(\u00b5(S )\u2212\u00b5(S+))2].\nj d! j j\n\u03c0\nThis is just the LOCO parameter averaged over all possible submodels. The Shapley value\nfor a group of variables can be defined similarly.\nIt is clear that this parameter is difficult to compute and inference, while possible\n(Williamson and Feng, 2020) is very challenging. The appeal of the Shapley value is that\nit has the following nice properties:\n(A1): (cid:80) s = E[(Y \u2212\u00b5(Z))2].\nj j\n(A2) If E[(Y \u2212\u00b5(S (cid:83) {i}))2] = E[(Y \u2212\u00b5(S "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_16_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 16,
    "text": " j\n(A2) If E[(Y \u2212\u00b5(S (cid:83) {i}))2] = E[(Y \u2212\u00b5(S (cid:83) {j}))2] for every S not containing i or j,\nthen s = s .\ni j\n(A3)Ifwetreat{Z ,Z }asonevariable,thenitsShapleyvalues satisfiess = s +s .\nj k jk jk j k\n(A4) If E[(Y \u2212\u00b5(S (cid:83) {j}))2] = E[(Y \u2212\u00b5(S))2] for all S then s = 0.\nj\nHowever,weseetwoproblemswithShapleyvaluesappliedtoregression. First,itdefines\nvariable importance with respect to all submodels. But most of those submodels are not\nof interest. Indeed, most of them would be a bad fit"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_16_5",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 16,
    "text": " interest. Indeed, most of them would be a bad fit to the data and are not relevant.\nSo it is not clear why we should involve them in any definition of variable importance or\nin the axioms. (An intriguing idea might be to weight the submodels according to their\npredictive value). Second, they succumb to correlation distortion. To see this, suppose that\nY = \u03b2Z +(cid:15), that the Z \u2019s have variance 1 and that they are perfectly correlated, that is,\n1 j\nP(Z = Z ) = 1 for every j and k. The Shapley"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_16_6",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 16,
    "text": ",\n1 j\nP(Z = Z ) = 1 for every j and k. The Shapley value for Z turns out to be s = \u03b22/d\nj k 1 1\nwhich is close to 0 when d is large. In contrast, \u03c8 = \u03b22, which seems more a appropriate.\n0\n16"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_17_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 17,
    "text": "Variable Importance\nThe confidence interval for \u03c8 would have infinite length since the design is singular which\n0\nalso seems appropriate, since estimating the importance of a single variable among a set of\nperfectly correlated variables should be an impossible inferential task. For these reasons,\nwe feel that decorrelated LOCO may have some advantages over Shapley values.\n6 Conclusion\nWe showed that correlation distortion can be removed from LOCO by modifying the defini-\ntionappropriately. Thisl"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_17_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 17,
    "text": " by modifying the defini-\ntionappropriately. Thisleadstotheparameter\u03c8 . Aswehaveseen,gettingvalidinferences\n0\nfor \u03c8 nonparametrically is difficult even in fairly simple examples. This is mainly because\n0\n(cid:82)\nthe parameter involves the function \u00b5 (z) = \u00b5(x,z)p(x)dx which requires estimating\n0\n\u00b5(x,z) in regions where there is little data due to the dependence between x and z. The\neasiest remedy is to remove correlated variables as we did for \u03c8 but this led to disappoint-\n1\ning behavior. The o"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_17_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 17,
    "text": " but this led to disappoint-\n1\ning behavior. The other remedy was to use a semiparametric model for \u00b5(x,z) which led\nto \u03c8 and \u03c8 . This appears to be the best approach. We emphasize that even when the\n2 3\ncoverage for \u03c8 and \u03c8 is low, (when the semiparametric model is misspecified), these pa-\n2 3\nrameters are still useful if we interpret them as projections. For example, \u03c8 measures the\n2\nvariable importance of X in the regression function of the form \u03b2x+f(z) that best approx-\nimates \u00b5(x,z). In the"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_17_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 17,
    "text": "rm \u03b2x+f(z) that best approx-\nimates \u00b5(x,z). In the sense \u03c8 still captures part of the variable importance. Graham and\n2\nde Xavier Pinto (2021) discuss in detail the interpretation of misspecified semiparametric\nmodels.\nWe only dealt with low dimensional models. The methods extend to high dimensional\nmodels by using the usual sparsity based estimators for the nuisance functions \u00b5(z) and\n\u03bd(z). We plan to explore this in future work.\nFinally, we briefly discussed the role of Shapley values which ha"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_17_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 17,
    "text": "efly discussed the role of Shapley values which have become popular in\nthe literature on variable importance. The motivation for using Shapley values appears to\nbe that they might alleviate correlation distortion. Indeed, if the variables were indepen-\ndent, Shapley values would probably not be considered. But we argued that they do not\nadequately address the problem. Instead, we believe that some form of decorrelation might\nbe preferred.\nAcknowledgments\nWe would like to acknowledge two referees"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_17_5",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 17,
    "text": "edgments\nWe would like to acknowledge two referees, whose comments helped to improve the paper.\n7 Appendix\nIn this appendix we have proofs and details for a few other parameters.\n17"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_18_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 18,
    "text": "Verdinelli and Wasserman\n7.1 Proofs\nTheorem 1. Let \u03c8 (\u00b5,p) = (cid:82) (cid:82) (\u00b5(x,z) \u2212 \u00b5 (z))2p(x)p(z)dxdz. The efficient influence\n0 0\nfunction is\n(cid:90) (cid:90)\n\u03c6(X,Y,Z,\u00b5,p) = (\u00b5(x,Z)\u2212\u00b5 (Z))2p(x)dx+ (\u00b5(X,z)\u2212\u00b5 (z))2p(z)dz\n0 0\np(X)p(Z)\n+ 2 (\u00b5(X,Z)\u2212\u00b5 (Z))(Y \u2212\u00b5(X,Z))\u22122\u03c8(p).\n0\np(X,Z)\nIn particular, we have the following von Mises expansion\n(cid:90) (cid:90)\n\u03c8 (\u00b5,p) = \u03c8 (\u00b5,p)+ \u03c6(x,y,z,\u00b5,p)dP(x,y,z)+R\n0 0\nwhere the remainder R satisfies\n||R|| = O(||p(x,z)\u2212p(x,z)||2)+O(||\u00b5(x,z)\u2212\u00b5(x,z)||2)\n+ O(||p"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_18_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 18,
    "text": "(||p(x,z)\u2212p(x,z)||2)+O(||\u00b5(x,z)\u2212\u00b5(x,z)||2)\n+ O(||p(x,z)\u2212p(x,z)||\u00d7||\u00b5(x,z)\u2212\u00b5(x,z)||).\nProof. To show that \u03c6(X,Y,Z,\u00b5,p) is the efficient influence function we verify that\n\u03c6(X,Y,Z,\u00b5,p) is the Gateuax derivative of \u03c8 and that it has the claimed second order\nremainder. We will use the symbol (cid:48) to denote the Gateuax derivative defined by\n\u03c8 ((1\u2212(cid:15))P +(cid:15)\u03b4 )\u2212\u03c8 (P)\n0 XYZ 0\nlim\n(cid:15)\u21920 (cid:15)\nwhere\u03b4 isapointmassat(X,Y,Z). Also,let\u03b4 denoteapointmassatX,\u03b4 apoint\nXYZ X XY\nmass at (X,Y)"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_18_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 18,
    "text": "enoteapointmassatX,\u03b4 apoint\nXYZ X XY\nmass at (X,Y) etc. Let w(x,z) = p(x)p(z). Then \u03c8 = (cid:82) (cid:82) (\u00b5(x,z)\u2212\u00b5 (z))2w(x,z)dxdz.\n0 0\nNow\n(cid:90) (cid:90) (cid:90) (cid:90)\n\u03c8(cid:48) = (\u00b5(x,z)\u2212\u00b5 (z))2w(cid:48)(x,z)dxdz+2 w(x,z)(\u00b5(x,z)\u2212\u00b5 (z))(\u00b5(cid:48)(x,z)\u2212\u00b5(cid:48)(z))dxdz\n0 0 0\nFirst, note that w(cid:48)(x,z) = p(x)(\u03b4 (z)\u2212p(z))+p(z)(\u03b4 (x)\u2212p(x)). Next\nZ X\n(cid:90) (cid:90)\np(x,y,z)\n\u00b5(x,z) = yp(y|x,z)dy = y dy\np(x,z)\nand\n(cid:90)\np(x,y,z)+(cid:15)(\u03b4 \u2212p(x,y,z))\nXYZ\n\u00b5 (x,z) = y dy\n(cid:15)\np(x"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_18_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 18,
    "text": "d:15)(\u03b4 \u2212p(x,y,z))\nXYZ\n\u00b5 (x,z) = y dy\n(cid:15)\np(x,z)+(cid:15)(\u03b4 \u2212p(x,z))\nXZ\nSo\n(cid:40) (cid:41)\n(cid:90)\np(x,z)(\u03b4 \u2212p(x,y,z))\u2212p(x,y,z)(\u03b4 \u2212p(x,z))\n\u00b5(cid:48)(x,z) = y XYZ XZ dy\np2(x,z)\nY \u00b5(x,z)I(x = X,z = Z)\n= I(x = X,z = Z)\u2212\u00b5(x,z)\u2212 +\u00b5(x,z)\np(X,Z) p(x,z)\n(Y \u2212\u00b5(x,z))\n= I(x = X,z = Z)\np(x,z)\n18"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_19_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 19,
    "text": "Variable Importance\n(cid:82)\nNow \u00b5 (z) = \u00b5(x,z)p(x) dx so\n0\n(cid:90) (cid:90)\n\u00b5(cid:48)(z) = \u00b5(x,z)(\u03b4 (x)\u2212p(x))dx+ p(x)\u00b5(cid:48)(x,z)dx\n0 X\n(Y \u2212\u00b5(X,z))p(X)\n= \u00b5(X,z)\u2212\u00b5 (z)+ I(z = Z)\n0\np(X,z)\nso\n(cid:90) (cid:90)\n\u03c6(X,Y,Z,\u00b5,p) = (\u00b5(x,Z)\u2212\u00b5 (Z))2p(x)dx\u2212\u03c8+ (\u00b5(X,z)\u2212\u00b5 (z))2p(z)dz\u2212\u03c8\n0 0\n(Y \u2212\u00b5(X,Z))\n+ 2 w(X,Z)(\u00b5(X,Z)\u2212\u00b5 (Z))\n0\np(X,Z)\n(cid:90) (cid:90)\n\u2212 2 w(x,z)(\u00b5(x,z)\u2212\u00b5 (z))(\u00b5(X,z)\u2212\u00b5 (z))dxdz\n0 0\n(cid:90)\n(Y \u2212\u00b5(X,Z))p(X)\n\u2212 2 w(x,Z)(\u00b5(x,Z)\u2212\u00b5 (Z))dx\n0\np(X,Z)\n(cid:90) (cid:90)\n= (\u00b5(x,Z)\u2212\u00b5 (Z))2p(x)dx+ (\u00b5(X,z"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_19_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 19,
    "text": "\n(cid:90) (cid:90)\n= (\u00b5(x,Z)\u2212\u00b5 (Z))2p(x)dx+ (\u00b5(X,z)\u2212\u00b5 (z))2p(z)dz\u22122\u03c8\n0 0\n(Y \u2212\u00b5(X,Z))\n+ 2 w(X,Z)(\u00b5(X,Z)\u2212\u00b5 (Z))\n0\np(X,Z)\n(cid:90) (cid:90)\n\u2212 2 w(x,z)(\u00b5(x,z)\u2212\u00b5 (z))(\u00b5(X,z)\u2212\u00b5 (z))dxdz\n0 0\n(cid:90)\n(Y \u2212\u00b5(X,Z))p(X)p(Z)\n\u2212 2 p(x)(\u00b5(x,Z)\u2212\u00b5 (Z))dx\n0\np(X,Z)\n(cid:90) (cid:90)\n= (\u00b5(x,Z)\u2212\u00b5 (Z))2p(x)dx+ (\u00b5(X,z)\u2212\u00b5 (z))2p(z)dz\n0 0\np(X)p(Z)\n+ 2 (\u00b5(X,Z)\u2212\u00b5 (Z))(Y \u2212\u00b5(X,Z))\u22122\u03c8(p)\n0\np(X,Z)\nwhich has the claimed form.\nNow we consider the von Mises remainder. The remainder at (p,\u00b5) in the direction of\n(p,\u00b5) is\n(cid:90)\n"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_19_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 19,
    "text": "er at (p,\u00b5) in the direction of\n(p,\u00b5) is\n(cid:90)\nR = \u03c8(p,\u00b5)\u2212\u03c8(p,\u00b5)\u2212 \u03c6(u,\u00b5,p)dP(u).\nNow\n\u2212R = \u03c8(p,\u00b5)\u2212\u03c8(p,\u00b5)\n(cid:90) (cid:90) (cid:90) (cid:90)\n+ p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))2 dxdz+ p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))2 dxdz\n0 0\n(cid:90) (cid:90) (cid:90)\np(x)p(z)\n+ 2 p(x,y,z) (\u00b5(x,z)\u2212\u00b5 (z))(y\u2212\u00b5(x,z)) dxdydz\u22122\u03c8(p)\np(x,z) 0\n(cid:90) (cid:90) (cid:90) (cid:90)\n= p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))2 dxdz+ p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))2 dxdz\n0 0\n(cid:90) (cid:90) (cid:90)\np(x)p(z)\n+ 2 p(x,y,z) (\u00b5(x,z)\u2212\u00b5 (z))(y\u2212\u00b5(x,z)) dxdydz\u2212\u03c8(p,\u00b5)\u2212\u03c8(p,\u00b5)\n"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_19_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 19,
    "text": ",z) (\u00b5(x,z)\u2212\u00b5 (z))(y\u2212\u00b5(x,z)) dxdydz\u2212\u03c8(p,\u00b5)\u2212\u03c8(p,\u00b5)\np(x,z) 0\n19"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_20_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 20,
    "text": "Verdinelli and Wasserman\n(cid:90) (cid:90) (cid:90) (cid:90)\n= p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))2 dxdz+ p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))2 dxdz\n0 0\n(cid:90) (cid:90)\np(x)p(z)\n+ 2 p(x,z) (\u00b5(x,z)\u2212\u00b5 (z))(\u00b5(x,z)\u2212\u00b5(x,z)) dxdz\u2212\u03c8(p,\u00b5)\u2212\u03c8(p,\u00b5)\np(x,z) 0\n(cid:90) (cid:90) (cid:90) (cid:90)\n= p(x) p(z) (\u00b5(x,z)\u2212\u00b5 (z))2 dxdz+ p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))2 dxdz\n0 0\n(cid:90) (cid:90) (cid:90) (cid:90)\n\u2212 p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))2 dxdz\u2212 p(x)p(z)(\u00b5\u2212\u00b5 )2 dxdz+2S\n0 0\nwhere\n(cid:90) (cid:90)\nS = 2 (p(x,z)\u2212p(x,z))(\u00b5(x,z)\u2212\u00b5(x,z))p(x)p(z)(\u00b5(x,z)\u2212\u00b5 ("
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_20_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 20,
    "text": " (p(x,z)\u2212p(x,z))(\u00b5(x,z)\u2212\u00b5(x,z))p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))dxdz.\n0\n(cid:82) (cid:82)\nNow consider the term m = p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))(\u00b5(x,z)\u2212\u00b5(x,z)) dxdz. We have\n0\n(cid:90) (cid:90)\nm = p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))(\u00b5(x,z)\u2212\u00b5(x,z)) dxdz\n0\n(cid:90) (cid:90)\n= p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))(\u00b5(x,z)\u2212\u00b5 (z)+\u00b5 (z)\u2212\u00b5 (z)+\u00b5 (z)\u2212\u00b5(x,z)) dxdz\n0 0 0 0 0\n(cid:90) (cid:90)\n= p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))(\u00b5(x,z)\u2212\u00b5 (z)) dxdz\n0 0\n(cid:90) (cid:90)\n+ p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))(\u00b5 (z)\u2212\u00b5 (z)) dxdz\n0 0 0\n(cid:90) (cid:90)\n+ p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))(\u00b5"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_20_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 20,
    "text": "0 0 0\n(cid:90) (cid:90)\n+ p(x)p(z)(\u00b5(x,z)\u2212\u00b5 (z))(\u00b5 (z)\u2212\u00b5(x,z)) dxdz\n0 0\n(cid:90) (cid:90) \u221a (cid:112) (cid:90) (cid:90)\n= p(x)p(z) \u03b4 \u03b4 dxdz+0\u2212 p(x)p(z)\u03b4 dxdz,\nwhere \u03b4 = \u00b5(x,z)\u2212\u00b5 (z) and \u03b4 = \u00b5(x,z)\u2212\u00b5 (z). Hence,\n0 0\n(cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) \u221a (cid:112)\n\u2212R = p(x)p(z)\u03b4 dxdz+ p(x)p(z)\u03b4 dxdz+2 p(x)p(z) \u03b4 \u03b4 dxdz\n(cid:90) (cid:90) (cid:90) (cid:90)\n\u2212 2 p(x)p(z)\u03b4 dxdz\u2212 p(x)p(z)\u03b4 dxdz\n(cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90)\n\u2212 p(x)p(z)\u03b4 dxdz\u2212 p(x)p(z)\u03b4 dxdz+ p(x)"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_20_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 20,
    "text": "0) (cid:90)\n\u2212 p(x)p(z)\u03b4 dxdz\u2212 p(x)p(z)\u03b4 dxdz+ p(x)p(z)\u03b4 dxdz\n(cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:112) \u221a\n= p(x)p(z)\u03b4 dxdz+ p(x)p(z)\u03b4 dxdz\u2212 p(x)p(z)( \u03b4\u2212 \u03b4)2 dxdz\n(cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90)\n\u2212 2 p(x)p(z)\u03b4 dxdz\u2212 p(x)p(z)\u03b4 dxdz+ p(x)p(z)\u03b4 dxdz\n(cid:90) (cid:90) (cid:90) (cid:90)\n= (p(x)\u2212p(x))p(z)(\u03b4\u2212\u03b4) dxdz+ p(x)(p(z)\u2212p(z))(\u03b4\u2212\u03b4) dxdz\n(cid:90) (cid:90) (cid:90) (cid:90) (cid:112) \u221a\n+ (p(x)\u2212p(x))(p(z)\u2212p(z))\u03b4 dxdz\u2212 p(x)p(z)( \u03b4\u2212 \u03b4)2 dxdz.\nAnd hence\n||R|| = "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_20_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 20,
    "text": ")\u03b4 dxdz\u2212 p(x)p(z)( \u03b4\u2212 \u03b4)2 dxdz.\nAnd hence\n||R|| = O(||p(x)\u2212p(x)|| ||\u03b4\u2212\u03b4||)+O(||p(z)\u2212p(z)|| ||\u03b4\u2212\u03b4||)\n20"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_21_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 21,
    "text": "Variable Importance\n+ O(||p(x)\u2212p(x)|| ||p(z)\u2212p(z)||)+O(||\u03b4\u2212\u03b4||2)\n= O(||p(x,z)\u2212p(x,z)||2)+O(||\u00b5(x,z)\u2212\u00b5(x,z)||2)\n+ O(||p(x,z)\u2212p(x,z)||\u00d7||\u00b5(x,z)\u2212\u00b5(x,z)||). (cid:3)\nLemma 3. Suppose that ||\u00b5(x,v)\u2212\u00b5(x,v)|| = o (n\u22121/4). Then, when \u03c8 (cid:54)= 0, we have\n\u221a (cid:98) P 1\nthat n(\u03c8(cid:98)1 \u2212\u03c8\n1\n) (cid:32) N(0,\u03c42) for some \u03c42.\nProof We have\nY\ni\n\u2212\u00b5\n(cid:98)\n(V(cid:98)i ) = (Y\ni\n\u2212\u00b5(V\ni\n))+(\u00b5(V\ni\n)\u2212\u00b5(V(cid:98)i ))+(\u00b5(V(cid:98)i )\u2212\u00b5\n(cid:98)\n(V(cid:98)i ))\n= (Y\ni\n\u2212\u00b5(V\ni\n))\u2212(V(cid:98)i \u2212V\ni\n)T\u2207\u00b5(V(cid:101)i )+("
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_21_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 21,
    "text": "\ni\n\u2212\u00b5(V\ni\n))\u2212(V(cid:98)i \u2212V\ni\n)T\u2207\u00b5(V(cid:101)i )+(\u00b5(V(cid:98)i )\u2212\u00b5\n(cid:98)\n(V(cid:98)i ))\nfor some V(cid:101)i between V\ni\nand V(cid:98)i . Squaring, summing and letting (cid:15)\ni\n= Y\ni\n\u2212\u00b5(V\ni\n),\n1 (cid:88) 1 (cid:88) 1 (cid:88) 1 (cid:88)\nn\n(Y\ni\n\u2212\u00b5\n(cid:98)\n(V(cid:98)i ))2 =\nn\n(cid:15)2\ni\n+\nn\n((V(cid:98)i \u2212V\ni\n)T\u2207\u00b5(V(cid:101)i ))2+\nn\n(\u00b5(V(cid:98)i )\u2212\u00b5\n(cid:98)\n(V(cid:98)i ))\ni i i i\n2 (cid:88) 2 (cid:88)\n+ (cid:15)\ni\n(V(cid:98)i \u2212V\ni\n)T\u2207\u00b5(V(cid:101)i )2+ (cid:15)\ni\n(\u00b5(V(cid:98)i )\u2212\u00b5\n(cid:98)\n"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_21_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 21,
    "text": "d:101)i )2+ (cid:15)\ni\n(\u00b5(V(cid:98)i )\u2212\u00b5\n(cid:98)\n(V(cid:98)i ))\nn n\ni i\n2\n+ (V(cid:98)i \u2212V\ni\n)T\u2207\u00b5(V(cid:101)i )(\u00b5(V(cid:98)i )\u2212\u00b5\n(cid:98)\n(V(cid:98)i ))\nn\n1 (cid:88) 2 (cid:88) 2 (cid:88)\n=\nn\n(cid:15)2\ni\n+\nn\n(cid:15)\ni\n(V(cid:98)i \u2212V\ni\n)T\u2207\u00b5(V(cid:101)i )+\nn\n(cid:15)\ni\n(\u00b5(V(cid:98)i )\u2212\u00b5\n(cid:98)\n(V(cid:98)i ))+R\nn\ni i i\nwhere R\nn\n= O(||\u03b4(cid:98)\u2212\u03b4||2)+O(||\u00b5\n(cid:98)\n\u2212\u00b5||2)+O(||\u03b4(cid:98)\u2212\u03b4|| ||\u00b5\n(cid:98)\n\u2212\u00b5||2) = o\nP\n(n\u22121/2). The mean of\nthe first three terms is E[(Y \u2212\u00b5(V))2]. By a similar argume"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_21_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 21,
    "text": " three terms is E[(Y \u2212\u00b5(V))2]. By a similar argument,\n1 (cid:88) 1 (cid:88) 2 (cid:88)\nn\n(Y\ni\n\u2212\u00b5\n(cid:98)\n(X\ni\n,V(cid:98)i ))2 =\nn (cid:101)\n(cid:15)2\ni\n+\nn (cid:101)\n(cid:15)\ni\n(V(cid:98)i \u2212V\ni\n)T\u2207\u00b5(X\ni\n,V(cid:101)i )\ni i i\n2 (cid:88)\n+\n(cid:101)\n(cid:15)\ni\n(\u00b5(X\ni\n,V(cid:98)i )\u2212\u00b5\n(cid:98)\n(X\ni\n,V(cid:98)i ))+R(cid:101)n\nn\ni\nwhere\n(cid:101)\n(cid:15)\ni\n= Y\ni\n\u2212\u00b5(X\ni\n,V\ni\n),R(cid:101)n = O(||\u03b4(cid:98)\u2212\u03b4||2)+O(||\u00b5\n(cid:98)\n\u2212\u00b5||2)+O(||\u03b4(cid:98)\u2212\u03b4||||\u00b5\n(cid:98)\n\u2212\u00b5||2) = o\nP\n(n\u22121/2)\nand the mean of the"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_21_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 21,
    "text": "\n(cid:98)\n\u2212\u00b5||2) = o\nP\n(n\u22121/2)\nand the mean of the first three terms is E[(Y \u2212\u00b5(X,V))2]. The result follows from the CLT\n\u221a\nand the fact that n(R\nn\n+R(cid:101)n ) = o\nP\n(1).\nLemma 4. We have that \u03c8 under the partially linear model with interactions, is equal\n0\nto \u03c8 = \u03b8T\u2126\u03b8 where\n3\n(cid:18) 1 mT (cid:19)\n\u2126 = \u03a3 \u2297 Z .\nX m \u03a3\nZ Z\nProof. Let us write\nh\n(cid:88)\n\u00b5(x,z) = \u03b8TW \u2261 \u03b8TX + \u03b8TXZ\n0 j j\nj=1\n21"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_22_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 22,
    "text": "Verdinelli and Wasserman\nwhere we have written \u03b8 = (\u03b8 ,\u03b8 ,...,\u03b8 ) and so \u00b5 (z) = \u03b8Tm + (cid:80)h \u03b8Tm Z . Thus\n0 1 h 0 0 X j=1 j X j\nh\n(cid:88)\n(\u00b5(x,z)\u2212\u00b5 (z))2 = \u03b8T(X \u2212m )(X \u2212m )T\u03b8 + \u03b8T(X \u2212m )(X \u2212m )TZ2\u03b8\n0 0 X X 0 j X X j j\nj=1\nh\n(cid:88) (cid:88)\n+ 2 \u03b8T(X \u2212m )(X \u2212m )TZ \u03b8 +2 \u03b8T(X \u2212m )(X \u2212m )TZ Z \u03b8\n0 X X j j j X X j k k\nj=1 j(cid:54)=k\nand so\nh\n(cid:88)\nE [(\u00b5(x,z)\u2212\u00b5 (z))2] = \u03b8T\u03a3 \u03b8 + \u03b8T\u03a3 (\u03a3 (j,j)+m2(j))\u03b8\n0 0 X 0 j X Z Z j\nj=1\nh\n(cid:88) (cid:88)\n+ 2 \u03b8T\u03a3 m (j)\u03b8 +2 \u03b8T\u03b8 (\u03a3 (j,k)+m (j) m (k))\n0 X Z j j"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_22_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 22,
    "text": " \u03b8T\u03a3 m (j)\u03b8 +2 \u03b8T\u03b8 (\u03a3 (j,k)+m (j) m (k))\n0 X Z j j k Z Z Z\nj=1 j(cid:54)=k\n= \u03b8T\u2126 \u03b8. (cid:3)\n7.2 \u03c8 Under the Semiparametric Model\nL\nHere we give the form that \u03c8 takes under the semiparametric model. Under the model\nL\n\u00b5(x,z) = f(z)+xT\u03b2(z), we have \u03c8 = E[\u03b2T(Z)(X \u2212\u03bd(Z))(X \u2212\u03bd(Z))T\u03b2(Z)] which has\nL\nefficient influence function\n\u03c6 = 2\u03b2(Z)T(X \u2212\u03bd(Z))(X \u2212\u03bd(Z))TV\u22121(Z)XY\n\u2212 2\u03b2(Z)T(X \u2212\u03bd(Z))(X \u2212\u03bd(Z))TV\u22121(Z)(X \u2212\u03bd(Z))(X \u2212\u03bd(Z))T\u03b2\n\u2212 \u03b2T(X \u2212\u03bd(Z))(X \u2212\u03bd(Z))T\u03b2 \u2212\u03c8 .\nL\nWhen \u00b5(x,z) = \u03b2Tx+ (cid:80) \u03b3 x z +f(z) then\njk jk j "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_22_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 22,
    "text": "n \u00b5(x,z) = \u03b2Tx+ (cid:80) \u03b3 x z +f(z) then\njk jk j k\n\u03c8 = \u03b8T(\u2126 +\u2126 +\u2126 +\u2126 )\nL 11 12 21 22\nwhere\n(cid:18) 1 mT (cid:19)\n\u2126 = Z \u2297\u03a3 ,\n11 m \u03a3 +m mT X\nZ Z Z Z\n(cid:18) 1 mT (cid:19)\n\u2126 = Z \u2297E[(X \u2212m )(m \u2212\u03bd(Z))T],\n12 m \u03a3 +m mT X X\nZ Z Z Z\n(cid:18) 1 mT (cid:19)\n\u2126 = Z \u2297E[(X \u2212m )(m \u2212\u03bd(Z))T],\n21 m \u03a3 +m mT X X\nZ Z Z Z\n(cid:18) 1 mT (cid:19)\n\u2126 = Z \u2297E[(m \u2212\u03bd(Z))(X \u2212m )T],\n12 m \u03a3 +m mT X X\nZ Z Z Z\n(cid:18) 1 mT (cid:19)\n\u2126 = Z \u2297E[(m \u2212\u03bd(Z))(m \u2212\u03bd(Z))T].\n22 m \u03a3 +m mT X X\nZ Z Z Z\nWe omit the expression for influence func"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_22_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 22,
    "text": "\nZ Z Z Z\nWe omit the expression for influence function.\n22"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_23_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 23,
    "text": "Variable Importance\n7.3 Partial Correlation\nIn this section, we give the decorrelated version of the partial correlation. Recall that\nE [(Y \u2212\u00b5 (Z))(X \u2212\u03bd (Z))] (cid:82) (cid:82) (\u00b5(x,z)\u2212\u00b5 (z))(x\u2212m )p(x)p(z)dxdz\n0 0 0 0 X\n\u03c1 = = .\n0 (cid:112)E 0 (Y \u2212\u00b5 0 (Z))2E 0 (X \u2212\u03bd 0 (Z))2 \u03c3 (cid:113) (cid:82) (cid:82) (cid:82) (y\u2212\u00b5\u2217(z))2p(y|x,z)p(x)p(z)\nX\nTheorem 5 The efficient influence function for \u03c1 is\n0\n(cid:40) (cid:41)\n1 \u03c8 \u03c8\n1 2\n\u03c6 = \u221a \u03c6 \u2212 \u03c6 \u2212 \u03c6\n1 2 3\n\u03c6 \u03c6 2\u03c8 2\u03c8\n2 3 2 3\nwhere, in this section, we define\n(c"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_23_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 23,
    "text": "2\u03c8 2\u03c8\n2 3 2 3\nwhere, in this section, we define\n(cid:90) (cid:90)\n\u03c8 = (\u00b5(x,z)\u2212\u00b5 (z))(x\u2212m )p(x)p(z)dxdz\n1 0 X\n\u03c8 = \u03c32\n2 X\n(cid:90) (cid:90) (cid:90)\n\u03c8 = (y\u2212\u00b5 (z))2p(y|x,z)p(x)p(z)dxdzdy\n3 0\nand\nY \u2212\u00b5(X,Z)\n\u03c6 = \u00b5 (X)(X \u2212m)+(X \u2212m) p(X)p(Z)+(X \u2212m)p(X)\u00b5(X,Z)\u2212\u00b5 (z)\u22122\u03c8\n1 0 0 1\np(X,Z)\n\u03c6 = (X \u2212m)2\u2212\u03c32\n2 X\np(X)p(Z)\n\u03c6 = (Y \u2212v(Z))2\u2212\u03c8 \u22122 (Y \u2212\u00b5(X,Z)).\n3 3\np(X,Z)\n\u221a\nProof Let us write \u03c1 = f(\u03c8 ,\u03c8 ,\u03c8 ) where f(a,b,c) = a/ bc and\n0 1 2 3\n\u03c8 = E [(Y \u2212\u00b5 (Z))(X \u2212\u03bd (Z))]\n1 0 0 0\n\u03c8 = \u03c32\n2 X\n(cid:90) (cid:90) (cid:90)\n\u03c8 = (y"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_23_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 23,
    "text": "0 0 0\n\u03c8 = \u03c32\n2 X\n(cid:90) (cid:90) (cid:90)\n\u03c8 = (y\u2212\u00b5\u2217(z))2p(y|x,z)p(x)p(z).\n3\nSo the influence function is\nf (\u03c8 ,\u03c8 ,\u03c8 )\u03c6 +f (\u03c8 ,\u03c8 ,\u03c8 )\u03c6 +f (\u03c8 ,\u03c8 ,\u03c8 )\u03c6\n1 1 2 3 1 2 1 2 3 2 3 1 2 3 3\nwhere f = \u2202f/\u2202\u03c8 and \u03c6 is the influence function for \u03c8 . Hence,\nj j j j\n(cid:40) (cid:41)\n1 \u03c8 \u03c8\n1 2\n\u03c6 = \u221a \u03c6 \u2212 \u03c6 \u2212 \u03c6 .\n1 2 3\n\u03c6 \u03c6 2\u03c8 2\u03c8\n2 3 2 3\nNow\n(cid:90) (cid:90)\n\u03c8 = (\u00b5 (x)\u2212\u03c8 )(x\u2212m )p(x)dx = \u00b5 (x)(x\u2212m )p(x)dx\n1 0 0 X 0 X\n23"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_24_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 24,
    "text": "Verdinelli and Wasserman\n(cid:82)\nwhere \u00b5 (x) = \u00b5(x,z)p(z). So\n0\n(cid:90) (cid:90) (cid:90)\n\u03c6 = \u00b5 (x)(cid:48)(x\u2212m )p(x) dx\u2212 \u00b5 (x)m(cid:48) p(x) dx+ \u00b5 (x)(x\u2212m )p(x)(cid:48) dx\n1 0 X 0 X 0 X\n(cid:90) (cid:90)\n= 7 \u00b5 (x)(cid:48)(x\u2212m )p(x) dx\u2212 \u00b5 (x)(X \u2212m )p(x) dx+\u00b5 (X)(X \u2212m )\u2212\u03c8\n0 X 0 X 0 X 1\n(cid:90)\n= \u00b5 (X)(X \u2212m )+ \u00b5 (x)(cid:48)(x\u2212m )p(x) dx\u22122\u03c8 .\n0 X 0 X 1\nNow\n(cid:90)\n\u00b5 (x)(cid:48) = \u00b5(cid:48)(x,z)p(z) dz+\u00b5(x,Z)\u2212\u00b5 (z)\n0 0\n(cid:90)\nY \u2212\u00b5(x,z)\n= I(X = x,Z = z)p(z) dz+\u00b5(x,Z)\u2212\u00b5 (z)\n0\np(x,z)\nY \u2212\u00b5(x,Z)\n= "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_24_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 24,
    "text": "x,Z = z)p(z) dz+\u00b5(x,Z)\u2212\u00b5 (z)\n0\np(x,z)\nY \u2212\u00b5(x,Z)\n= I(x = X) p(Z)+\u00b5(x,Z)\u2212\u00b5 (z).\n0\np(x,Z)\nThus,\n(cid:40) (cid:41)\n(cid:90) (cid:90)\nY \u2212\u00b5(x,Z)\n\u00b5(cid:48)(x,z)p(z) dz = (x\u2212m)p(x) I(x = X) p(Z)+\u00b5(x,Z)\u2212\u00b5 (z)\n0\np(x,Z)\nY \u2212\u00b5(X,Z)\n= (X \u2212m) p(X)p(Z)+(X \u2212m)p(X)\u00b5(X,Z)\u2212\u00b5 (z)\n0\np(X,Z)\nSo\nY\u2212\u00b5(X,Z)\n\u03c6 = \u00b5 (X)(X \u2212m)+(X \u2212m) p(X)p(Z)+(X \u2212m)p(X)\u00b5(X,Z)\u2212\u00b5 (z)\u22122\u03c8 .\n1 0 p(X,Z) 0 1\nAlso\n\u03c6 = (X \u2212m)2\u2212\u03c32.\n2\nNow we turn to \u03c8 = (cid:82) (cid:82) (cid:82) (y\u2212\u00b5\u2217(z))2p(x,y,z). Then\n3\n(cid:90)\n\u03c6 = (Y \u2212v(Z))2\u2212\u03c8 \u22122 p(x,y,z)(y\u2212v(z))v(c"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_24_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 24,
    "text": "3\n(cid:90)\n\u03c6 = (Y \u2212v(Z))2\u2212\u03c8 \u22122 p(x,y,z)(y\u2212v(z))v(cid:48)(z)dz\n3 3\n(cid:90)\n= (Y \u2212v(Z))2\u2212\u03c8 \u22122 p(x,z)(\u00b5\u2212v(z))v(cid:48)(z)dz\n3\nand\np(X)(Y \u2212\u00b5(X,z))\nv(cid:48)(z) = \u00b5(X,z)\u2212v(z)+I(z = Z)\np(X,z)\nso that\n\u03c6 = (Y \u2212v(Z))2\u2212\u03c8 \u22122 (cid:82) p(x,z)(\u00b5\u2212v(z))v(cid:48)(z)dz\n3 3\n= (Y \u2212v(Z))2\u2212\u03c8 \u22122 p(X)p(Z) (Y \u2212\u00b5(X,Z)).\n3 p(X,Z)\nTheremaindercanbeshowntobesecondorderinasimilarwayto\u03c8 . Weomitthedetails.\n0\n24"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_25_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 25,
    "text": "Variable Importance\n7.4 Varying Coefficient Model\nLet \u00b5(x,z) = xT\u03b2(z)+f(z). In this case \u03c8 becomes \u03c8 = tr(\u03a3 H). Define\n0 4 X\nV(z) = Var[X|Z = z] C(z) = Cov[X,Y|Z = z]\nf(z) = \u00b5(z)\u2212\u03bd(z)T\u03b2(z) \u03b2(z) = V\u22121(Z)C(z)\nM = E[\u03b2(Z)] S = Var[\u03b2(Z)].\nLemma 6 The efficient influence function for \u03c8 is\n4\n\u03c6 = tr(\u03a3 \u03c6 )+(X \u2212m )TH(X \u2212m)\u2212\u03c8\nX H X 4\nwhere H = E[\u03b2(Z)\u03b2(Z)T],\n\u03c6 = \u03b2(Z)\u03b2(Z)T \u2212H +\u03b2(Z)[YXT \u2212\u03b2(Z)T(X \u2212\u03bd(Z))(X \u2212\u03bd(Z))T]V\u22121(Z)\nH\n+ V\u22121(Z)[XY \u2212(X \u2212\u03bd(Z))(X \u2212\u03bd(Z))T\u03b2(Z)]\u03b2(Z)T.\nHence, the estimator is\n1 (cid:88) 1 (cid:88)"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_25_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 25,
    "text": "Z)T.\nHence, the estimator is\n1 (cid:88) 1 (cid:88)\n\u03c8(cid:98)4 = tr(\u03a3(cid:98)X \u03c6(cid:98)H (U\ni\n))+ (X\ni\n\u2212X)TH(U\ni\n)(X\ni\n\u2212X).\nn n\ni i\nReferences\nKjersti Aas, Martin Jullum, and Anders L\u00f8land. Explaining individual predictions when\nfeatures are dependent: More accurate approximations to shapley values. arXiv preprint\narXiv:1903.10464, 2019.\nCl\u00b4ement B\u00b4enard, G\u00b4erard Biau, S\u00b4ebastien Da Veiga, and Erwan Scornet. Shaff: Fast and\nconsistent shapley effect estimates via random forests. arXiv preprint a"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_25_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 25,
    "text": "ect estimates via random forests. arXiv preprint arXiv:2105.11724,\n2021.\nIan Covert and Su-In Lee. Improving kernelshap: Practical shapley value estimation via\nlinear regression. arXiv preprint arXiv:2012.01536, 2020.\nIan Covert, Scott Lundberg, and Su-In Lee. Understanding global feature contributions\nwith additive importance measures. arXiv preprint arXiv:2004.00668, 2020.\nBen Dai, Xiaotong Shen, and Wei Pan. Significance tests of feature relevance for a blackbox\nlearner. arXiv preprint arXiv:"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_25_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 25,
    "text": "ance for a blackbox\nlearner. arXiv preprint arXiv:2103.04985, 2021.\nAlexander D\u2019Amour, Peng Ding, Avi Feller, Lihua Lei, and Jasjeet Sekhon. Overlap in\nobservational studies with high-dimensional covariates. Journal of Econometrics, 221(2):\n644\u2013654, 2021.\nDaniel Fryer, Inga Strumke, and Hien Nguyen. Shapley value confidence intervals for vari-\nable selection in regression models. 2020.\n25"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_26_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 26,
    "text": "Verdinelli and Wasserman\nBryan S Graham and Cristine Campos de Xavier Pinto. Semiparametrically efficient esti-\nmation of the average linear regression function. Journal of Econometrics, 2021.\nArthurGretton, KarstenMBorgwardt, MalteJRasch, BernhardSch\u00a8olkopf, andAlexander\nSmola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723\u2013\n773, 2012.\nOliver Hines, Oliver Dukes, Karla Diaz-Ordaz, and Stijn Vansteelandt. Demystifying sta-\ntistical learning based on efficient influ"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_26_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 26,
    "text": "ng sta-\ntistical learning based on efficient influence functions. arXiv preprint arXiv:2107.00681,\n2021.\nRustam Ibragimov and Ulrich K Mu\u00a8ller. t-statistic based correlation and heterogeneity\nrobust inference. Journal of Business & Economic Statistics, 28(4):453\u2013468, 2010.\nOsnat Israeli. A shapley-based decomposition of the r-square of a linear regression. The\nJournal of Economic Inequality, 5(2):199\u2013212, 2007.\nJing Lei, Max G\u2019Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman.\nDis"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_26_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 26,
    "text": "naldo, Ryan J Tibshirani, and Larry Wasserman.\nDistribution-free predictive inference for regression. Journal of the American Statisti-\ncal Association, 113(523):1094\u20131111, 2018.\nWei-Yin Loh and Peigen Zhou. Variable importance scores. arXiv preprint\narXiv:2102.07765, 2021.\nScottLundbergandSu-InLee. Anunexpectedunityamongmethodsforinterpretingmodel\npredictions. arXiv preprint arXiv:1611.07478, 2016.\nMasayoshi Mase, Art B Owen, and Benjamin Seiler. Explaining black box decisions by\nshapley cohort"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_26_3",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 26,
    "text": ". Explaining black box decisions by\nshapley cohort refinement. arXiv preprint arXiv:1911.00467, 2019.\nAndreas Messalas, Yiannis Kanellopoulos, and Christos Makris. Model-agnostic inter-\npretability with shapley values. In 2019 10th International Conference on Information,\nIntelligence, Systems and Applications (IISA), pages 1\u20137. IEEE, 2019.\nWhitney K Newey and James R Robins. Cross-fitting and fast remainder rates for semi-\nparametric estimation. arXiv preprint arXiv:1801.09138, 2018.\nArt B Owen"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_26_4",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 26,
    "text": " arXiv preprint arXiv:1801.09138, 2018.\nArt B Owen and Cl\u00b4ementine Prieur. On shapley value for measuring importance of depen-\ndent inputs. SIAM/ASA Journal on Uncertainty Quantification, 5(1):986\u20131002, 2017.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Model-agnostic interpretability\nof machine learning. arXiv preprint arXiv:1606.05386, 2016.\nAlessandroRinaldo,LarryWasserman,andMaxG\u2019Sell. Bootstrappingandsamplesplitting\nfor high-dimensional, assumption-lean inference. The Annals of S"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_26_5",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 26,
    "text": "sional, assumption-lean inference. The Annals of Statistics, 47(6):3438\u2013\n3469, 2019.\nAlexander M Samarov. Exploring regression structure using nonparametric functional es-\ntimation. Journal of the American Statistical Association, 88(423):836\u2013847, 1993.\n26"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_27_0",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 27,
    "text": "Variable Importance\nNumair Sani, Jaron Lee, Razieh Nabi, and Ilya Shpitser. A semiparametric approach to\ninterpretable machine learning. arXiv preprint arXiv:2006.04732, 2020.\nDino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equiv-\nalence of distance-based and rkhs-based statistics in hypothesis testing. The Annals of\nStatistics, pages 2263\u20132291, 2013.\nGa\u00b4bor J Sz\u00b4ekely, Maria L Rizzo, and Nail K Bakirov. Measuring and testing dependence\nby correlation of distances. Th"
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_27_1",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 27,
    "text": "testing dependence\nby correlation of distances. The annals of statistics, 35(6):2769\u20132794, 2007.\nBrian Williamson and Jean Feng. Efficient nonparametric statistical inference on popula-\ntion feature importance using shapley values. In International Conference on Machine\nLearning, pages 10282\u201310291. PMLR, 2020.\nBrianDWilliamson,PeterBGilbert,NoahRSimon,andMarcoCarone.Aunifiedapproach\nforinferenceonalgorithm-agnosticvariableimportance.arXivpreprintarXiv:2004.03683,\n2020.\nBrian D Williamson, Peter "
  },
  {
    "chunk_id": "doc_5dbefb04_chunk_27_2",
    "doc_id": "doc_5dbefb04",
    "doc_name": "ml_research_paper1.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-01",
    "source": "ml_research_paper1.pdf",
    "page": 27,
    "text": "arXiv:2004.03683,\n2020.\nBrian D Williamson, Peter B Gilbert, Marco Carone, and Noah Simon. Nonparametric\nvariable importance assessment using machine learning techniques. Biometrics, 77(1):\n9\u201322, 2021.\nLuZhangandLucasJanson. Floodgate: inferenceformodel-freevariableimportance. arXiv\npreprint arXiv:2007.01283, 2020.\n27"
  },
  {
    "chunk_id": "doc_88121c78_chunk_1_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 1,
    "text": "JournalofMachineLearningResearch25(2024)1-26 Submitted3/23;Revised12/23;Published1/24\nImproving physics-informed neural networks\nwith meta-learned optimization\nAlex Bihlo abihlo@mun.ca\nDepartment of Mathematics and Statistics\nMemorial University of Newfoundland,\nSt. John\u2019s (NL) A1C 5S7, Canada\nEditor: Samy Bengio\nAbstract\nWe show that the error achievable using physics-informed neural networks for solving dif-\nferential equations can be substantially reduced when these networks are trained using"
  },
  {
    "chunk_id": "doc_88121c78_chunk_1_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 1,
    "text": "ally reduced when these networks are trained using\nmeta-learned optimization methods rather than using fixed, hand-crafted optimizers as\ntraditionally done. We choose a learnable optimization method based on a shallow multi-\nlayer perceptron that is meta-trained for specific classes of differential equations. We illus-\ntrate meta-trained optimizers for several equations of practical relevance in mathematical\nphysics,includingthelinearadvectionequation,Poisson\u2019sequation,theKorteweg\u2013deVries\nequati"
  },
  {
    "chunk_id": "doc_88121c78_chunk_1_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 1,
    "text": "ation,Poisson\u2019sequation,theKorteweg\u2013deVries\nequation and Burgers\u2019 equation. We also illustrate that meta-learned optimizers exhibit\ntransferlearningabilities,inthatameta-trainedoptimizerononedifferentialequationcan\nalso be successfully deployed on another differential equation.\nKeywords: Scientific machine learning, Physics-informed neural networks, Learnable\noptimization, Meta-learning, Transfer learning\n1. Introduction\nPhysics-informed neural networks are a class of methods for solving systems"
  },
  {
    "chunk_id": "doc_88121c78_chunk_1_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 1,
    "text": "etworks are a class of methods for solving systems of differen-\ntial equations. Originally proposed in the 1990s by Lagaris et al. (1998) and popularized\nthrough the work of Raissi et al. (2019), physics-informed neural networks have seen an\nimmense raise in popularity in the past several years. This is in part due to the overall\nrise in interest in all things related to deep neural networks, see LeCun et al. (2015), but\nalso due to some practical advantages of this method compared to traditiona"
  },
  {
    "chunk_id": "doc_88121c78_chunk_1_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 1,
    "text": "l advantages of this method compared to traditional numerical\napproaches such as finite difference, finite elements or finite volume methods. These ad-\nvantages include the evaluation of derivatives using automatic differentiation, see Baydin\net al. (2018), their mesh-free nature and an overall ease of implementation through modern\ndeep-learning frameworks such as JAX (Bradbury et al., 2018), TensorFlow (Abadi et al.,\n2015)orPyTorch(Paszkeetal.,2019). Giventheexpressivepowerofdeepneuralnetworks,"
  },
  {
    "chunk_id": "doc_88121c78_chunk_1_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 1,
    "text": "019). Giventheexpressivepowerofdeepneuralnetworks,\ncf. Cybenko (1989), neural networks are also a well-suited class of function approximation\nfor the solution of systems of differential equations.\nAmaindownsideofphysics-informedneuralnetworksisthatacomplicatedoptimization\nproblem involving a rather involved composite loss function has to be solved Raissi et al.\n(2019). Thedifficultyinsolvingsuchso-calledmulti-taskproblemsiswell-documentedinthe\ndeep learning literature, see e.g. Yu et al. (2020)."
  },
  {
    "chunk_id": "doc_88121c78_chunk_1_6",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 1,
    "text": "ep learning literature, see e.g. Yu et al. (2020). Moreover, since essentially all methods\n(cid:13)c2024AlexBihlo.\nLicense: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/. Attributionrequirementsareprovided\nathttp://jmlr.org/papers/v25/23-0356.html."
  },
  {
    "chunk_id": "doc_88121c78_chunk_2_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 2,
    "text": "Bihlo\nof optimization for deep neural network used to solve differential equations today are at\nmost of first-order, such as stochastic gradient descent, and its momentum-based flavours\nsuch as Adam, Kingma and Ba (2014), the level of error that can typically be achieved\nwith vanilla physics-informed neural networks as proposed by Lagaris et al. (1998); Raissi\net al. (2019) is often subpar compared to their traditional counterparts used in numerical\nanalysis. Whilelowernumericalerrorcanbeachieve"
  },
  {
    "chunk_id": "doc_88121c78_chunk_2_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 2,
    "text": "cal\nanalysis. Whilelowernumericalerrorcanbeachievedusingmoreinvolvedstrategies,suchas\ndomain decomposition approaches, see Jagtap et al. (2020); modified loss functions, see Jin\net al. (2021); Wang et al. (2022); operator-based approaches, see Wang and Perdikaris\n(2023); or higher-order optimizers such as L-BFGS, see Nocedal and Wright (1999), all of\nthese approaches either sacrifice some of the simplicity of vanilla physics-informed neural\nnetworks or substantially increase their training times"
  },
  {
    "chunk_id": "doc_88121c78_chunk_2_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 2,
    "text": "rks or substantially increase their training times.\nSince a main culprit in the overall unsatisfactory error levels achievable with vanilla\nphysics-informed neural networks is the optimization method used, it is natural to aim to\nfind better optimizers. More broadly, optimization is a topic extensively studied in the\nfield of machine learning, with many new optimizers being proposed that aim to overcome\nsome of the (performance or memory) shortcomings of the de-facto standard Adam, see\ne.g. Luca"
  },
  {
    "chunk_id": "doc_88121c78_chunk_2_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 2,
    "text": "mings of the de-facto standard Adam, see\ne.g. Lucas et al. (2019); Shazeer and Stern (2018). There has also been growing interest\nin the field of learnable optimization, referred to as learning to learn (Chen et al., 2022),\nwhich aims to develop optimization methods parameterized by neural networks, that are\nthen meta-learned on a suitably narrow class of tasks, on which they typically outperform\ngeneric (non-learnable) optimization methods.\nTheaimofthispaperistoexploretheuseoflearnableoptimizat"
  },
  {
    "chunk_id": "doc_88121c78_chunk_2_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 2,
    "text": "imofthispaperistoexploretheuseoflearnableoptimizationfortrainingphysics-\ninformed neural networks. We show that meta-trained learnable optimizers with very few\nparameters can substantially outperform standard optimizer in this field. Moreover, once\nmeta-trained, these optimizers can be used to train physics-informed neural networks with\nminimal computational overhead compared to traditional optimizers.\nThe further organization of this paper is as follows. In Section 2 we present a more\nformalize"
  },
  {
    "chunk_id": "doc_88121c78_chunk_2_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 2,
    "text": " follows. In Section 2 we present a more\nformalized review on how neural networks can be used to solve differential equations. Sec-\ntion 3 presents a short overview of the relevant previous work on both physics-informed\nneural networks and learnable optimization. The main Section 4 introduces the class of\nlearnable optimizers used in this work. Section 5 contains the numerical results obtained\nby using these meta-trained optimizers for solving a variety of differential equations us-\ning physics-"
  },
  {
    "chunk_id": "doc_88121c78_chunk_2_6",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 2,
    "text": "variety of differential equations us-\ning physics-informed neural networks. The transfer learning capabilities of the proposed\nlearnable optimizers are investigated in Section 6. A summary with a discussion on further\npossible research directions can be found in the final Section 7.\n2. Solving differential equations with neural networks\nThe numerical solution of differential equations with neural networks was first proposed\nin Lagaris et al. (1998). In this algorithm, the trial solution is broug"
  },
  {
    "chunk_id": "doc_88121c78_chunk_2_7",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 2,
    "text": "8). In this algorithm, the trial solution is brought into a form that\naccounts for initial and/or boundary conditions (as hard constraints), with the actual solu-\ntion being found upon minimizing the mean-squared error that is defined as the residual of\nthe given differential equations evaluated over a finite number of collocation points which\nare distributed over the domain of the problem. This method was recently popularized\nby Raissi et al. (2019), coining the term physics-informed neural net"
  },
  {
    "chunk_id": "doc_88121c78_chunk_2_8",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 2,
    "text": "019), coining the term physics-informed neural networks, and extended to\n2"
  },
  {
    "chunk_id": "doc_88121c78_chunk_3_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 3,
    "text": "Meta-learned optimization for PINNs\nalso allow for the identification of differential equations from data. A recent review on this\nsubject can be found in Cuomo et al. (2022).\nMore formally, consider the following initial\u2013boundary value problem for a general sys-\ntem of L partial differential equations of order n,\n\u2206l(t,x,u ) = 0, l = 1,...,L, t \u2208 [0,t ], x \u2208 \u2126,\n(n) f\nIli(x,u ) = 0, l = 1,...,L, x \u2208 \u2126, (1)\n(ni)|t=0 i i\nBl b(t,x,u ) = 0, l = 1,...,L , t \u2208 [0,t ], x \u2208 \u2202\u2126,\n(n ) b b f\nb\nwhere t \u2208 [0,"
  },
  {
    "chunk_id": "doc_88121c78_chunk_3_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 3,
    "text": "L , t \u2208 [0,t ], x \u2208 \u2202\u2126,\n(n ) b b f\nb\nwhere t \u2208 [0,t ] is the time variable, x = (x ,...,x ) \u2208 \u2126 is the tuple of spatial independent\nf 1 d\nvariables, u = (u1,...,uq) is the tuple of dependent variables, and u is the tuple of all\n(n)\nderivativesofthedependentvariableswithrespecttotheindependentvariablesofordernot\ngreaterthann. TheinitialvalueoperatorisdenotedbyI = (I1,...ILi)andB = (B1,...,BL b)\ndenotes the boundary value operator. The spatial domain is \u2126 and the final time is t .\nf\nIn the followi"
  },
  {
    "chunk_id": "doc_88121c78_chunk_3_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 3,
    "text": "in is \u2126 and the final time is t .\nf\nIn the following, we consider evolution equations for which the initial value operator\nreduces to\nI = u(0,x)\u2212f(x),\nwheref(x) = (f1(x),...,fq(x))isafixedvector-valuedfunction. WealsoconsiderDirichlet\nboundary conditions of the form\nB = u(t,x)\u2212g(t,x),\nwhere g(t,x) = (g1(t,x),...,gq(t,x)) is another fixed vector-valued function.\nSolving system (1) with a neural network N\u03b8 requires the parameterization of the so-\nlution of this system in the form u\u03b8 = N\u03b8(t,x), whe"
  },
  {
    "chunk_id": "doc_88121c78_chunk_3_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 3,
    "text": "ution of this system in the form u\u03b8 = N\u03b8(t,x), where the weights \u03b8 of the neural network\nare found upon minimizing the loss function\nL(\u03b8) = L (\u03b8)+\u03b3L(\u03b8)+\u03b3 L (\u03b8). (2a)\n\u2206 i i b b\nHere\nL \u2206 (\u03b8) = N 1 (cid:88)\nN\u2206\n(cid:88)\nL\n(cid:12) (cid:12)\u2206l(cid:0) ti \u2206 ,xi \u2206 ,u\u03b8 (n) (ti \u2206 ,xi \u2206 ) (cid:1)(cid:12) (cid:12) 2 ,\n\u2206\ni=1 l=1\nL i (\u03b8) = N 1 (cid:88)\nNi\n(cid:88)\nLi\n(cid:12) (cid:12)Ili (cid:0) xi i ,u\u03b8 (ni) (0,xi i ) (cid:1)(cid:12) (cid:12) 2 , (2b)\ni\ni=1li=1\nN L\nL b (\u03b8) = N 1 (cid:88)b (cid:88)b (cid:12) ("
  },
  {
    "chunk_id": "doc_88121c78_chunk_3_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 3,
    "text": "1\nN L\nL b (\u03b8) = N 1 (cid:88)b (cid:88)b (cid:12) (cid:12)Bli (cid:0) ti b ,xi b ,u\u03b8 (n b ) (ti b ,xi b ) (cid:1)(cid:12) (cid:12) 2 ,\nb\ni=1l =1\nb\nare the mean squared error losses corresponding to the differential equation, the initial\ncondition and the boundary value residuals, respectively, and \u03b3 and \u03b3 are positive scaling\ni b\nconstants. These losses are evaluated over the collection of collocation points (cid:8) (ti ,xi ) (cid:9)N\u2206\n\u2206 \u2206 i=1\nfor the system \u2206, (cid:8) (0,xi) (cid:9)Ni for the in"
  },
  {
    "chunk_id": "doc_88121c78_chunk_3_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 3,
    "text": " the system \u2206, (cid:8) (0,xi) (cid:9)Ni for the initial data, and (cid:8) (ti,xi) (cid:9)N b for the boundary data,\ni i=1 b b i=1\nrespectively. Upon successful minimization, the neural network N\u03b8 provides a numerical\nparameterization of the solution of the given initial\u2013boundary value problem.\n3"
  },
  {
    "chunk_id": "doc_88121c78_chunk_4_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 4,
    "text": "Bihlo\n3. Related work\nPhysics-informed neural networks were proposed by Lagaris et al. (1998), and popularized\nthrough the work of Raissi et al. (2019), and have since been used extensively for solving\ndifferential equations in science and engineering. While the general algorithm for training\nneuralnetworkstosolvedifferentialequationsisstraightforward, severalcomplicationsarise\nin practice. Firstly, balancing the individual loss contributions in (2b) so that all the\ninitial values, the boundary "
  },
  {
    "chunk_id": "doc_88121c78_chunk_4_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 4,
    "text": "(2b) so that all the\ninitial values, the boundary values, and the differential equations are adequately enforced\nsimultaneously constitutes a multi-task learning problem which may not be properly solved\nby minimizing the composite loss function (2a), see Sener and Koltun (2018); Yu et al.\n(2020) for some work on multi-task learning problems. Secondly, it is well-known that\ntrainingneuralnetworksusinggradientdescentmethodsleadstoaspectralbiasintheform\nof low frequencies being learned first and hi"
  },
  {
    "chunk_id": "doc_88121c78_chunk_4_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 4,
    "text": "form\nof low frequencies being learned first and high-frequencies requiring longer training times,\nsee Rahaman et al. (2019). Correspondingly, oscillatory solutions or stiff problems may not\nbe accurately learned using standard physics-informed neural networks. Lastly, the general\nsetup (2) requires proportionally more collocation points the larger the spatio-temporal\ndomain of the differential equation being solved is. Training neural networks for solving\ndifferential equations over large spatio"
  },
  {
    "chunk_id": "doc_88121c78_chunk_4_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 4,
    "text": "r solving\ndifferential equations over large spatio-temporal domains can destabilize training, which is\nfrequentlyencounteredinpractice. Inmostcases,physics-informedneuralnetworksforsuch\nproblemsincorrectlyconvergetoatrivialconstantsolutionofthegivendifferentialequation,\nsee e.g. Bihlo and Popovych (2022); Penwarden et al. (2023); Wang et al. (2022). One\nstraightforward solution for this problem is to break the entire domain into multiple sub-\ndomains, and solve a sequence of smaller problems wit"
  },
  {
    "chunk_id": "doc_88121c78_chunk_4_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 4,
    "text": "ains, and solve a sequence of smaller problems with multiple neural networks instead.\nThis multi-model approach has recently been used for solving the shallow-water equations\non a rotating sphere by Bihlo and Popovych (2022).\nLearnable optimization has been the topic of research since the works by Bengio et al.\n(1990, 1995), with Andrychowicz et al. (2016) popularizing the use of neural network based\nlearning to learn optimization. The latter paper specifically introduced an LSTM-type\nneural net"
  },
  {
    "chunk_id": "doc_88121c78_chunk_4_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 4,
    "text": "er specifically introduced an LSTM-type\nneural network optimizer that is being trained using gradient descent. Subsequent work\nfocussed on improving the performance of learnable neural network based optimizers by\nimproving their training strategies, see e.g. Lv et al. (2017); Vicol et al. (2021), improving\nthe LSTM architecture of the optimizer (Wichrowska et al., 2017), or replacing the LSTM-\nbased architecture in favour of a simpler MLP-based one, cf. Harrison et al. (2022); Metz\net al. (2022)"
  },
  {
    "chunk_id": "doc_88121c78_chunk_4_6",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 4,
    "text": "ne, cf. Harrison et al. (2022); Metz\net al. (2022). Also, exploratory work has been done that aims to understand what exactly\nthese learnable optimizers are learning (Maheswaranathan et al., 2021). Below, we will use\nthe optimizer proposed in Harrison et al. (2022), as this optimizer was found to be both\nstable and fast to meta-train, and able to generalize to problems that are different from\nthosetheoptimizerwastrainedon,allofwhicharepropertiesdesirableforphysics-informed\nneural networks. For a"
  },
  {
    "chunk_id": "doc_88121c78_chunk_4_7",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 4,
    "text": "esirableforphysics-informed\nneural networks. For a more comprehensive review on learnable optimization consult the\nrecent review paper by Chen et al. (2022).\nTo the best of our knowledge, the use of learnable optimization for physics-informed\nneural networks has not been pursued so far. The related field of using meta-learning to\naccelerating the training of physics-informed neural networks has been investigated in Liu\net al. (2022) and Psaros et al. (2022) recently. Specifically, in these works"
  },
  {
    "chunk_id": "doc_88121c78_chunk_4_8",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 4,
    "text": " al. (2022) recently. Specifically, in these works the authors used\nmeta-learning to discover suitable initialization methods and physics-informed neural net-\n4"
  },
  {
    "chunk_id": "doc_88121c78_chunk_5_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 5,
    "text": "Meta-learned optimization for PINNs\nwork loss functions that generalize across relevant task distributions, respectively, thereby\nspeeding up training of individual physics-informed neural networks from these task distri-\nbutions.\n4. Meta-learnable optimization for physics-informed neural networks\nA main goal of meta-learned optimization it to improve hand-designed optimization rules\nsuch as the Adam optimizer introduced by Kingma and Ba (2014) for updating the weight\nvector \u03b8 of a neural networ"
  },
  {
    "chunk_id": "doc_88121c78_chunk_5_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 5,
    "text": "or updating the weight\nvector \u03b8 of a neural network with loss function L(\u03b8). Recall that the Adam update rule is\ngiven by\nm = \u03b2 m +(1\u2212\u03b2 )\u2207 L(\u03b8 ), v = \u03b2 v +(1\u2212\u03b2 )(\u2207 L(\u03b8 ))2,\nt 1 t\u22121 1 \u03b8 t\u22121 t 2 t\u22121 2 \u03b8 t\u22121\nm\u02c6 = m /(1\u2212\u03b2t), v\u02c6 = v /(1\u2212\u03b2t),\nt t 1 t t 2\n(cid:112)\n\u03b8 = \u03b8 \u2212\u03b7w = \u03b8 \u2212\u03b7m\u02c6 /( v\u02c6 +\u03b5),\nt t\u22121 adam t t t\nwhere t = 1,..., is the optimization time step, m and v are the first and second moment\nvectors, with \u03b2 ,\u03b2 \u2208 [0,1) being the exponential decay rates for the moment estimates, \u03b5\n1 2\nbeing a regul"
  },
  {
    "chunk_id": "doc_88121c78_chunk_5_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 5,
    "text": "ates for the moment estimates, \u03b5\n1 2\nbeing a regularization constant, and \u03b7 being the learning rate.\nSimilarly, the parameter updates of a meta-learned optimizer is structured as\n\u03b8 = \u03b8 \u2212f(z ;\u03d1), (3)\nt t\u22121 t\nwhere f is the parametric update function with z referring to the input features of the\nt\nlearnable optimizer, and \u03d1 are the trainable meta-parameters of the optimizer, usually the\nweights of a neural network. To allow for the learnable optimizer to be transferable to\nneural networks of diffe"
  },
  {
    "chunk_id": "doc_88121c78_chunk_5_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 5,
    "text": "zer to be transferable to\nneural networks of different sizes it is customary to have the parameter update rule (3)\nact component-wise, with each weight \u03b8 of the weight vector \u03b8 being updated in the same\ni\nway. Thus, in the following we describe the parameteric update formula in terms of scalar\nvariables, rather than vectorial quantities.\nWhilethereareseveraloptimizerarchitecturesthathavebeenproposedintheliterature,\ncf.Chenetal.(2022),hereweusearelativelysimplemulti-layerperceptronfortheoptimizer"
  },
  {
    "chunk_id": "doc_88121c78_chunk_5_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 5,
    "text": "lativelysimplemulti-layerperceptronfortheoptimizer\narchitecture. Notably, we follow the work by Harrison et al. (2022) and structure the\nparametric update formula for each weight \u03b8 as\ni\n\u03bb\nf = \u03bb exp(\u03bb sadam))w + \u221a 3 dbbexp(\u03bb sbb), (4)\n1 2 \u03d1 adam v +\u03b5 \u03d1 4 \u03d1\nt\nwhere \u03bb , i = 1,...,4 are positive constants, w corresponds to the Adam update step\ni adam\nand sadam, sbb and dbb are to the output heads of the meta-learned optimizer with neural\n\u03d1 \u03d1 \u03d1\nnetwork weights \u03d1.\nOnahighlevel, thefirstterminthelearna"
  },
  {
    "chunk_id": "doc_88121c78_chunk_5_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 5,
    "text": "k weights \u03d1.\nOnahighlevel, thefirstterminthelearnableupdateformula(4)canbeseenasanomi-\nnaltermderivedfromtheAdamupdateformulawithscalablelearningrate\u03bb exp(\u03bb sadam)),\n1 2 \u03d1\nwhich guarantees an update step in a descent direction, and the second term corresponds to\na blackbox update term structured as the product of a directional and magnitudinal term,\n\u221a\ndbb and exp(\u03bb sbb), respectively, with the denominator v +\u03b5 acting as a pre-conditioner\n\u03d1 4 \u03d1 t\nthat should guarantee that the overall update form"
  },
  {
    "chunk_id": "doc_88121c78_chunk_5_6",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 5,
    "text": "that should guarantee that the overall update formula leads corresponds to a descending on\n5"
  },
  {
    "chunk_id": "doc_88121c78_chunk_6_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 6,
    "text": "Bihlo\nthe loss surface. For more details on the rationale behind the update rule (4), consult (Har-\nrison et al., 2022).\nTheinputsz atoptimizationstepttothemulti-layerperceptronoptimizerwithoutput\nt\nheads sadam, sbb and dbb are chosen as follows:\n\u03d1 \u03d1 \u03d1\n1. The weights \u03b8 ;\nt\n2. The gradients \u2207 L(\u03b8 );\n\u03b8 t\n3. The second momentum accumulators v with decay rates \u03b2 \u2208 {0.5,0.9,0.99,0.999};\nt 2\n4. One over the square root of the above four second momentum accumulators;\n5. The time step t.\nHere, we build "
  },
  {
    "chunk_id": "doc_88121c78_chunk_6_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 6,
    "text": " accumulators;\n5. The time step t.\nHere, we build upon the extensive study carried out in Metz et al. (2022), with the above\ninput parameters heuristically being found to perform well for the physics-informed neural\nnetworks that were trained in this work.\nAll input features (except the time step) were normalized to have a second moment of\none. The time step is converted into a total of 11 features by computing tanh(t/x) where\nx \u2208 {1,3,10,30,100,300,1000,3000,10k,30k,100k}. All features were the"
  },
  {
    "chunk_id": "doc_88121c78_chunk_6_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 6,
    "text": "300,1000,3000,10k,30k,100k}. All features were then concatenated\nandpassedthroughastandardmulti-layerperceptrontoyieldtheabovethreeoutputheads.\nIt is also interesting to point out that the work by Choi et al. (2019) has shown that\nupon the right choice of hyper-parameters, a more general optimizer including another\noptimizer as a specific case, should never perform worse than this specific optimizer. Since\nfor the choice of \u03bb = 1, \u03bb = \u03bb = 0 the learnable optimizer reduces to standard Adam\n1 2 3\n"
  },
  {
    "chunk_id": "doc_88121c78_chunk_6_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 6,
    "text": "earnable optimizer reduces to standard Adam\n1 2 3\nas a specific case, upon the right tuning of parameters of the learnable optimizer (which is\ndone with a persistent evolutionary strategy here, see the following Section 5 for a further\ndiscussion), the learnable optimizer should always outperform Adam. We show below that\nthis is indeed the case.\n5. Numerical results\nInthissectionweshowcasetheuseofmeta-learnedoptimizationforsolvingsomewell-known\ndifferential equations from mathematical physics, t"
  },
  {
    "chunk_id": "doc_88121c78_chunk_6_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 6,
    "text": "ifferential equations from mathematical physics, that have been extensively studied using\nphysics-informedneuralnetworks. Inallofthefollowingexamplesweusethevanillaversion\nof physics-informed neural networks as laid out in Lagaris et al. (1998); Raissi et al. (2019).\nAs discussed in Section 3, it is well-understood by now that this formulation can suffer from\nseveral drawbacks which to remedy is currently an active research field. As such, the goal\nof this section is not to obtain the best possi"
  },
  {
    "chunk_id": "doc_88121c78_chunk_6_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 6,
    "text": "al\nof this section is not to obtain the best possible numerical solution for each given model,\nbuttoshowhowmeta-learnedoptimizationcanimprovetheresultsobtainableusingvanilla\nphysics-informed neural network when compared to using standard optimization. Our base\noptimizer we compare against is the Adam optimizer, the de-facto standard being used in\nthe field of physics-informed neural networks today. In Section 6, when investigating the\ntransferlearningcapabilitiesoflearnableoptimizers, wethenwill"
  },
  {
    "chunk_id": "doc_88121c78_chunk_6_6",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 6,
    "text": "rningcapabilitiesoflearnableoptimizers, wethenwillshowamoresophisticated\ntraining strategy for physics-informed neural networks.\nInalltheexamplesbelow,theoutputheadsofthemeta-learnedoptimizerwereinitialized\nusing a normal distribution with zero mean and variance of 10\u22123, to guarantee that the\nneural network output is close to zero at the beginning of meta-training of the optimizer.\n6"
  },
  {
    "chunk_id": "doc_88121c78_chunk_7_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 7,
    "text": "Meta-learned optimization for PINNs\nDue to the form of the meta-learned optimizer (4), this means that before meta-training\nstarts, the meta-learned optimizer is very close to the standard Adam optimizer.\nFor all examples, the multi-layer perceptron being used for the meta-learned optimizer\nhastwohiddenlayerswith32unitseach, usingtheswish activationfunction. Thisarchitec-\nture was found using hyperparameter tuning to give a good balance between computational\noverheadofmeta-trainingtheoptimizeran"
  },
  {
    "chunk_id": "doc_88121c78_chunk_7_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 7,
    "text": "omputational\noverheadofmeta-trainingtheoptimizeranderrorleveloftheresultingoptimizer. Weshould\nlike to note here that in contrast to the application of meta-learned optimization in areas of\nmodern deep learning, such as computer vision or natural language processing, which work\nwithneuralnetworkswithuptohundredsofhiddenlayersandbillionsofweights,theneural\nnetworks arising in physics-informed neural networks are typically relatively small. In fact,\nall of the architectures considered in this pape"
  },
  {
    "chunk_id": "doc_88121c78_chunk_7_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 7,
    "text": ",\nall of the architectures considered in this paper have less than 10,000 trainable parameters.\nThis allows for larger neural networks being used for the meta-learned optimizer, without\nincurring computationally infeasible costs. Still, the underlying multi-layer perceptron of\nthe meta-learned optimizer is relatively small, having only 2,115 trainable parameters.\nWetrainthisoptimizerusingthepersistentevolutionarystrategy,azeroth-orderstochas-\ntic optimization method described in Vicol et al. (20"
  },
  {
    "chunk_id": "doc_88121c78_chunk_7_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 7,
    "text": " optimization method described in Vicol et al. (2021). This algorithm has several hyper-\nparameters, including the total number of particles N used for gradient computation, the\npartial unroll length K of the inner optimization problem before a meta-gradient update is\ncomputed, the standard deviation of perturbations \u03c3 and the learning rate \u03b1 for the meta-\nlearned weight update. Using hyperparamter tuning, we determined N = 2 (antithetic)\nparticles, K = 1 epochs and a learning rate of \u03b1 = 10\u22124 t"
  },
  {
    "chunk_id": "doc_88121c78_chunk_7_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 7,
    "text": "es, K = 1 epochs and a learning rate of \u03b1 = 10\u22124 to be the best hyperparameters\nfor our problem, but it would be interesting to carry out a more in-depth study on the\nhyperparameters of this algorithm. For more details, see Algorithm 2 in Vicol et al. (2021).\nForeachproblem, unlessotherwisespecified, wethensampleatotalof20differenttasks\nandmeta-trainthelearnableoptimizerforatotalof50epochsontheassociatedtasks. Each\ntaskcorrespondstoanewinstantiationoftheparticularneuralnetworkarchitecture, where"
  },
  {
    "chunk_id": "doc_88121c78_chunk_7_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 7,
    "text": "ionoftheparticularneuralnetworkarchitecture, where\nwe also slightly perturb the parameters for each equation during meta-training, which we\nfound useful in aiding the generalization capabilities of the learned optimizers. Specifically,\nfor each task we sample uniformly randomly c \u2208 [0.9,1.1] for the advection velocity in the\nlinear advection equation \u03bd \u2208 [0,0.1] for the dispersion coefficient in the KdV equation and\n\u00b5 \u2208 [0,0.01] for the diffusion coefficient for Burgers\u2019 equation. We found empir"
  },
  {
    "chunk_id": "doc_88121c78_chunk_7_6",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 7,
    "text": " coefficient for Burgers\u2019 equation. We found empirically that\ntraining the meta-learned optimizer for relatively few epochs (50 epochs compared to using\nthe learned optimizer for more than 1,000 epochs at testing stage) provided a good balance\nbetweenperformanceandmeta-trainingcost. Wealsonotethatduetothestochasticnature\nofthetrainingalgorithmandtasksamplingstrategies, thelearnableoptimizermayattimes\nconverge to a suboptimal solution. If this occurs, we simply re-train the optimizer again to\nbe "
  },
  {
    "chunk_id": "doc_88121c78_chunk_7_7",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 7,
    "text": "urs, we simply re-train the optimizer again to\nbe able to show the best possible results obtainable with the learnable optimizer under the\nchosen parameter and training regime. To guarantee a fair comparison at testing time, the\ninitialweightsandbiasesofallneuralnetworksbeingtrainedwiththerespectiveoptimizers\nare the same for any experiment.\nInTable1wesummarizetheparametersofthephysics-informedneuralnetworkstrained\nin this section. We use hyperbolic tangents as activation function for all hidden"
  },
  {
    "chunk_id": "doc_88121c78_chunk_7_8",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 7,
    "text": "lic tangents as activation function for all hidden layers. We\nuse mini-batch gradient computation with a total of 10 batches per epoch.\nWe report both the time series of the loss for the standard Adam optimizer and the\nmeta-learned optimizer, and the point-wise error e = u \u2212u , where u is either the\nnn ref ref\n7"
  },
  {
    "chunk_id": "doc_88121c78_chunk_8_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 8,
    "text": "Bihlo\nLinear adv. Poisson KdV Burgers Shallow-water adv.\n# hidden layers 2 4 6 6 4\n# units 20 20 20 20 20\n# PDE points 10,000 10,000 10,000 10,000 100,000\n# IC/BC points 100 400 100 100 10,000\n# epochs 6,000 2,000 1,500 3,000 3,000\nTable 1: Parameters of the physics-informed neural networks trained below.\nanalytical solution (if available), or a high-resolution numerical reference solution obtained\nfrom using a pseudo-spectral method for the spatial discretization and an adaptive Runge\u2013\nKutta me"
  },
  {
    "chunk_id": "doc_88121c78_chunk_8_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 8,
    "text": "ial discretization and an adaptive Runge\u2013\nKutta method for time stepping using the method of lines approach, see Durran (2010).\nThe algorithm described here has been implemented using TensorFlow 2.11 and the\ncodes will be made available on GitHub1.\n5.1 One-dimensional linear advection equation\nAs a first example, consider the one-dimensional linear advection equation\nu +cu = 0,\nt x\nwhere we consider t \u2208 [0,3] and x \u2208 [\u22121,1] with c = 1 being the advection velocity. We\nset u(0,x) = u (x) = sin\u03c0x a"
  },
  {
    "chunk_id": "doc_88121c78_chunk_8_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 8,
    "text": "dvection velocity. We\nset u(0,x) = u (x) = sin\u03c0x and use periodic boundary conditions. We enforce the periodic\n0\nboundary conditions as hard constraint in the physics-informed neural networks, using the\nstrategy introduced by Bihlo and Popovych (2022). We set \u03b3 = 1 in the loss function (2a).\ni\nTo establish standard Adam as a strong baseline for all subsequent problems, we test\na total of 7 different optimizers for this equation, namely momentum SGD (with \u03b2 = 0.9),\nstandard Adam, Adam with square"
  },
  {
    "chunk_id": "doc_88121c78_chunk_8_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 8,
    "text": "GD (with \u03b2 = 0.9),\nstandard Adam, Adam with square root learning rate decay, a hyper-parameter tuned\nversion of Adam, NAdam, and the proposed learnable optimizer as well as the learnable\noptimizerusingsquarerootlearningratedecay. Thelearningrateforallstandardoptimizers\nwas set to \u03b7 = 5\u00b710\u22124. For the hyper-parameter tuned version of Adam we were using\na tree-structured Parzen Estimator algorithm implemented in optuna, see Akiba et al.\n(2019). The search space for the parameters were the intervals"
  },
  {
    "chunk_id": "doc_88121c78_chunk_8_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 8,
    "text": "search space for the parameters were the intervals \u03b2 ,\u03b2 \u2208 [0.1,0.99999] and\n1 2\n\u03b7 \u2208 [10\u22125,10\u22122]. To keep the computational cost between the hyper-parameter tuned\nversion of Adam and the learnable optimizer comparable, we optimize the hyper-parameter\ntuned version of Adam for 50 epochs over a total of 50 trials. The optimal parameters\nfound were then \u03b2 = 0.347, \u03b2 = 0.424 and \u03b7 = 7.65\u00b710\u22123. The constants of the learnable\n1 2\noptimizer were chosen as \u03bb = 5 \u00b7 10\u22124 and \u03bb = 10\u22123,i = 2,...,4. We then u"
  },
  {
    "chunk_id": "doc_88121c78_chunk_8_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 8,
    "text": "s \u03bb = 5 \u00b7 10\u22124 and \u03bb = 10\u22123,i = 2,...,4. We then use each\n1 i\noptimizer to train a physics-informed neural network for the linear advection equation until\nthe learnable optimizers have converged, which is after about 1,900 and 2,800 epochs for\nthe learnable optimizer with and without learning rate decay, respectively. To provide a\ncomparison for the best obtainable numerical solutions for all optimizers, we in addition\ncontinue training the physics-informed neural networks by all other optimizer"
  },
  {
    "chunk_id": "doc_88121c78_chunk_8_6",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 8,
    "text": "cs-informed neural networks by all other optimizers for a total\nof 6,000 epochs, whence they have converged to the point that any further improvements\nwould require an excessive amount of additional training steps.\n1. https://github.com/abihlo/LearnableOptimizationPinns\n8"
  },
  {
    "chunk_id": "doc_88121c78_chunk_9_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 9,
    "text": "Meta-learned optimization for PINNs\n(a) Training loss for all optimizers. (b) Mean optimizer step updates.\nFigure 1: Time series of losses for the standard and meta-learned optimizers (left), and\nmean update step sizes for the meta-learned optimizer and the standard Adam\noptimizer with fixed learning rates (right), as used for the linear advection equa-\ntion. See also Table 2.\nThe numerical results for this example are depicted in Figures 1 and 2. For this par-\nticular example, the meta-learned "
  },
  {
    "chunk_id": "doc_88121c78_chunk_9_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 9,
    "text": ". For this par-\nticular example, the meta-learned optimizers considerably outperform all baseline standard\noptimizers, resulting in a training loss and point-wise error that is more than 10 times\nsmaller by the time they have converged to their final solutions. Both learnable optimizers\nstill outperform all standard optimizers by the time they have obtained their final solu-\ntions. Adding in a learning-rate decay also shows that the performance of the learnable\noptimizer can be further boosted, "
  },
  {
    "chunk_id": "doc_88121c78_chunk_9_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 9,
    "text": "f the learnable\noptimizer can be further boosted, to the point that it achieves an error of the same level as\nthe learnable optimizer with fixed learning rate after less than 2,000 epochs. This situation\nsomewhat parallels the case of Adam with learning rate decay, which does converge slightly\nfaster than standard Adam with fixed learning rate.\nFrom Fig. 1a it is also interesting to note that the hyper-parameter tuned version of\nAdam has a lower loss than all other optimizers for the first sever"
  },
  {
    "chunk_id": "doc_88121c78_chunk_9_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 9,
    "text": "loss than all other optimizers for the first several hundred epochs, but\nflattens out at a significantly higher loss level than the learnable optimizers. This is note-\nworthy, as both the hyper-parameter tuned version of Adam and the learnable optimizer\nhave been trained/optimized for the same number of epochs. This result illustrates that\nthe learnable optimizer used here is more than just an optimally hyper-parameter tuned\nversion of the standard Adam optimizer.\nFig. 2 and Table 2 then illustr"
  },
  {
    "chunk_id": "doc_88121c78_chunk_9_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 9,
    "text": "rd Adam optimizer.\nFig. 2 and Table 2 then illustrate that these substantially different loss values also result\nin quite different numerical solutions. Notably, even after 6,000 epochs the numerical errors\nobtained by the Adam optimizer family is still higher than the numerical error obtained by\nthe learnable optimizers in less than half the number of training steps. Momentum SGD\nstalls at a loss of around 10\u22121, much worse than all other optimizers.\nFig. 1b depicts the mean gradient step update"
  },
  {
    "chunk_id": "doc_88121c78_chunk_9_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 9,
    "text": "ers.\nFig. 1b depicts the mean gradient step updates corresponding to the standard Adam\noptimizer, the scaled Adam part of the learnable optimizer (the first term in Eqn. (4)) and\nthe blackbox term of the learnable optimizer (the second term in Eqn. (4)). For the first\n9"
  },
  {
    "chunk_id": "doc_88121c78_chunk_10_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 10,
    "text": "Bihlo\nFigure 2: Numerical results for the linear advection equation. Top row: Standard Adam\noptimizer after 2,845 epochs, the number of training steps required for the learn-\nable optimizer to converge. Middle row: Adam after 6,000 epochs, the number\nof training steps required for Adam to converge. Bottom row: Meta-learned\noptimizer. Left to right shows the numerical solution obtained from the physics-\ninformed neural networks, the exact solution, and the difference between the\nnumerical solutio"
  },
  {
    "chunk_id": "doc_88121c78_chunk_10_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 10,
    "text": ", and the difference between the\nnumerical solution and the exact solution.\n500 epochs, the Adam part of the learnable optimizer closely follows the standard Adam\noptimizer, but once the blackbox term starts to dominate the overall gradient step update,\nalso the Adam part of the learnable optimizer behaves differently from the standard Adam\noptimizer, indicating that these optimizers indeed follow different paths to their respective\nminima. It is also noteworthy that the rather steep increase in"
  },
  {
    "chunk_id": "doc_88121c78_chunk_10_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 10,
    "text": " also noteworthy that the rather steep increase in the blackbox part after\n1,000 epochs followed by an equally steep decrease after 2,000 epochs corresponds directly\nto the substantially better performance of the learnable optimizer compared to the Adam\noptimizer in this period of the training regime.\nA quantitative evaluation of the numerical solutions themselves is given in Table 2.\nThis table shows that the two learnable optimizers with fixed and learning rate decay,\nrespectively, give the be"
  },
  {
    "chunk_id": "doc_88121c78_chunk_10_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 10,
    "text": "and learning rate decay,\nrespectively, give the best numerical solutions among all optimizers tested. This table also\n10"
  },
  {
    "chunk_id": "doc_88121c78_chunk_11_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 11,
    "text": "Meta-learned optimization for PINNs\nEpochs L2O L2O decay Adam Adam decay Adam tuned NAdam SGDM\n1,925 1.33\u00b710\u22124 1.33\u00b710\u22125 1.16\u00b710\u22121 1.98\u00b710\u22123 5.79\u00b710\u22123 1.60\u00b710\u22121 3.09\u00b710\u22121\n2,845 3.58\u00b710\u22125 \u2014 3.54\u00b710\u22124 2.69\u00b710\u22124 3.91\u00b710\u22123 2.94\u00b710\u22124 2.48\u00b710\u22121\n6,000 \u2014 \u2014 7.80\u00b710\u22125 7.68\u00b710\u22125 3.41\u00b710\u22123 1.57\u00b710\u22124 1.48\u00b710\u22121\nTable 2: Numerical results for the linear advection equation. Shown are the l -errors of\n2\nthe obtained solutions for three experiments: i) After training with all optimizers\nfor 1,925 epochs, i.e. whe"
  },
  {
    "chunk_id": "doc_88121c78_chunk_11_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 11,
    "text": "ing with all optimizers\nfor 1,925 epochs, i.e. when the learnable optimizer with learning rate decay (L2O\ndecay) has converged. ii) After training with all but the L2O decay optimizer for\n2,845 epochs, i.e. when the learnable optimizer with fixed learning rate (L2O) has\nconverged. iii) After training with all standard optimizers for 6,000 epochs, i.e.\nwhen the comparison optimizers have converged as well.\nshows that the standard Adam optimizers with or without learning rate decay give the best\nn"
  },
  {
    "chunk_id": "doc_88121c78_chunk_11_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 11,
    "text": "ith or without learning rate decay give the best\nnumericalresultsamongallstandardoptimizerstested, providedtheyhavebeengivenmore\nthan twice the number of optimization steps than the learnable optimizers.\nIn view of these results, in the following examples we will only show the comparison\nbetween standard Adam, the optimizer of choice for virtually all physics-informed neural\nnetworks today, and the learnable optimizer proposed above although the results obtained\nfor the linear advection equation"
  },
  {
    "chunk_id": "doc_88121c78_chunk_11_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 11,
    "text": "results obtained\nfor the linear advection equation do hint at further improvements that are possible with\nsuitable learning rate decay strategies.\n5.2 Poisson equation\nAsanexampleforaboundary-valueproblem,considerthetwo-dimensionalPoissonequation\nu +u = f(x,y),\nxx yy\nover the domain \u2126 = [\u22121,1]\u00d7[\u22121,1] for the exact solution\nu (x,y) = (0.1sin2\u03c0x+tanh10x)sin2\u03c0y,\nexact\nwith the associated right-hand side using Dirichlet boundary conditions. This problem\nwas considered in Kharazmi et al. (2021). Sinc"
  },
  {
    "chunk_id": "doc_88121c78_chunk_11_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 11,
    "text": "lem\nwas considered in Kharazmi et al. (2021). Since this is a boundary value problem, there\nis no initial loss in the loss function (2a) and we use \u03b3 = 1000. This value was chosen\nb\nheuristically to balance the differential equation and boundary value losses. The learning\nrateforAdamforthisexamplewassetto\u03b7 = 5\u00b710\u22124 andtheconstantsofthemeta-learned\noptimizer, were \u03bb = 5\u00b710\u22124 and \u03bb = 10\u22123, i = 2,...,4.\n1 i\nThe training loss for this example is shown in Fig. 3, the numerical results as compared\nto "
  },
  {
    "chunk_id": "doc_88121c78_chunk_11_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 11,
    "text": "n in Fig. 3, the numerical results as compared\nto the exact solution with the associated point-wise error are depicted in Fig. 4. As with\nthe linear advection equation from the previous example, also for the Poisson equation the\nmeta-learned optimization method leads to better results both in terms of a lower training\nloss and smaller point-wise errors compared to the standard Adam optimizer.\n11"
  },
  {
    "chunk_id": "doc_88121c78_chunk_12_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 12,
    "text": "Bihlo\nFigure 3: Training loss for the Adam and meta-learned optimizers for the two-dimensional\nPoisson equation.\nFigure 4: Numerical results for the Poisson equation. Top row: Standard Adam optimizer.\nBottom row: Meta-learned optimizer. Left to right shows the numerical solution\nobtained from the physics-informed neural networks, the exact solution, and the\ndifference between the numerical solution and the exact solution.\n5.3 Korteweg\u2013de Vries equation\nWe next consider the Korteweg\u2013de Vries equa"
  },
  {
    "chunk_id": "doc_88121c78_chunk_12_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 12,
    "text": "uation\nWe next consider the Korteweg\u2013de Vries equation\nu +uu \u2212\u03bdu = 0,\nt x xxx\n12"
  },
  {
    "chunk_id": "doc_88121c78_chunk_13_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 13,
    "text": "Meta-learned optimization for PINNs\nwith initial condition u(0,x) = cos\u03c0x using periodic boundary conditions over the domain\nx \u2208 [\u22121,1] and t \u2208 [0,1], setting \u03bd = 0.0025. This equation has been extensively studied\nusing physics-informed neural networks, see e.g. Jagtap et al. (2020); Raissi et al. (2019).\nAgain, we enforce the periodic boundary conditions as hard constraint and set \u03b3 = 1 in\ni\nthe loss function (2a). The learning rate of the Adam optimizer was chosen as \u03b7 = 5\u00b710\u22124,\nand the consta"
  },
  {
    "chunk_id": "doc_88121c78_chunk_13_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 13,
    "text": "optimizer was chosen as \u03b7 = 5\u00b710\u22124,\nand the constants of the meta-learned optimizer were set to \u03bb = 5\u00b710\u22124 and \u03bb = 10\u22123,\n1 i\ni = 2,...,4.\nFigure 5: Training loss for the Adam and meta-learned optimizers for the Korteweg\u2013de\nVries equation.\nFigure 5 contains the respective training losses of the Adam and meta-learned optimiz-\ners. The numerical solutions for the associated trained physics-informed neural networks as\ncompared against the numerical solution obtained from a pseudo-spectral numerical "
  },
  {
    "chunk_id": "doc_88121c78_chunk_13_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 13,
    "text": "olution obtained from a pseudo-spectral numerical inte-\ngration method are featured in Figure 6. These plots again illustrate that the meta-learned\noptimizer reduces the training loss considerably faster than the standard Adam optimizer,\nwhich also improves upon the point-wise error of the numerical solution compared to the\nreference solution. In fact, the training loss after about 1,100 epochs is lower for the meta-\nlearned optimizer than what the Adam optimizer achieves at the end of training."
  },
  {
    "chunk_id": "doc_88121c78_chunk_13_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 13,
    "text": "he Adam optimizer achieves at the end of training.\n5.4 Burgers\u2019 equation\nAs our next example we consider Burgers\u2019 equation\nu +uu \u2212\u00b5u = 0,\nt x xx\noverthetemporal-spatialdomain[0,1]\u00d7[\u22121,1]withinitialconditionu(0,x) = \u2212sin\u03c0xand\nperiodicboundaryconditionsinx-direction. Thediffusionparameterwassetas\u00b5 = 0.01/\u03c0.\nBurgers equation is also one of the most prominent examples considered using physics-\ninformed neural networks, see Raissi et al. (2019) for some results.\nAs for the Korteweg\u2013de Vries equation,"
  },
  {
    "chunk_id": "doc_88121c78_chunk_13_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 13,
    "text": "me results.\nAs for the Korteweg\u2013de Vries equation, we enforce the periodic boundary conditions\nas hard constraints, use \u03b3 = 1 in the loss function (2a), and set the learning rate of\ni\n13"
  },
  {
    "chunk_id": "doc_88121c78_chunk_14_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 14,
    "text": "Bihlo\nFigure 6: Numerical results for the Kortweg\u2013de Vries equation. Top row: Standard Adam\noptimizer. Bottom row: Meta-learned optimizer. Left to right shows the numer-\nical solution obtained from the physics-informed neural networks, the numerical\nreference solution, and the difference between the numerical solution and the\nreference solution.\nthe Adam optimizer to \u03b7 = 10\u22124, and the constants of the meta-learned optimizer to\n\u03bb = 10\u22124 and \u03bb = 10\u22123, i = 2,...,4. The meta-learned optimizer is bei"
  },
  {
    "chunk_id": "doc_88121c78_chunk_14_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 14,
    "text": "\u22123, i = 2,...,4. The meta-learned optimizer is being trained as for\n1 i\nthe previous examples, i.e. using Burgers\u2019 equation on 20 tasks, which each task being a\nnewly instantiated neural network with different random initial weights.\nFigure 7: Training loss for the Adam and meta-learned optimizers for Burgers\u2019 equation.\n14"
  },
  {
    "chunk_id": "doc_88121c78_chunk_15_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 15,
    "text": "Meta-learned optimization for PINNs\nFigure 8: Numerical results for Burgers\u2019 equation. Top row: Standard Adam optimizer.\nBottom row: Meta-learned optimizer using Burgers equation. Left to right shows\nthe numerical solution obtained from the physics-informed neural networks, a\nnumerical reference solution, and the difference between the numerical solution\nand the reference solution.\nFigures 7 and 8 contain the associated numerical results for this example, showing\nthe training loss of the respect"
  },
  {
    "chunk_id": "doc_88121c78_chunk_15_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 15,
    "text": " example, showing\nthe training loss of the respective optimizers and the actual numerical results for solving\nBurgers\u2019 equation using the trained neural networks. Figures 7 illustrates that the meta-\nlearned optimizer again outperforms the Adam optimizer with the final loss value of the\nAdam optimizer being reached already after 2,200 epochs by the meta-learned optimizer.\nThelossvaluesarealsoconsistentwiththenumericalresultsshowninFig.8,illustrating\nthat the meta-learned optimizer using Burgers\u2019"
  },
  {
    "chunk_id": "doc_88121c78_chunk_15_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 15,
    "text": "ing\nthat the meta-learned optimizer using Burgers\u2019 equation leads to a much more accurate\nsolution in the vicinity of the developing shock, which is failed to be captured by the\nstandard Adam optimizer.\n5.5 Linear advection on the sphere\nThe last example we consider here is the shallow-water advection on a sphere. This is\none of the standard test cases from Williamson et al. (1992) for numerical methods for the\nshallow-water equations on the sphere. Specifically, the equation to solve reads:\nu v"
  },
  {
    "chunk_id": "doc_88121c78_chunk_15_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 15,
    "text": "re. Specifically, the equation to solve reads:\nu v\nh + h + h = 0,\nt \u03bb \u03b8\nacos\u03b8 a\n15"
  },
  {
    "chunk_id": "doc_88121c78_chunk_16_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 16,
    "text": "Bihlo\nwhere h = h(t,\u03bb,\u03b8) is the height field in spherical geometry with longitude \u03bb \u2208 [\u2212\u03c0,\u03c0] and\nlatitude \u03b8 \u2208 [\u2212\u03c0/2,\u03c0/2], u and v are prescribed velocity fields of the form\nu = U(cos\u03b8cos\u03b1+sin\u03b8cos\u03bbsin\u03b1),\nv = \u2212U sin\u03bbsin\u03b1,\nand a is the radius of Earth. The initial condition for h is chosen as a cosine bell of the\nform\n(cid:40)\nH(1+cos(\u03c0r/R))/2 if r < R\nh(0,\u03bb,\u03b8) =\n0 if r (cid:62) R,\nwhere r = aarccos(sin\u03b8 sin\u03b8 +cos\u03b8 cos\u03b8cos(\u03bb\u2212\u03bb ). We choose the constants as U =\nc c c\n2\u03c0a/12 m\u00b7days\u22121, H = 1000 m and "
  },
  {
    "chunk_id": "doc_88121c78_chunk_16_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 16,
    "text": "ants as U =\nc c c\n2\u03c0a/12 m\u00b7days\u22121, H = 1000 m and R = a/3. The centre of the bell is placed at (\u03bb ,\u03b8 ) =\nc c\n(\u03c0/2,0). Weset\u03b1 = 0makingthebelltraversealonetheequator. Theboundaryconditions\non the sphere are enforced using hard constraints, as discussed in Bihlo and Popovych\n(2022), with the equations being solved for t \u2208 [0,2] days. The learning rate for the Adam\noptimizer was set to \u03b7 = 10\u22123 and the constants of the meta-learned optimizer are all\n\u03bb = 10\u22123,i = 1,...,4. The meta-learned optimizer "
  },
  {
    "chunk_id": "doc_88121c78_chunk_16_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 16,
    "text": "\n\u03bb = 10\u22123,i = 1,...,4. The meta-learned optimizer is trained for 100 tasks for a total of\ni\n20 steps, with each task being a newly instantiated neural network.\nFigure 9: Training loss for the Adam and meta-learned optimizers for the shallow-water\nadvection equation on the sphere.\nThe numerical results for this test case are summarized in Figs. 9 and 10. As in the\nprevious examples, the meta-learned optimizer again drastically outperforms the standard\nAdam optimizer. Here, the minimum is reached "
  },
  {
    "chunk_id": "doc_88121c78_chunk_16_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 16,
    "text": "dard\nAdam optimizer. Here, the minimum is reached after about 600 epochs, by which time the\nAdam optimizer is still stuck in a plateau on the loss surface. Thus, to provide a more\nreasonable comparison we train the comparison physics-informed neural network with the\nAdam optimizer for 3,000 epochs instead, by which time the loss is starting to level out.\nStill, it is evident from Fig. 10 that the meta-learned optimizer still outperforms Adam\ndespite the latter having had roughly 5 time more comp"
  },
  {
    "chunk_id": "doc_88121c78_chunk_16_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 16,
    "text": "ite the latter having had roughly 5 time more compute on the evaluation problem.\n16"
  },
  {
    "chunk_id": "doc_88121c78_chunk_17_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 17,
    "text": "Meta-learned optimization for PINNs\nFigure 10: Numerical results for the shallow-water advection equation on the sphere. Top\nrow: Standard Adam optimizer after 3,000 epochs of training. Bottom row:\nMeta-learnedoptimizerafter600epochsoftraining. Lefttorightshowstheerror\nof the numerical solution obtained from the physics-informed neural networks at\ndays zero (initial condition), one and two, when compared to the exact reference\nsolution.\n6. Transfer learning\nThe previous section illustrated the p"
  },
  {
    "chunk_id": "doc_88121c78_chunk_17_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 17,
    "text": "er learning\nThe previous section illustrated the potential of meta-learnable optimization to outperform\nstandard optimizers for training physics-informed neural networks. We should like to re-\nmindherethatphysics-informedneuralnetworksfollowadifferentideologyfromtraditional\nmachine learning, in that these networks are not trained with generalization capabilities in\nmind. Physics-informed neural networks are meant to be solution interpolants, whereas in\ntraditional machine learning, extrapolation"
  },
  {
    "chunk_id": "doc_88121c78_chunk_17_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 17,
    "text": "eas in\ntraditional machine learning, extrapolation (from the training to the testing dataset) is the\nmain goal of learning.\nStill, having accomplished this goal, in this section we ask the following wider question:\nDometa-learnableoptimizersforphysics-informedneuralnetworksgeneralizeacrosssimilar\ntasks or do they only overfit the single task they were trained on? While it is non-trivial to\nproperly define the notion of similarity for partial differential equations in this context, in\nthis sectio"
  },
  {
    "chunk_id": "doc_88121c78_chunk_17_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 17,
    "text": "erential equations in this context, in\nthis section we provide a first investigation into the transfer learning capabilities of meta-\nlearned optimization for physics-informed neural networks.\n6.1 Transfer learning across similar tasks: Different initial conditions\nA common task in the numerical solution of differential equations is to change the initial\ncondition of a given system of equations. For standard physics-informed neural networks\nthisrequiresre-trainingofthenetwork,whichiscomputationa"
  },
  {
    "chunk_id": "doc_88121c78_chunk_17_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 17,
    "text": "equiresre-trainingofthenetwork,whichiscomputationallycostly. Wethusinvestigate\nhere the transfer learning abilities of meta-learnable optimizers that have been trained on\nan ensemble of initial conditions for one fixed differential equation.\nForthesakeofillustration,wechoosetheKorteweg\u2013deVriesequationhere. Morespecif-\nically, we sample our task distribution for meta-training the optimizer from an ensemble of\ninitial conditions here, where for the sake of simplicity we consider initial conditions"
  },
  {
    "chunk_id": "doc_88121c78_chunk_17_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 17,
    "text": " sake of simplicity we consider initial conditions of the\n17"
  },
  {
    "chunk_id": "doc_88121c78_chunk_18_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 18,
    "text": "Bihlo\nform\nu(0,x) = cos(kx+\u03c6),\nwhere k is sampled from integers between 1 and 3 and \u03c6 is sampled uniformly from\n[\u2212\u03c0/2,\u03c0/2]. We choose a relatively narrow task distribution to speed up meta-learning.\nOnce trained, we evaluate the optimizer on the unseen test problem with k = 2 and\n\u03c6 = \u2212\u03c0/4. Since this is a harder problem than using the meta-learned optimizer on the\nsame problem (i.e. same initial condition and same differential equation), we meta-train the\noptimizer on a total of 50 tasks here in"
  },
  {
    "chunk_id": "doc_88121c78_chunk_18_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 18,
    "text": "train the\noptimizer on a total of 50 tasks here instead of the 20 tasks used so far.\nFigure 11: Training loss for the Adam and meta-learned optimizers for the Korteweg\u2013de\nVries equation using transfer learning.\nThe results of this experiment are depicted in Figures 11 and 12. These figures again\nshow improvement of the meta-learned optimizer when compared to the results obtained\nusing Adam. The meta-learned optimizer is able to generalize to the unseen test problem,\nby reaching both lower loss v"
  },
  {
    "chunk_id": "doc_88121c78_chunk_18_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 18,
    "text": "unseen test problem,\nby reaching both lower loss values and a smaller overall point-wise error in the solution\nof the KdV equation. This example demonstrates that transfer learning across the same\nequation class, i.e. choosing different initial values but keeping the equation the same, is\nindeed feasible.\n6.2 Transfer learning across similar tasks: Longer time integrations\nIn this paper we have focussed exclusively on improving vanilla physics-informed neural\nnetworks using learnable optimizatio"
  },
  {
    "chunk_id": "doc_88121c78_chunk_18_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 18,
    "text": "formed neural\nnetworks using learnable optimization. However, as was reviewed in Section 3, numerous\nmodified training methodologies were put forth in the literature that aim to improve some\nof the shortcomings of vanilla physics-informed neural networks. Among these strategies,\nwe focus on the multi-model approach here: Rather than training a single neural network\nfor the entire spatio-temporal domain, we split the domain into smaller sub-domains and\ntrain one neural network for each sub-domain"
  },
  {
    "chunk_id": "doc_88121c78_chunk_18_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 18,
    "text": "s and\ntrain one neural network for each sub-domain, taking into account the interface conditions\nbetween sub-domains in a suitable manner. As we focus exclusively on evolution equations\nhere, we consider the splitting into temporal slices only, and do not consider spatial domain\n18"
  },
  {
    "chunk_id": "doc_88121c78_chunk_19_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 19,
    "text": "Meta-learned optimization for PINNs\nFigure 12: Numerical results for the Kortweg\u2013de Vries equation using transfer learning.\nTop row: Standard Adam optimizer. Bottom row: Meta-learned optimizer.\nLeft to right shows the numerical solution obtained from the physics-informed\nneural networks, the numerical reference solution, and the difference between\nthe numerical solution and the reference solution.\ndecomposition here. The initial conditions for later neural networks are then derived from\nthefinal"
  },
  {
    "chunk_id": "doc_88121c78_chunk_19_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 19,
    "text": "ter neural networks are then derived from\nthefinalsolutionsofearlierneuralnetworks,withtrainingproceedinginasequentialfashion.\nFor more details on this approach, see e.g. Bihlo and Popovych (2022).\nIn particular, we consider solving the linear advection equation for twice the size of the\ntemporal domain considered in Section 5.1, i.e. here t \u2208 [0,6], and we use two sub-models\ntrained for t \u2208 [0,3] and t \u2208 [3,6], respectively, to patch together the solution over the\nentire domain. Notably, the me"
  },
  {
    "chunk_id": "doc_88121c78_chunk_19_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 19,
    "text": "e solution over the\nentire domain. Notably, the meta-learned optimizer is trained only for the first sub-domain\nt \u2208 [0,3], and then applied for both sub-domains t \u2208 [0,3] and t \u2208 [3,6], i.e. the optimizer\nhas never seen any solution on the temporal interval t \u2208 [3,6].\nThe numerical results of this investigation are depicted in Figures 13 and 14. These\nresultsillustratethattheoptimizertrainedforoneparticularsolution(ortask),thesolution\nof the linear advection equation over the interval t \u2208 [0,3],"
  },
  {
    "chunk_id": "doc_88121c78_chunk_19_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 19,
    "text": "ar advection equation over the interval t \u2208 [0,3], generalizes to another particular\nsolution (or task), the solution of the same equation over the interval t \u2208 [3,6]. This\nis especially useful as the multi-model approach is currently one of the best possibilities\nof improving numerical solutions obtainable using physics-informed neural networks, as\ntrainingmanyneuralnetworksovershortertimeintervalsgenerallyleadstoamoreaccurate\nsolution than training a single neural network over a longer time in"
  },
  {
    "chunk_id": "doc_88121c78_chunk_19_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 19,
    "text": "ning a single neural network over a longer time interval.\nIn Fig. 13 it is also interesting to note that the Adam optimizer has lower loss for the\nsecond model during the first 1,200 epochs of training. This is, however, not a reflection\nof the standard optimizer outperforming the learnable one in this first part of training the\nsecond model; rather, it is a reflection that failure to learn the solution over the initial\n19"
  },
  {
    "chunk_id": "doc_88121c78_chunk_20_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 20,
    "text": "Bihlo\nFigure 13: Training loss for the Adam and meta-learned optimizers for solving the linear\nadvection equation over the temporal domain t \u2208 [0,6] using the multi-model\napproach with two sub-models. We train the meta-learned optimizer for the\ntemporal domain t \u2208 [0,3] and then apply the learned optimizer for training a\nphysics-informed neural network for the initial temporal domain t \u2208 [0,3] and a\nsecond network for the subsequent temporal domain t \u2208 [3,6].\ninterval adequately results in a sim"
  },
  {
    "chunk_id": "doc_88121c78_chunk_20_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 20,
    "text": "in t \u2208 [3,6].\ninterval adequately results in a simpler solution that will be used as an initial condition at\ntime t = 3 for the second model.\n6.3 Transfer learning across different tasks\nAs the last investigation into transfer learning capabilities of learnable optimizers, we in-\nvestigate the capabilities of meta-learned optimizers across different tasks. Specifically, we\ninvestigatetheabilitiesofameta-learnedoptimizertrainedononeparticularclassofpartial\ndifferential equations to generalize acr"
  },
  {
    "chunk_id": "doc_88121c78_chunk_20_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 20,
    "text": "ofpartial\ndifferential equations to generalize across different partial differential equations.\nFor this, we meta-train a new learnable optimizer on each of the evolutionary equations\nfrom Section 5 and then apply it to all the evolutionary equations presented in that section.\nWeexcludethePoissonequationfromthisstudyasthisisaboundary-valueproblemandas\nsuchrequiresadifferenttrainingsetupfromtheotherequationsfromSection5. Specifically,\nthe loss function to be minimized for the initial value proble"
  },
  {
    "chunk_id": "doc_88121c78_chunk_20_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 20,
    "text": "ction to be minimized for the initial value problems is L(\u03b8) = L (\u03b8)+\u03b3L(\u03b8)\n\u2206 i i\nwhile for the Poisson equation it is L(\u03b8) = L (\u03b8)+\u03b3 L (\u03b8).\n\u2206 b b\nFor all experiments, we use neural networks with 6 hidden layers and 20 units per layer.\nA learning rate of \u03b7 = 2\u00b710\u22124 was used, and the constants of the meta-learned optimizer\nwere set to \u03bb = 2\u00b710\u22124 and \u03bb = 10\u22123, i = 2,...,4. Each optimizer is trained and applied\n1 i\nfive times, with mean values and standard deviations of the errors reported in Table "
  },
  {
    "chunk_id": "doc_88121c78_chunk_20_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 20,
    "text": "andard deviations of the errors reported in Table 3.\nThese errors are the final l -errors of the numerical solution obtained after 1,000 epochs of\n2\ntraining, as compared to the reference solutions for these problems, which were the same\nas in Section 5.\n20"
  },
  {
    "chunk_id": "doc_88121c78_chunk_21_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 21,
    "text": "Meta-learned optimization for PINNs\nFigure 14: Numerical results for the Adam and meta-learned optimizers for solving the\nlinear advection equation over the temporal domain t \u2208 [0,6] using the multi-\nmodel approach with two sub-models.\nEvaluated for \u2192\nLA KdV Burgers\nTrained on \u2193\nLA 4.01\u00b710\u22123\u00b14.52\u00b710\u22123 5.52\u00b710\u22122\u00b11.52\u00b710\u22122 2.48\u00b710\u22123\u00b11.88\u00b710\u22123\nKdV 7.36\u00b710\u22122\u00b16.34\u00b710\u22122 5.48\u00b710\u22122\u00b13.14\u00b710\u22122 7.45\u00b710\u22123\u00b14.67\u00b710\u22123\nBurgers 5.86\u00b710\u22122\u00b15.28\u00b710\u22122 5.81\u00b710\u22122\u00b13.65\u00b710\u22122 2.26\u00b710\u22123\u00b19.71\u00b710\u22124\nAdam 1.39\u00b710\u22121\u00b13.48\u00b710\u22122 "
  },
  {
    "chunk_id": "doc_88121c78_chunk_21_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 21,
    "text": "10\u22122 2.26\u00b710\u22123\u00b19.71\u00b710\u22124\nAdam 1.39\u00b710\u22121\u00b13.48\u00b710\u22122 1.14\u00b710\u22121\u00b11.66\u00b710\u22122 5.91\u00b710\u22123\u00b12.55\u00b710\u22123\nTable 3: Transfer learning abilities for the meta-learned optimizers across different tasks.\nShown are the l -errors for meta-learned optimizers that have been trained on one\n2\nparticular class of evolution equations from Section 5 and then being used for all\nthe evolution equations from Section 5.\nSimilar as shown in Section 5, Table 3 again shows that each meta-learned optimizer\noutperforms Adam if it was"
  },
  {
    "chunk_id": "doc_88121c78_chunk_21_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 21,
    "text": " meta-learned optimizer\noutperforms Adam if it was trained and evaluated on the same equation. Interestingly,\nwith the exception of the KdV-trained learnable optimizer evaluated for Burgers\u2019 equation,\nall meta-learned optimizers trained on one equation but evaluated for a different equation\noutperform the standard Adam optimizer for that different equation, hinting at these op-\ntimizers being able to learn transferable features beyond the class of equations they have\nbeen trained on. Note also t"
  },
  {
    "chunk_id": "doc_88121c78_chunk_21_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 21,
    "text": "f equations they have\nbeen trained on. Note also that the optimizer trained for the linear advection equation\nperforms close to optimal for all models. This suggest that meta-learning an optimizer on\neasier equations (such as the linear advection equation), and then applying them to more\ncomplicated equations is preferable compared to training them on more complicated equa-\n21"
  },
  {
    "chunk_id": "doc_88121c78_chunk_22_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 22,
    "text": "Bihlo\ntions (such as Burgers\u2019 or the KdV equation) and then applying them to simpler equations.\nThis is physically reasonable, as both Burgers\u2019 equation and the KdV equation have some\ncharacteristics of the simple advection equation, which the meta-learned optimizer trained\non the linear advection equation appears to be able to leverage when applied to these more\ncomplicated models. Conversely, the optimizers trained on the more complicated equations\nseem to learn the specifics of those models, "
  },
  {
    "chunk_id": "doc_88121c78_chunk_22_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 22,
    "text": "ions\nseem to learn the specifics of those models, which then do not readily generalize to other\nmodels.\n7. Conclusion\nWe have investigated the use of meta-learned optimization for improving the training of\nphysics-informed neural networks in this work. Meta-learned optimization, or learning-to-\nlearn, has become an increasingly popular topic in deep learning and thus it is natural to\ninvestigate its applicability to scientific machine learning as well. We have done so here by\nillustrating that m"
  },
  {
    "chunk_id": "doc_88121c78_chunk_22_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 22,
    "text": " well. We have done so here by\nillustrating that meta-learned optimization can be used to improve the numerical results\nobtainable using physics-informed neural networks, which is a popular machine learning-\nbased method for solving differential equations. We have also shown that meta-learned\noptimization exhibits transfer learning capabilities that could be leveraged to further speed\nup training of physics-informed neural networks by allowing learned optimizers to be reused\nfor different classe"
  },
  {
    "chunk_id": "doc_88121c78_chunk_22_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 22,
    "text": "arned optimizers to be reused\nfor different classes of equations.\nThe goal of this paper was to illustrate that meta-learned optimization alone can sub-\nstantially improve the vanilla form of physics-informed neural networks, which was laid\nout in the seminal works by Lagaris et al. (1998) and Raissi et al. (2019). This form has\nbeen extensively studied, and we have shown here that meta-learned optimization can give\n(sometimes substantially) better numerical results compared to standard hand-cra"
  },
  {
    "chunk_id": "doc_88121c78_chunk_22_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 22,
    "text": "er numerical results compared to standard hand-crafted op-\ntimization rules. This means that meta-learned optimizers are able to reach a particular\nerror level quicker than standard optimizers, resulting in either shorter training times (for\na given target computational error) or better numerical accuracy (for the same number of\ntraining epochs). It is also possible that these learnable optimizers reach a minimum not\nachievable with a standard optimizer.\nThere are several avenues for future rese"
  },
  {
    "chunk_id": "doc_88121c78_chunk_22_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 22,
    "text": "timizer.\nThere are several avenues for future research that would provide natural extensions to\nthe present work. Firstly, one could investigate the use of meta-learned optimization for\nother formulations of physics-informed neural networks. Here we have shown that meta-\nlearned optimization improves vanilla physics-informed neural networks, and also works\nfor temporal domain decomposition approaches. The latter is a main remedy for training\nissues encountered in physics-informed neural networks"
  },
  {
    "chunk_id": "doc_88121c78_chunk_22_6",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 22,
    "text": "es encountered in physics-informed neural networks. However, there is also a zoo of\nother variations to the main physics-informed neural network approach that are applicable\nto different classes of differential equations. This list of methods includes, to name a few,\nvariational formulations, see e.g. Kharazmi et al. (2021), formulations based on spatio-\ntemporaldomaindecompositions,seee.g.Jagtapetal.(2020),techniquesbasedonimproved\nloss functions, such as shown by Psaros et al. (2022); Wang et "
  },
  {
    "chunk_id": "doc_88121c78_chunk_22_7",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 22,
    "text": "s, such as shown by Psaros et al. (2022); Wang et al. (2022), re-sampling\nstrategies as introduced by Wu et al. (2023), and operator-based formulations as described\nin Wang and Perdikaris (2023). It should also be stressed that not all of these methods\nsubstantially outperform vanilla physics-informed networks, and the latter are still being\nused extensively in the literature today, see e.g. Cuomo et al. (2022) for a recent review.\n22"
  },
  {
    "chunk_id": "doc_88121c78_chunk_23_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 23,
    "text": "Meta-learned optimization for PINNs\nSecondly,thereisamultitudeofothermeta-learnedoptimizationalgorithmsbasedonneural\nnetworks that have been proposed in the literature, see the review paper by Chen et al.\n(2022) for an extensive list of such optimizers. There are also several training strategies\navailable for meta-learned optimization, including gradient descent, evolutionary strategies\nandreinforcementlearningbasedones, seeagainChenetal.(2022). Together, thisprovides\narichsetoftrainingstrategie"
  },
  {
    "chunk_id": "doc_88121c78_chunk_23_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 23,
    "text": "Together, thisprovides\narichsetoftrainingstrategies,meta-learnableoptimizerarchitecturesandphysics-informed\nmodelformulationsthatcouldbeexploredtogethertopossiblyfindmoreaccuratesolutions\nof differential equations using physics-informed neural networks. We plan to explore some\nof these possibilities in the near future.\nAcknowledgments\nThis research was undertaken thanks to funding from the Canada Research Chairs program\nandtheNSERCDiscoveryGrantprogram. Theauthorthanksthethreeanonymousreferees\nf"
  },
  {
    "chunk_id": "doc_88121c78_chunk_23_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 23,
    "text": "rogram. Theauthorthanksthethreeanonymousreferees\nfor their valuable suggestions in the course of the review process.\nReferences\nM. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis,\nJ. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,\nR. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man\u00b4e, R. Monga, S. Moore,\nD. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar,\nP. Tucker, V. Vanhoucke, V. Vasudevan"
  },
  {
    "chunk_id": "doc_88121c78_chunk_23_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 23,
    "text": ", K. Talwar,\nP. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi\u00b4egas, O. Vinyals, P. Warden, M. Wat-\ntenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on\nheterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available\nfrom tensorflow.org.\nT. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama. Optuna: A next-generation\nhyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD in-\nternational conference on knowledge discovery & data minin"
  },
  {
    "chunk_id": "doc_88121c78_chunk_23_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 23,
    "text": "nal conference on knowledge discovery & data mining, pages 2623\u20132631, 2019.\nM.Andrychowicz, M.Denil, S.Gomez, M.W.Hoffman, D.Pfau, T.Schaul, B.Shillingford,\nand N. De Freitas. Learning to learn by gradient descent by gradient descent. Advances\nin neural information processing systems, 29, 2016.\nA.G.Baydin, B.A.Pearlmutter, A.A.Radul, andJ.M.Siskind. Automaticdifferentiation\nin machine learning: a survey. J. Mach. Learn. Res., 18:Paper No. 153, 2018.\nS. Bengio, Y. Bengio, and J. Cloutier. On the "
  },
  {
    "chunk_id": "doc_88121c78_chunk_23_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 23,
    "text": "18.\nS. Bengio, Y. Bengio, and J. Cloutier. On the search for new learning rules for ANNs.\nNeural Process. Lett., 2(4):26\u201330, 1995.\nY. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule. Universit\u00b4e de\nMontr\u00b4eal, 1990.\nA. Bihlo and R. O. Popovych. Physics-informed neural networks for the shallow-water\nequations on the sphere. J. Comput. Phys., 456:111024, 2022.\n23"
  },
  {
    "chunk_id": "doc_88121c78_chunk_24_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 24,
    "text": "Bihlo\nJ. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula,\nA. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable trans-\nformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\nT.Chen,X.Chen,W.Chen,H.Heaton,J.Liu,Z.Wang,andW.Yin. Learningtooptimize:\nA primer and a benchmark. J. Mach. Learn. Res., 23(189):1\u201359, 2022.\nD. Choi, C. J. Shallue, Z. Nado, J. Lee, C. J. Maddison, and G. E. Dahl. On empirical\ncomparisons of optim"
  },
  {
    "chunk_id": "doc_88121c78_chunk_24_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 24,
    "text": " and G. E. Dahl. On empirical\ncomparisons of optimizers for deep learning. arXiv:1910.05446, 2019.\nS. Cuomo, V. S. Di Cola, F. Giampaolo, G. Rozza, M. Raissi, and F. Piccialli. Scientific\nmachine learning through physics\u2013informed neural networks: where we are and what\u2019s\nnext. J. Sci. Comput., 92(3):88, 2022.\nG. Cybenko. Approximation by superpositions of a sigmoidal function. Math. Control\nSignals Syst., 2(4):303\u2013314, 1989.\nD. R. Durran. Numerical methods for fluid dynamics: With applications to"
  },
  {
    "chunk_id": "doc_88121c78_chunk_24_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 24,
    "text": "l methods for fluid dynamics: With applications to geophysics,\nvolume 32. Springer Science & Business Media, 2010.\nJ.Harrison, L.Metz, andJ.Sohl-Dickstein. Acloserlookatlearnedoptimization: Stability,\nrobustness, and inductive biases. Advances in neural information processing systems, 35:\n3758\u20133773, 2022.\nA. D. Jagtap, E. Kharazmi, and G. E. Karniadakis. Conservative physics-informed neural\nnetworks on discrete domains for conservation laws: applications to forward and inverse\nproblems. Comput. "
  },
  {
    "chunk_id": "doc_88121c78_chunk_24_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 24,
    "text": "ications to forward and inverse\nproblems. Comput. Methods Appl. Mech. Eng., 365:113028, 2020.\nX. Jin, S. Cai, H. Li, and G. E. Karniadakis. NSFnets (Navier\u2013Stokes flow nets): Physics-\ninformed neural networks for the incompressible Navier\u2013Stokes equations. J. Comput.\nPhys., 426:109951, 2021.\nE. Kharazmi, Z. Zhang, and G. E. Karniadakis. hp-VPINNs: Variational physics-informed\nneural networks with domain decomposition. Comput. Methods Appl. Mech. Eng., 374:\n113547, 2021.\nD. P. Kingma and J. Ba. A"
  },
  {
    "chunk_id": "doc_88121c78_chunk_24_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 24,
    "text": "Eng., 374:\n113547, 2021.\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv:1412.6980,\n2014.\nI. E. Lagaris, A. Likas, and D. I. Fotiadis. Artificial neural networks for solving ordinary\nand partial differential equations. IEEE Trans. Neural Netw., 9(5):987\u20131000, 1998.\nY. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436\u2013444, 2015.\nX. Liu, X. Zhang, W. Peng, W. Zhou, and W. Yao. A novel meta-learning initialization\nmethod for physics-informed neural netw"
  },
  {
    "chunk_id": "doc_88121c78_chunk_24_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 24,
    "text": "ialization\nmethod for physics-informed neural networks. Neural. Comput. Appl., 34(17):14511\u2013\n14534, 2022.\nJ. Lucas, S. Sun, R. Zemel, and R. Grosse. Aggregated momentum: Stability through\npassive damping. In International Conference on Learning Representations, 2019.\n24"
  },
  {
    "chunk_id": "doc_88121c78_chunk_25_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 25,
    "text": "Meta-learned optimization for PINNs\nK. Lv, S. Jiang, and J. Li. Learning gradient descent: Better generalization and longer\nhorizons. In International Conference on Machine Learning, pages 2247\u20132255. PMLR,\n2017.\nN. Maheswaranathan, D. Sussillo, L. Metz, R. Sun, and J. Sohl-Dickstein. Reverse en-\ngineering learned optimizers reveals known and novel mechanisms. Advances in neural\ninformation processing systems, 34:19910\u201319922, 2021.\nL. Metz, C. D. Freeman, J. Harrison, N. Maheswaranathan, and J. S"
  },
  {
    "chunk_id": "doc_88121c78_chunk_25_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 25,
    "text": "Freeman, J. Harrison, N. Maheswaranathan, and J. Sohl-Dickstein. Prac-\ntical tradeoffs between memory, compute, and performance in learned optimizers. In\nConference on Lifelong Learning Agents, pages 142\u2013164. PMLR, 2022.\nJ. Nocedal and S. J. Wright. Numerical optimization. Springer, 1999.\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison,\nA. Tejani, S. Chilamkurthy, B. Steiner, L. Fa"
  },
  {
    "chunk_id": "doc_88121c78_chunk_25_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 25,
    "text": "son,\nA. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch:\nAn imperative style, high-performance deep learning library. In Advances in Neural\nInformation Processing Systems 32:8024\u20138035, 2019.\nM.Penwarden,A.D.Jagtap,S.Zhe,G.E.Karniadakis,andR.M.Kirby. Aunifiedscalable\nframework for causal sweeping strategies for physics-informed neural networks (PINNs)\nand their temporal decompositions. arXiv:2302.14227, 2023.\nA. F. Psaros, K. Kawaguchi, and G. E. Karniadakis. Meta-l"
  },
  {
    "chunk_id": "doc_88121c78_chunk_25_3",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 25,
    "text": "saros, K. Kawaguchi, and G. E. Karniadakis. Meta-learning PINN loss functions.\nJ. Comput. Phys., 458:111121, 2022.\nN. Rahaman, A. Baratin, D. Arpit, F. Draxler, M. Lin, F. Hamprecht, Y. Bengio, and\nA. Courville. On the spectral bias of neural networks. In Proceedings of the 36th Inter-\nnational Conference on Machine Learning, pages 5301\u20135310. PMLR, 2019.\nM. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep\nlearning framework for solving forward and inverse pr"
  },
  {
    "chunk_id": "doc_88121c78_chunk_25_4",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 25,
    "text": "rning framework for solving forward and inverse problems involving nonlinear partial\ndifferential equations. J. Comput. Phys., 378:686\u2013707, 2019.\nO.SenerandV.Koltun. Multi-tasklearningasmulti-objectiveoptimization. InProceedings\nof the 32nd International Conference on Neural Information Processing Systems, pages\n525\u2013536, 2018. arXiv:1810.04650.\nN. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning, pages 4596\u20134604."
  },
  {
    "chunk_id": "doc_88121c78_chunk_25_5",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 25,
    "text": "l Conference on Machine Learning, pages 4596\u20134604. PMLR, 2018.\nP.Vicol, L.Metz, andJ.Sohl-Dickstein. Unbiasedgradientestimationinunrolledcomputa-\ntion graphs with persistent evolution strategies. In International Conference on Machine\nLearning, pages 10553\u201310563. PMLR, 2021.\nS. Wang and P. Perdikaris. Long-time integration of parametric evolution equations with\nphysics-informed deeponets. J. Comput. Phys., 475:111855, 2023.\n25"
  },
  {
    "chunk_id": "doc_88121c78_chunk_26_0",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 26,
    "text": "Bihlo\nS. Wang, S. Sankaran, and P. Perdikaris. Respecting causality is all you need for training\nphysics-informed neural networks. arXiv:2203.07404, 2022.\nO.Wichrowska,N.Maheswaranathan,M.W.Hoffman,S.G.Colmenarejo,M.Denil,N.Fre-\nitas, andJ.Sohl-Dickstein. Learnedoptimizersthatscaleandgeneralize. InInternational\nconference on machine learning, pages 3751\u20133760. PMLR, 2017.\nD. L. Williamson, J. B. Drake, J. J. Hack, J. Ru\u00a8diger, and P. N. Swarztrauber. A stan-\ndard test set for numerical approximat"
  },
  {
    "chunk_id": "doc_88121c78_chunk_26_1",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 26,
    "text": "er. A stan-\ndard test set for numerical approximations to the shallow water equations in spherical\ngeometry. J. Comput. Phys., 102:211\u2013224, 1992.\nC. Wu, M. Zhu, Q. Tan, Y. Kartha, and L. Lu. A comprehensive study of non-adaptive and\nresidual-basedadaptivesamplingforphysics-informedneuralnetworks. Comput. Methods\nAppl. Mech. Eng., 403:115671, 2023.\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea\nFinn. Gradient surgery for multi-task learning. In Advances in Neu"
  },
  {
    "chunk_id": "doc_88121c78_chunk_26_2",
    "doc_id": "doc_88121c78",
    "doc_name": "ml_research_paper2.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-01-02",
    "source": "ml_research_paper2.pdf",
    "page": 26,
    "text": "urgery for multi-task learning. In Advances in Neural Information\nProcessing Systems, 33:5824\u20135836, 2020\n26"
  },
  {
    "chunk_id": "doc_70b59811_chunk_1_0",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 1,
    "text": "International Journal for Multidisciplinary Research (IJFMR)\nE-ISSN: 2582-2160 \u25cf Website: www.ijfmr.com \u25cf Email: editor@ijfmr.com\nResearch Paper on Machine Learning\nDivyanshu Singh\nSoftware Engineer, Reva university\nAbstract\nMachine learning (ML), a subset of artificial intelligence (AI), has gained significant traction in recent\nyears due to its ability to analyze and interpret vast amounts of data. This paper explores the fundamental\nconcepts, methodologies, applications, challenges, and futur"
  },
  {
    "chunk_id": "doc_70b59811_chunk_1_1",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 1,
    "text": "methodologies, applications, challenges, and future trends of machine learning. By\nunderstanding its impact across various domains, we can appreciate its transformative potential and\naddress the associated challenges.\n1. Introduction\nMachine learning refers to the scientific study of algorithms and statistical models that enable computers\nto perform specific tasks without explicit programming. Instead of following predefined rules, ML systems\nlearn from data, making them particularly effective i"
  },
  {
    "chunk_id": "doc_70b59811_chunk_1_2",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 1,
    "text": "rn from data, making them particularly effective in recognizing patterns and making predictions. The\ngrowing availability of data, coupled with advances in computational power, has propelled machine\nlearning into mainstream use, influencing sectors such as finance, healthcare, marketing, and technology.\n2. Evolution of Machine Learning\n2.1 Historical Background\nMachine learning's roots trace back to the mid-20th century, evolving through several key phases:\n\u2022 1950s-60s: Early concepts of machine"
  },
  {
    "chunk_id": "doc_70b59811_chunk_1_3",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 1,
    "text": "key phases:\n\u2022 1950s-60s: Early concepts of machine learning emerged alongside AI research. Notable developments\nincluded the perceptron model for binary classification.\n\u2022 1980s: The introduction of backpropagation in neural networks revitalized interest in ML. Expert\nsystems gained traction, offering rule-based reasoning.\n\u2022 1990s-2000s: The rise of support vector machines and ensemble methods marked significant\nadvancements in supervised learning techniques.\n\u2022 2010s-Present: Deep learning, drive"
  },
  {
    "chunk_id": "doc_70b59811_chunk_1_4",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 1,
    "text": " techniques.\n\u2022 2010s-Present: Deep learning, driven by advancements in neural network architectures and access to\nlarge datasets, has revolutionized machine learning, leading to breakthroughs in image and speech\nrecognition.\n2.2 Methodologies\nMachine learning can be categorized into three primary types based on the nature of the learning process:\n1. Supervised Learning: The model is trained on labeled data, learning to map input features to\ncorresponding outputs. Common algorithms include linear"
  },
  {
    "chunk_id": "doc_70b59811_chunk_1_5",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 1,
    "text": "sponding outputs. Common algorithms include linear regression, decision trees, and neural\nnetworks.\n2. Unsupervised Learning: The model learns from unlabeled data, identifying patterns and structures\nwithin the data. Techniques include clustering (e.g., K-means) and dimensionality reduction (e.g.,\nPCA).\n3. Reinforcement Learning: In this paradigm, an agent learns by interacting with an environment,\nreceiving feedback in the form of rewards or penalties. This approach is commonly used in robotics"
  },
  {
    "chunk_id": "doc_70b59811_chunk_1_6",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 1,
    "text": "alties. This approach is commonly used in robotics\nIJFMR240425857 Volume 6, Issue 4, July-August 2024 1"
  },
  {
    "chunk_id": "doc_70b59811_chunk_2_0",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 2,
    "text": "International Journal for Multidisciplinary Research (IJFMR)\nE-ISSN: 2582-2160 \u25cf Website: www.ijfmr.com \u25cf Email: editor@ijfmr.com\nand game AI.\n3. Applications of Machine Learning\nMachine learning is applied across numerous domains, significantly enhancing efficiency and\neffectiveness.\n3.1 Healthcare\nML algorithms analyze patient data for early disease detection, treatment personalization, and drug\ndiscovery. Applications include predicting patient outcomes, diagnosing medical images, and optimiz"
  },
  {
    "chunk_id": "doc_70b59811_chunk_2_1",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 2,
    "text": "t outcomes, diagnosing medical images, and optimizing\nclinical workflows.\n3.2 Finance\nIn finance, machine learning is utilized for fraud detection, algorithmic trading, credit scoring, and risk\nmanagement. ML models can analyze transaction patterns and market trends to inform investment\nstrategies.\n3.3 Marketing\nMachine learning enhances customer segmentation, targeting, and personalization in marketing\ncampaigns. Predictive analytics helps businesses understand consumer behavior and optimize ad"
  },
  {
    "chunk_id": "doc_70b59811_chunk_2_2",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 2,
    "text": "esses understand consumer behavior and optimize advertising\nefforts.\n3.4 Transportation\nSelf-driving cars rely heavily on machine learning for perception, navigation, and decision-making. ML\nalgorithms process sensor data to identify obstacles, predict traffic patterns, and optimize routes.\n3.5 Natural Language Processing (NLP)\nNLP applications leverage machine learning to improve language understanding and generation.\nExamples include chatbots, language translation, and sentiment analysis.\n4. C"
  },
  {
    "chunk_id": "doc_70b59811_chunk_2_3",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 2,
    "text": "language translation, and sentiment analysis.\n4. Challenges in Machine Learning\nDespite its success, machine learning faces several challenges:\n4.1 Data Quality and Quantity\nMachine learning models require high-quality, representative datasets for accurate predictions. Poor data\nquality or insufficient data can lead to biased or unreliable outcomes.\n4.2 Overfitting and Underfitting\nStriking the right balance between model complexity and generalization is crucial. Overfitting occurs\nwhen a model "
  },
  {
    "chunk_id": "doc_70b59811_chunk_2_4",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 2,
    "text": "ation is crucial. Overfitting occurs\nwhen a model learns noise rather than the underlying data patterns, while underfitting happens when the\nmodel is too simple to capture important trends.\n4.3 Interpretability\nMany machine learning models, particularly deep learning systems, operate as \"black boxes,\" making it\ndifficult to understand how they arrive at specific decisions. This lack of transparency raises ethical and\naccountability concerns.\n4.4 Computational Resources\nTraining sophisticated mac"
  },
  {
    "chunk_id": "doc_70b59811_chunk_2_5",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 2,
    "text": "Computational Resources\nTraining sophisticated machine learning models often requires substantial computational power and\nmemory, posing challenges for organizations with limited resources.\nIJFMR240425857 Volume 6, Issue 4, July-August 2024 2"
  },
  {
    "chunk_id": "doc_70b59811_chunk_3_0",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 3,
    "text": "International Journal for Multidisciplinary Research (IJFMR)\nE-ISSN: 2582-2160 \u25cf Website: www.ijfmr.com \u25cf Email: editor@ijfmr.com\n5. Future Trends in Machine Learning\n5.1 Explainable AI\nThere is a growing emphasis on developing explainable AI models that provide insights into their decision-\nmaking processes. This trend aims to enhance transparency and trust in machine learning applications.\n5.2 Federated Learning\nFederated learning allows multiple devices to collaboratively train models while k"
  },
  {
    "chunk_id": "doc_70b59811_chunk_3_1",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 3,
    "text": "le devices to collaboratively train models while keeping data localized.\nThis approach addresses privacy concerns by reducing the need to transfer sensitive data to central servers.\n5.3 Transfer Learning\nTransfer learning enables models trained on one task to be adapted for another, reducing the need for large\ndatasets. This approach is particularly useful in domains where labeled data is scarce.\n5.4 Integration with Other Technologies\nThe convergence of machine learning with other technologies,"
  },
  {
    "chunk_id": "doc_70b59811_chunk_3_2",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 3,
    "text": "gence of machine learning with other technologies, such as the Internet of Things (IoT),\nblockchain, and augmented reality, will drive innovative applications and solutions across industries.\n6. Conclusion\nMachine learning has emerged as a pivotal technology with the potential to transform various sectors by\nenabling systems to learn from data and improve over time. While challenges such as data quality,\ninterpretability, and computational requirements persist, ongoing research and development e"
  },
  {
    "chunk_id": "doc_70b59811_chunk_3_3",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 3,
    "text": "ements persist, ongoing research and development efforts aim to\naddress these issues. As machine learning continues to evolve, its integration into everyday life will\nenhance efficiency, decision-making, and innovation across diverse fields.\nReferences\n1. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n2. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\n3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\n4. Dom"
  },
  {
    "chunk_id": "doc_70b59811_chunk_3_4",
    "doc_id": "doc_70b59811",
    "doc_name": "ml_research_paper3.pdf",
    "doc_type": "research_paper",
    "published_date": "2024-03-12",
    "source": "ml_research_paper3.pdf",
    "page": 3,
    "text": "ville, A. (2016). Deep Learning. MIT Press.\n4. Domingos, P. (2012). A Few Useful Things to Know About Machine Learning. Communications of\nthe ACM, 55(10), 78-87.\n5. Zhang, Y., & Yang, Q. (2017). A Survey on Multi-Task Learning. IEEE Transactions on Knowledge\nand Data Engineering, 30(5), 1815-1830.\nIJFMR240425857 Volume 6, Issue 4, July-August 2024 3"
  },
  {
    "chunk_id": "doc_d18b6434_chunk_1_0",
    "doc_id": "doc_d18b6434",
    "doc_name": "ai_healthcare_blog.md",
    "doc_type": "blog",
    "published_date": "2025-07-07",
    "source": "ai_healthcare_blog.md",
    "page": 1,
    "text": "# Artificial Intelligence in Healthcare: Transforming Medicine and Patient Care\n\nArtificial Intelligence (AI) is reshaping healthcare by improving diagnostics, personalizing treatment, and optimizing hospital operations. With the increasing availability of medical data and advances in machine learning, AI is becoming a critical tool for clinicians, researchers, and healthcare administrators.\n\n---\n\n## 1. The Evolution of AI in Healthcare\n\nThe use of computation in medicine dates back decades, but"
  },
  {
    "chunk_id": "doc_d18b6434_chunk_1_1",
    "doc_id": "doc_d18b6434",
    "doc_name": "ai_healthcare_blog.md",
    "doc_type": "blog",
    "published_date": "2025-07-07",
    "source": "ai_healthcare_blog.md",
    "page": 1,
    "text": "of computation in medicine dates back decades, but early systems were rule-based and limited in scope. Modern AI systems, powered by machine learning and deep learning, learn directly from data rather than relying on hand-crafted rules.\n\nKey milestones include:\n- Expert systems for diagnosis in the 1970s and 1980s\n- Medical imaging analysis using classical ML in the 2000s\n- Deep learning breakthroughs in radiology and pathology after 2012\n- Large-scale clinical decision support systems in recent"
  },
  {
    "chunk_id": "doc_d18b6434_chunk_1_2",
    "doc_id": "doc_d18b6434",
    "doc_name": "ai_healthcare_blog.md",
    "doc_type": "blog",
    "published_date": "2025-07-07",
    "source": "ai_healthcare_blog.md",
    "page": 1,
    "text": "-scale clinical decision support systems in recent years\n\nToday, AI systems can process massive datasets that include medical images, electronic health records (EHRs), genomic data, and real-time sensor data.\n\n---\n\n## 2. AI in Medical Imaging and Diagnostics\n\nMedical imaging is one of the most successful applications of AI in healthcare.\n\n### 2.1 Radiology\nDeep learning models can detect abnormalities in:\n- X-rays\n- CT scans\n- MRI images\n- Mammograms\n\nAI systems assist radiologists by:\n- Highlig"
  },
  {
    "chunk_id": "doc_d18b6434_chunk_1_3",
    "doc_id": "doc_d18b6434",
    "doc_name": "ai_healthcare_blog.md",
    "doc_type": "blog",
    "published_date": "2025-07-07",
    "source": "ai_healthcare_blog.md",
    "page": 1,
    "text": "rams\n\nAI systems assist radiologists by:\n- Highlighting suspicious regions\n- Reducing false negatives\n- Improving workflow efficiency\n\n### 2.2 Pathology\nIn digital pathology, AI models analyze tissue slides to:\n- Detect cancer cells\n- Grade tumors\n- Identify rare patterns invisible to the human eye\n\nThese tools help pathologists focus on complex cases while maintaining consistency and accuracy.\n\n---\n\n## 3. Personalized Medicine and Treatment Planning\n\nAI enables personalized healthcare by tailor"
  },
  {
    "chunk_id": "doc_d18b6434_chunk_1_4",
    "doc_id": "doc_d18b6434",
    "doc_name": "ai_healthcare_blog.md",
    "doc_type": "blog",
    "published_date": "2025-07-07",
    "source": "ai_healthcare_blog.md",
    "page": 1,
    "text": "ning\n\nAI enables personalized healthcare by tailoring treatments to individual patients.\n\n### 3.1 Genomics and Precision Medicine\nMachine learning models analyze genomic data to:\n- Predict disease risk\n- Identify drug-response patterns\n- Support targeted cancer therapies\n\n### 3.2 Treatment Recommendation Systems\nAI-driven systems combine:\n- Patient history\n- Clinical guidelines\n- Population-level data\n\nThis allows clinicians to choose optimal treatment plans with higher confidence.\n\n---\n\n## 4. A"
  },
  {
    "chunk_id": "doc_d18b6434_chunk_1_5",
    "doc_id": "doc_d18b6434",
    "doc_name": "ai_healthcare_blog.md",
    "doc_type": "blog",
    "published_date": "2025-07-07",
    "source": "ai_healthcare_blog.md",
    "page": 1,
    "text": "atment plans with higher confidence.\n\n---\n\n## 4. AI in Drug Discovery and Development\n\nDrug discovery is traditionally expensive and time-consuming. AI significantly accelerates this process.\n\n### 4.1 Molecule Discovery\nAI models:\n- Predict molecular properties\n- Identify promising drug candidates\n- Reduce laboratory experimentation costs\n\n### 4.2 Clinical Trials Optimization\nAI helps by:\n- Identifying suitable trial participants\n- Predicting trial outcomes\n- Monitoring patient adherence\n\nThis s"
  },
  {
    "chunk_id": "doc_d18b6434_chunk_1_6",
    "doc_id": "doc_d18b6434",
    "doc_name": "ai_healthcare_blog.md",
    "doc_type": "blog",
    "published_date": "2025-07-07",
    "source": "ai_healthcare_blog.md",
    "page": 1,
    "text": "al outcomes\n- Monitoring patient adherence\n\nThis shortens development timelines and lowers failure rates.\n\n---\n\n## 5. Hospital Operations and Workflow Optimization\n\nBeyond clinical care, AI improves hospital efficiency.\n\nApplications include:\n- Predicting patient admission rates\n- Optimizing staff scheduling\n- Managing supply chains\n- Reducing emergency room wait times\n\nPredictive analytics help hospitals allocate resources more effectively and improve patient satisfaction.\n\n---\n\n## 6. Ethical, "
  },
  {
    "chunk_id": "doc_d18b6434_chunk_1_7",
    "doc_id": "doc_d18b6434",
    "doc_name": "ai_healthcare_blog.md",
    "doc_type": "blog",
    "published_date": "2025-07-07",
    "source": "ai_healthcare_blog.md",
    "page": 1,
    "text": "mprove patient satisfaction.\n\n---\n\n## 6. Ethical, Legal, and Privacy Challenges\n\nDespite its promise, AI in healthcare presents serious challenges.\n\n### 6.1 Data Privacy\nHealthcare data is highly sensitive. Ensuring compliance with regulations such as HIPAA and GDPR is critical.\n\n### 6.2 Bias and Fairness\nAI models trained on biased datasets can:\n- Produce inaccurate diagnoses\n- Worsen healthcare disparities\n\n### 6.3 Explainability\nClinicians must understand AI recommendations. Black-box models "
  },
  {
    "chunk_id": "doc_d18b6434_chunk_1_8",
    "doc_id": "doc_d18b6434",
    "doc_name": "ai_healthcare_blog.md",
    "doc_type": "blog",
    "published_date": "2025-07-07",
    "source": "ai_healthcare_blog.md",
    "page": 1,
    "text": "t understand AI recommendations. Black-box models can reduce trust and hinder adoption.\n\n---\n\n## 7. The Future of AI in Healthcare\n\nThe future will likely include:\n- AI-assisted clinicians, not replacements\n- Real-time health monitoring via wearables\n- Integration of multimodal data (text, images, signals)\n- Stronger regulatory frameworks\n\nAI will augment human expertise, allowing healthcare professionals to focus more on patient care and complex decision-making.\n\n---\n\n## Conclusion\n\nArtificial "
  },
  {
    "chunk_id": "doc_d18b6434_chunk_1_9",
    "doc_id": "doc_d18b6434",
    "doc_name": "ai_healthcare_blog.md",
    "doc_type": "blog",
    "published_date": "2025-07-07",
    "source": "ai_healthcare_blog.md",
    "page": 1,
    "text": " decision-making.\n\n---\n\n## Conclusion\n\nArtificial Intelligence has the potential to revolutionize healthcare by improving accuracy, efficiency, and personalization. However, success depends on responsible deployment, ethical oversight, and collaboration between technologists and medical professionals. When used correctly, AI can lead to better outcomes for patients and healthcare systems worldwide.\n"
  },
  {
    "chunk_id": "doc_29dec407_chunk_1_0",
    "doc_id": "doc_29dec407",
    "doc_name": "ml_notes.txt",
    "doc_type": "notes",
    "published_date": "2025-11-22",
    "source": "ml_notes.txt",
    "page": 1,
    "text": "PAGE 1: INTRODUCTION TO ML IN PERSONNEL MANAGEMENT\n--------------------------------------------------\nMachine Learning (ML) is increasingly used in personnel and human resource\nmanagement to support data-driven decision-making. Personnel ML focuses on\nanalyzing employee-related data to improve hiring, performance evaluation,\nretention, and workforce planning.\n\nCommon data sources include resumes, application forms, performance reviews,\nattendance logs, engagement surveys, payroll data, and train"
  },
  {
    "chunk_id": "doc_29dec407_chunk_1_1",
    "doc_id": "doc_29dec407",
    "doc_name": "ml_notes.txt",
    "doc_type": "notes",
    "published_date": "2025-11-22",
    "source": "ml_notes.txt",
    "page": 1,
    "text": " logs, engagement surveys, payroll data, and training records.\nThese datasets may be structured (tables, scores) or unstructured (text,\nemails, feedback).\n\nKey objectives of personnel ML:\n- Improve quality and speed of recruitment\n- Reduce employee turnover\n- Identify high-potential employees\n- Detect bias and ensure fairness\n- Optimize training and development programs\n\nChallenges specific to personnel ML:\n- Data privacy and legal compliance (GDPR, labor laws)\n- Bias and discrimination risks\n- "
  },
  {
    "chunk_id": "doc_29dec407_chunk_1_2",
    "doc_id": "doc_29dec407",
    "doc_name": "ml_notes.txt",
    "doc_type": "notes",
    "published_date": "2025-11-22",
    "source": "ml_notes.txt",
    "page": 1,
    "text": "PR, labor laws)\n- Bias and discrimination risks\n- Interpretability of models\n- Ethical use of employee data\n\nTypical ML task types:\n- Classification (e.g., hire/no-hire, promotion readiness)\n- Regression (e.g., performance score prediction)\n- Clustering (e.g., employee segmentation)\n- Natural Language Processing (resume screening, feedback analysis)\n\n--------------------------------------------------\n\n\nPAGE 2: RECRUITMENT AND TALENT ACQUISITION\n-----------------------------------------\nML plays "
  },
  {
    "chunk_id": "doc_29dec407_chunk_1_3",
    "doc_id": "doc_29dec407",
    "doc_name": "ml_notes.txt",
    "doc_type": "notes",
    "published_date": "2025-11-22",
    "source": "ml_notes.txt",
    "page": 1,
    "text": "----------------------------------------\nML plays a major role in modern recruitment pipelines. Automated resume\nscreening systems use NLP techniques to extract skills, experience, and\neducation from resumes and compare them against job requirements.\n\nKey techniques:\n- Keyword extraction and embeddings\n- Similarity scoring between resumes and job descriptions\n- Ranking candidates based on predicted job fit\n\nCommon models:\n- Logistic Regression\n- Random Forests\n- Gradient Boosting\n- Transformer-b"
  },
  {
    "chunk_id": "doc_29dec407_chunk_1_4",
    "doc_id": "doc_29dec407",
    "doc_name": "ml_notes.txt",
    "doc_type": "notes",
    "published_date": "2025-11-22",
    "source": "ml_notes.txt",
    "page": 1,
    "text": "Random Forests\n- Gradient Boosting\n- Transformer-based NLP models (for text-heavy tasks)\n\nBenefits:\n- Reduced time-to-hire\n- Consistent candidate evaluation\n- Ability to process large applicant pools\n\nRisks:\n- Historical hiring bias reflected in training data\n- Over-reliance on automated screening\n- Exclusion of non-traditional candidates\n\nBest practices:\n- Regular bias audits\n- Human-in-the-loop decision-making\n- Transparent scoring criteria\n- Diverse training datasets\n\n------------------------"
  },
  {
    "chunk_id": "doc_29dec407_chunk_1_5",
    "doc_id": "doc_29dec407",
    "doc_name": "ml_notes.txt",
    "doc_type": "notes",
    "published_date": "2025-11-22",
    "source": "ml_notes.txt",
    "page": 1,
    "text": "iverse training datasets\n\n--------------------------------------------------\n\n\nPAGE 3: PERFORMANCE MANAGEMENT AND PREDICTION\n---------------------------------------------\nPerformance management ML models aim to predict or evaluate employee\nperformance using historical data. Inputs may include KPIs, peer reviews,\nmanager ratings, project outcomes, and attendance patterns.\n\nUse cases:\n- Performance score prediction\n- Early identification of underperformers\n- Recognition of high-performing employee"
  },
  {
    "chunk_id": "doc_29dec407_chunk_1_6",
    "doc_id": "doc_29dec407",
    "doc_name": "ml_notes.txt",
    "doc_type": "notes",
    "published_date": "2025-11-22",
    "source": "ml_notes.txt",
    "page": 1,
    "text": "rformers\n- Recognition of high-performing employees\n- Objective support for promot\n"
  }
]